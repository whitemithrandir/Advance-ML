## What is Reinforcement Learning?
Makine öğrenimi uzmanlığının bu son haftasına hoş geldiniz. Bu uzmanlığın sonuna yaklaşıyor olmamız benim için biraz buruk ama bu haftayı sabırsızlıkla bekliyorum, sizinle takviyeli öğrenme hakkında bazı heyecan verici fikirleri paylaşıyorum. Makine öğreniminde, takviye öğrenimi, bugün henüz ticari uygulamalarda çok yaygın olarak uygulanmasa da makine öğreniminin temel direklerinden biri olan fikirlerden biridir. Ve onu destekleyen ve her gün geliştiren çok sayıda heyecan verici araştırma var. Pekiştirmeli öğrenmenin ne olduğuna bir göz atarak başlayalım. Bir örnekle başlayalım. İşte otonom bir helikopterin resmi. Bu aslında Stanford otonom helikopteri, 32 pound ağırlığında ve şu anda ofisimde duruyor. Diğer birçok otonom helikopter gibi, yerleşik bir bilgisayar, GPS, ivmeölçerler, jiroskoplar ve manyetik pusula ile donatılmıştır, böylece her zaman nerede olduğunu oldukça doğru bir şekilde bilir. Size bu helikopterin anahtarlarını versem ve onu uçurmak için bir program yazmanızı istesem, bunu nasıl yaparsınız? Radyo kontrollü helikopterler bunun gibi joysticklerle kontrol edilir ve bu nedenle görev saniyede on kez size helikopterin konumu, yönü ve hızı verilir. Ve helikopteri havada dengede tutabilmek için bu iki kontrol çubuğunu nasıl hareket ettireceğinize karar vermelisiniz. Bu arada, radyo kontrollü helikopterlerin yanı sıra dört rotorlu dronları kendim uçurdum. Ve radyo kontrollü helikopterleri uçurmak, havada dengede tutmak aslında biraz daha zordur. Peki bunu otomatik olarak yapacak bir programı nasıl yazarsınız? Size Stanford otonom helikopterimizle yapacağımız bir şeyin eğlenceli bir videosunu göstereyim. İşte bir takviyeli öğrenme algoritmasının kontrolü altında uçtuğu bir video. Ve videoyu oynatmama izin ver. O gün aslında kameraman bendim ve bu bilgisayar kontrolünde uçan helikopter ve videoyu uzaklaştırırsam gökyüzüne dikilmiş ağaçları görüyorsunuz. Takviyeli öğrenmeyi kullanarak, aslında baş aşağı uçmayı öğrenmek için bu helikopteri aldık. Baş aşağı uçmasını söyledik. Ve böylece güçlendirilmiş öğrenme, helikopterlerin çok çeşitli akrobasi hareketlerini uçurmasını sağlamak için kullanıldı ya da biz onlara akrobasi manevraları diyoruz. Bu arada, diğer videoları görmekle ilgileniyorsanız, bu URL'den onlara da göz atabilirsiniz. Peki pekiştirmeli öğrenmeyi kullanarak bir helikopterin kendi kendine uçmasını nasıl sağlarsınız? Görev, kontrol çubuklarını nasıl hareket ettireceğine karar vermek için helikopterin konumuna verilir. Takviyeli öğrenmede, helikopterin konumu, yönü ve hızı vb. durumlarını s olarak adlandırırız. Ve böylece görev, helikopterin durumundan eylem a'ya eşlenen bir işlev bulmaktır, yani helikopteri havada dengede tutmak, uçmak ve çarpmadan tutmak için iki kontrol çubuğunun ne kadar uzağa itilmesi gerektiği anlamına gelir. Bu sorunu denemenin bir yolu denetimli öğrenmeyi kullanmaktır. Bunun otonom helikopter uçuşu için harika bir yaklaşım olmadığı ortaya çıktı. Ama diyebilirsiniz ki, bir dizi durum gözlemi alabilirsek ve belki de uzman bir insan pilot bize yapılacak en iyi eylemin ne olduğunu söyler. Daha sonra, burada x olarak adlandırdığım durumlardan, burada y olarak adlandırdığım bir eyleme eşlemeyi doğrudan öğrenmek için denetimli öğrenmeyi kullanarak bir sinir ağını eğitebilirsiniz. Ancak, helikopter havada hareket ederken aslında çok belirsiz olduğu ortaya çıktı, yapılacak doğru eylemin tam olarak ne olduğu. Biraz sola mı yoksa çok daha fazla sola mı yatırıyorsunuz veya helikopter stresini biraz mı yoksa çok mu artırıyorsunuz? Bir x veri seti ve ideal eylem y elde etmek aslında çok zordur. Bu nedenle, helikopter ve diğer robotlar gibi bir robotu kontrol etme görevinin çoğunda, denetimli öğrenme yaklaşımı iyi çalışmıyor ve bunun yerine takviyeli öğrenme kullanıyoruz. Takviyeli öğrenme için önemli bir girdi, helikoptere ne zaman iyi ve ne zaman kötü gittiğini söyleyen ödül veya ödül işlevi olarak adlandırılan bir şeydir. Bu yüzden, ödül işlevini düşünmeyi sevdiğim şekilde, bir köpeği eğitmeye benziyor. Ben büyürken, ailemin bir köpeği vardı ve benim işim köpeği ya da yavruyu terbiye etmekti. Peki bir köpek yavrusu nasıl iyi davranır? Pekala, köpeğe bu kadarını gösteremezsin. Onun yerine işini yapmasına izin veriyorsun ve ne zaman iyi bir şey yapsa, sen gidiyorsun, ahbap. Ve ne zaman kötü bir şey yapsalar, gidersin, kötü köpek. Ve sonra umarım kendi kendine nasıl daha çok iyi köpek ve daha az kötü köpek işi yapılacağını öğrenir. Takviyeli öğrenme algoritması ile eğitim işte böyle bir şey. Helikopter iyi uçtuğunda, gidersiniz, iyi helikopter ve çarpma gibi kötü bir şey yaparsa, gidersiniz, kötü helikopter. Ve sonra, iyi helikopterden daha fazlasını ve kötü helikopter sonuçlarından daha azını nasıl elde edeceğinizi bulmak, pekiştirmeli öğrenme algoritmasının işidir.
Takviyeli öğrenmenin neden bu kadar güçlü olduğunu düşünmenin bir yolu, ona nasıl yapılacağından çok ne yapması gerektiğini söylemeniz gerektiğidir. Ve en uygun eylem yerine ödül işlevini belirtmek, sistemi nasıl tasarladığınız konusunda size çok daha fazla esneklik sağlar. Helikopteri uçurmak için somut olarak, ne zaman iyi uçuyorsa, ona iyi uçtuğu her saniye artı bir ödül verebilirsiniz. Ve belki de ne zaman kötü uçuyorsa, ona negatif bir ödül verebilirsiniz ya da düşerse, negatif 1000 gibi çok büyük bir negatif ödül verebilirsiniz. Ve bu, helikopteri iyi uçmak için çok daha fazla zaman harcamaya ve umarım asla düşmemeye teşvik eder. Ama işte başka bir eğlenceli video. Uzun yıllardır pekiştirmeli öğrenme için iyi köpek kötü köpek benzetmesini kullanıyordum. Ve sonra bir gün gerçekten robotik bir köpeğe el atmayı başardım ve bu pekiştirmeyi gerçekten iyi köpek kötü köpek öğrenme metodolojisini bir robot köpeği engelleri aşması için eğitmek için kullanabilirdim. Bu, onu ödüllendiren takviyeli öğrenmeyi kullanarak ekranın soluna doğru hareket ederek ayaklarını dikkatli bir şekilde yerleştirmeyi veya çeşitli engellerin üzerinden nasıl tırmanacağını öğrenen bir robot köpeğin videosu. Ve bir köpeği böyle programlamak için ne gerektiğini düşünürseniz, hiçbir fikrim yok, belirli bir engeli aşmak için bacaklarını yerleştirmenin en iyi yolunun ne olduğunu ona nasıl söyleyeceğimi gerçekten bilmiyorum. Bütün bunlar, robot tarafından sadece onu teşvik eden ödüller vererek, ekranın solundaki hedefe doğru ilerleme kaydederek otomatik olarak çözüldü. Günümüzde pekiştirmeli öğrenme, robotları kontrol etmekten çok çeşitli uygulamalara başarıyla uygulanmaktadır. Ve aslında bu haftanın ilerleyen saatlerinde uygulama laboratuvarında, simülasyonda bir aya iniş yapmak için kendinize bir pekiştirmeli öğrenme algoritması uyguluyorsunuz. Fabrika optimizasyonu için de kullanıldı. İş hacmini ve verimliliği ve finansal hisse senedi alım satımını en üst düzeye çıkarmak için fabrikadaki şeyleri nasıl yeniden düzenlersiniz? Örneğin, bir arkadaşım verimli stok uygulaması üzerinde çalışıyordu. Dolayısıyla, önümüzdeki birkaç gün içinde bir milyon hisse satmaya karar verdiyseniz, borsada aniden bir milyon hisseyi düşürmek istemeyebilirsiniz çünkü bu, fiyatları aleyhinize hareket ettirecektir. Öyleyse, satmak istediğiniz hisseleri satabilmeniz ve onlar için mümkün olan en iyi fiyatı alabilmeniz için alım satımlarınızı zaman içinde sıralamanın en iyi yolu nedir? Son olarak, pek çok video oyunu oynamanın yanı sıra damadan satranca, briç kart oyununa kadar oyun oynamaya kadar pek çok takviyeli öğrenme uygulaması da olmuştur. İşte bu pekiştirmeli öğrenmedir. Takviyeli öğrenme, neredeyse denetimli öğrenme kadar kullanılmasa da, günümüzde hala birkaç uygulamada kullanılmaktadır. Ve ana fikir, algoritmaya her bir girdi için doğru y çıktısının ne olduğunu söylemeniz yerine, tek yapmanız gereken ona ne zaman iyi gittiğini ve ne zaman kötü gittiğini söyleyen bir ödül işlevi belirtmektir. Ve iyi eylemlerin nasıl seçileceğini otomatik olarak bulmak algoritmanın işidir. Bununla birlikte, pekiştirmeli öğrenme problemini formüle edeceğimiz ve aynı zamanda iyi eylemleri otomatik olarak seçmek için algoritmalar geliştirmeye başlayacağımız bir sonraki videoya geçelim.

## Mars rover example
Takviyeli öğrenme şekilciliğini bitirmek için, bir helikopter veya robot köpek gibi karmaşık bir şeye bakmak yerine, genel olarak Mars gezicisinden ilham alan basitleştirilmiş bir örnek kullanabiliriz. Bu, Stanford profesörü Emma Branskill ve iş arkadaşlarımdan biri olan Jagriti Agrawal'ın şu anda Mars gezicisini gerçekten kontrol eden bir kod yazması nedeniyle örnekten uyarlandı ve bu da bu örneği geliştirmeme yardımcı oldu ve konuşmama yardımcı oldu. Hadi bir bakalım. Mars gezicisinden esinlenen basitleştirilmiş bir örnek kullanarak pekiştirmeli öğrenme geliştireceğiz. Bu uygulamada, gezici, buradaki altı kutuda gösterildiği gibi altı konumdan herhangi birinde olabilir. Gezici, örneğin, burada gösterilen dördüncü kutuya doğru yola çıkabilir. Mars gezicisinin konumu pekiştirmeli öğrenmede durum olarak adlandırılır ve ben bu altı durumu, durum 1, durum 2, durum 3, durum 4, durum 5 ve durum 6 olarak adlandıracağım ve böylece gezici başlıyor 4. durumda kapalı. Şimdi gezici, farklı bilim görevlerini yerine getirmeye çalışmak için Mars'a gönderildi. Gezegendeki farklı yerlerdeki kayayı analiz etmek için matkap, radar veya spektrometre gibi sensörlerini kullanmak üzere farklı yerlere gidebilir veya dünyadaki bilim adamlarının bakması için ilginç fotoğraflar çekmek üzere farklı yerlere gidebilir. Bu örnekte, burada soldaki durum 1, bilim adamlarının yüzey aracının örneklemesini isteyeceği çok ilginç bir yüzeye sahiptir. Durum 6 ayrıca, bilim adamlarının gezicinin örneklemesini isteyeceği oldukça ilginç bir yüzeye sahiptir, ancak durum 1 kadar ilginç değildir. Bilim görevini ve durum 1'i gerçekleştirme olasılığımız, durum 6'dan daha fazladır, ancak durum 1 daha uzaktadır. . Durum 1'in potansiyel olarak daha değerli olduğunu yansıtmanın yolu, ödül işlevidir. 1. durumdaki ödül 100'dür ve 6. aşamadaki ödül 40'tır ve aradaki diğer tüm eyaletlerdeki ödüller, sıfır ödülü olarak yazacağım çünkü bilim için o kadar ilginç bir şey yok. 2, 3, 4 ve 5 durumlarında yapılabilir. Her adımda, gezici iki eylemden birini seçer. Ya sola gidebilir ya da sağa gidebilir. Soru şu ki, gezici ne yapmalı? Takviyeli öğrenmede, ödüllere çok dikkat ederiz çünkü robotun iyi mi kötü mü yaptığını bu şekilde anlarız. Robot 4. durumdan başlayarak sola giderse ne olabileceğine dair bazı örneklere bakalım. Ardından başlangıçta 4. durumdan başlayarak, sıfır ödülü alacak ve sola gittikten sonra 3. duruma geçecek ve burada tekrar sıfır ödül alır. Sonra 2 durumuna gelir, 0 ödülünü alır ve son olarak 100'lük bir ödül aldığı 1 durumuna gelir. Bu uygulama için, ya 1. ya da 6. duruma geldiğinde, gün biter Takviyeli öğrenmede, buna bazen bir son durum diyoruz ve bunun anlamı, bu uç durumlardan birine ulaştıktan sonra, o durumda bir ödül alıyor, ancak bundan sonra hiçbir şey olmuyor. Belki robotların yakıtı bitmiştir veya o gün için zamanları kalmamıştır, bu yüzden sadece 100 veya 40 ödülünün tadını çıkarabilir, ama o gün için bu kadar. Bundan sonra ek ödüller kazanamaz. Artık robot sola gitmek yerine sağa gitmeyi de seçebilir, bu durumda 4 durumundan önce sıfır ödülü alır ve sonra sağa hareket eder ve 5 durumuna geçer, başka bir ödül alır. sıfır ve sonra sağdaki bu diğer terminal durumuna, durum 6'ya ulaşacak ve 40'lık bir ödül alacak. Ancak sola gitmek ve sağa gitmek tek seçenek. Robotun yapabileceği bir şey, 4. durumdan başlayıp sağa doğru hareket etmeye karar verebilmesidir. Durum 4-5'ten gider, durum 4'te sıfır ve durum 5'te sıfır ödülü alır ve sonra belki fikrini değiştirir ve aşağıdaki gibi sola gitmeye karar verir, bu durumda 4. durumda, 3. durumda, 2. durumda sıfır ödülü ve ardından 1. duruma geldiğinde 100 ödül. Bu eylemler ve durumlar dizisinde, robot daha iyi zaman harcıyor. Bu, harekete geçmek için harika bir yol olmayabilir, ancak algoritmanın seçebileceği bir seçimdir, ancak umarım bunu seçmezsiniz. Özetlemek gerekirse, her adımda, robot bir durumda, ben buna S diyeceğim ve bir eylem seçiyor ve ayrıca bu durumdan aldığı bazı ödüllerden, R of S'den keyif alıyor. Bu eylemin bir sonucu olarak, yeni bir S üssü durumuna geçer. Somut bir örnek olarak, robot 4. durumdayken ve sola git eylemini yaptığında, belki o 4. durumla ilişkili sıfır ödülünün tadını çıkarmadı ve yeni bir 3. durumu olmayacak. Özel pekiştirmeli öğrenme algoritmalarında, bu dört şeyin, durum, eylem, ödül ve sonraki durum olduğunu görürsünüz; bu, temel olarak her eylemde bulunduğunuzda olan şeydir ve takviyeli öğrenme algoritmalarının karar verirken bakacağı temel unsurlardır. nasıl önlemler alınır.
Açıklığa kavuşturmak için, buradaki ödül, R of S, bu, bu durumla ilişkili ödüldür. Bu sıfır ödülü, durum 3'ten ziyade durum 4 ile ilişkilendirilir. Takviyeli öğrenme uygulamasının çalışma şekli budur. Bir sonraki videoda, pekiştirmeli öğrenme algoritmasının yapmasını istediğimiz şeyi tam olarak nasıl belirttiğimize bir göz atalım. Özellikle pekiştirmeli öğrenmede geri dönüş adı verilen önemli bir fikirden bahsedeceğiz. Bunun ne anlama geldiğini görmek için bir sonraki videoya geçelim.

## The Return in reinforcement learning
Bir önceki videoda pekiştirmeli öğrenme uygulamasında durumların neler olduğunu ve yaptığınız işlemlere bağlı olarak nasıl farklı durumlardan geçtiğinizi ve farklı ödüller kazandığınızı gördünüz. Ancak belirli bir ödül setinin farklı bir ödül setinden daha iyi veya daha kötü olduğunu nasıl anlarsınız? Bu videoda tanımlayacağımız pekiştirmeli öğrenmedeki geri dönüş, bunu yakalamamızı sağlıyor. Bunu incelerken, faydalı bulabileceğiniz bir benzetme şu: Ayağınızın dibinde beş dolarlık bir banknot olduğunu hayal edin, aşağı uzanıp alabilirsiniz veya yarım saatte kasabanın öbür ucuna, yarım saatte yürüyebilir ve 10 dolarlık bir banknot al. Hangisinin peşinden gitmeyi tercih edersin? On dolar, beş dolardan çok daha iyidir, ancak gidip o 10 dolarlık banknotu almak için yarım saat yürümeniz gerekiyorsa, o zaman belki onun yerine beş dolarlık banknotu almanız daha uygun olabilir. Geri dönüş kavramı, daha hızlı alabileceğiniz ödüllerin, ulaşmanız uzun zaman alan ödüllerden belki daha çekici olduğunu yakalar. Bunun tam olarak nasıl çalıştığına bir göz atalım. İşte bir Mars Rover örneği. Durum 4'ten başlayarak sola giderseniz, aldığınız ödüllerin durum 4'ten ilk adımda sıfır, durum 3'ten sıfır, durum 2'den sıfır ve ardından uç durum olan durum 1'de 100 olacağını gördük. Getiri, bu ödüllerin toplamı olarak tanımlanır, ancak indirim faktörü olarak adlandırılan ek bir faktörle ağırlıklandırılır. İndirgeme faktörü 1'den biraz küçük bir sayıdır. İndirgeme faktörü olarak 0.9'u seçeyim. İlk adımdaki ödülü sıfır olarak ağırlıklandıracağım, ikinci adımdaki ödül bir indirim faktörü, bu ödülün 0,9 katı ve sonra artı indirim faktörü^2 çarpı bu ödül ve sonra artı indirim faktörü ^ 3 kat bu ödül. Bunu hesaplarsanız 0,729 çarpı 100 yani 72,9 çıkıyor. Geri dönüşün daha genel formülü, eğer robotunuz bir dizi durumdan geçerse ve birinci adımda R_1, ikinci adımda R_2 ve üçüncü adımda R_3 vb. alırsa, o zaman dönüş R_1 olur. artı indirim faktörü Gama, bu örnekte 0,9 olarak ayarladığım bu Yunan alfabesi Gamma, Gama çarpı R_2 artı Gama^2 çarpı R_3 artı Gamma^3 çarpı R_4 vb. siz terminal durumuna gelene kadar. İndirgeme faktörü Gamma'nın yaptığı şey, takviyeli öğrenme algoritmasını biraz sabırsız hale getirme etkisine sahip olmasıdır. Çünkü geri dönüş birinci ödüle tam kredi veriyor yüzde 100 yani 1 katı R_1 ama sonra biraz daha az kredi veriyor ikinci adımda aldığınız ödül 0,9 ile çarpılıyor ve daha sonra aldığınız ödüle daha da az kredi veriyor. bir sonraki adım R_3'te vb. alın ve böylece ödülleri daha erken almak, toplam getiri için daha yüksek bir değerle sonuçlanır. Pek çok takviyeli öğrenme algoritmasında, indirgeme faktörü için ortak bir seçim, 0,9 veya 0,99 veya hatta 0,999 gibi 1'e oldukça yakın bir sayı olacaktır. Ancak kullanacağım çalışan örnekte açıklama amacıyla, aslında 0,5'lik bir indirgeme faktörü kullanacağım. Bu çok ağır bir şekilde, gelecekte ödülleri çok ağır bir şekilde düşürür, çünkü her ek ayrıştırma zaman damgasıyla, bir adım önce alacağınız ödüllerin yalnızca yarısı kadar kredi alırsınız. Gama 0,5'e eşit olsaydı, yukarıdaki örnekte getiri 0 artı 0,5 çarpı 0 olurdu, bu denklemi üste koyar, artı 0,5^2 0 artı 0,5^3 çarpı 100. Durum 1'den uç duruma geçtiği için bu ödül kaybedildi, ve bu 12.5'lik bir getiri olarak çıkıyor. Finansal uygulamalarda iskonto faktörünün de faiz oranı veya paranın zaman değeri olarak çok doğal bir yorumu vardır. Bugün bir dolarınız varsa, bu, gelecekte yalnızca bir dolarınız olacak duruma göre biraz daha değerli olabilir. Çünkü bugün bir doları bile bankaya yatırabilir, biraz faiz kazanabilir ve bundan bir yıl sonra biraz daha fazla paraya sahip olabilirsiniz. Finansal uygulamalar için, genellikle, bu iskonto faktörü, bugün bir dolarla karşılaştırdığımda gelecekte bir doların ne kadar az olduğunu temsil eder. Bazı somut getiri örneklerine bakalım. Aldığınız geri dönüş, ödüllere bağlıdır ve ödüller, yaptığınız eylemlere bağlıdır ve dolayısıyla geri dönüş, yaptığınız eylemlere bağlıdır. Her zamanki örneğimizi kullanalım ve bu örnek için diyelim ki, ben her zaman sola gideceğim. Robot 4 durumunda başlarsa, önceki slaytta hesapladığımız gibi dönüşün 12,5 olduğunu daha önce görmüştük. Üçte başlasaydı, getiri 25 olurdu çünkü 100 ödüle bir adım daha erken ulaşır ve bu nedenle daha az iskonto edilir. Durum 2'de başlasaydı, dönüş 50 olurdu. Yeni başlayıp durum 1 olsaydı, pekala, hemen 100'lük ödülü alır, yani düşük indirimli olmaz.
Durum 1'den başlarsak getiri 100 olur ve bu iki durumdaki dönüş 6.25 olur. Görünüşe göre, son durum olan 6 durumundan başlarsanız, sadece ödülü ve dolayısıyla 40'ın geri dönüşünü alırsınız. Şimdi, farklı bir eylemler dizisi yapacak olsaydınız, geri dönüşler aslında farklı olurdu. Örneğin, her zaman sağa gidersek, eylemlerimiz bunlar olsaydı, o zaman 4. durumda başlarsanız, 0 ödül alırsınız. Sonra 5. duruma gelirsiniz, 0 ödül alırsınız ve 6 durumuna gelir ve 40'lık bir ödül alır. Bu durumda getiri 0 artı 0,5 olur, iskonto faktörü çarpı 0 artı 0,5'in karesi çarpı 40 olur ve bu da 0,5'in karesinin 1/4 olduğu ortaya çıkar, yani 40'ın 1/4'ü 10'dur. Bu halden, 4. halden getirisi 10'dur. Harekete geçecekseniz daima sağa gidin. Benzer bir mantıkla bu durumdan getiri 20, bu durumdan getiri beş, bu durumdan getiri 2,5 ve sonra dönüş determinant durumu 140 olur. tamamen mantıklıysa, videoyu duraklatıp matematiği iki kez kontrol etmekten çekinmeyin ve bunların geri dönüş için uygun değerler olduğuna kendinizi ikna edip edemeyeceğinize bakın. Çünkü farklı hallerden yola çıkarsanız ve her zaman sağa gidecekseniz. Her zaman sağa gittiğini görüyoruz. Almayı beklediğiniz getiri çoğu eyalet için daha düşüktür. Belki de her zaman sağa gitmek, her zaman sola gitmek kadar iyi bir fikir değildir. Ama her zaman sola gitmek zorunda olmadığımız, her zaman sağa gittiğimiz ortaya çıktı. Ayrıca 2. durumda olup olmadığınıza da karar verebiliriz, sola gidin. Durum 3'teyseniz, sola gidin. 4. durumdaysanız sola gidin. Ancak 5. durumdaysanız, bu ödüle çok yakınsınız demektir. Sağa gidelim. Bu, içinde bulunduğunuz duruma göre yapılacak eylemleri seçmenin farklı bir yolu olacaktır. Farklı durumlardan alacağınız getiri 100, 50, 25, 12.5, 20 ve 40 olacaktır. Bir vaka. 5. durumda başlayacak olsaydınız, burada sağa giderdiniz ve böylece aldığınız ödüller önce 5 durumunda sıfır, sonra 4 olur. Getiri sıfırdır, ilk ödül artı indirim faktörü 0,5 çarpı 40 yani 20 yani burada gösterilen işlemleri yaparsanız bu durumdan 20 dönüş neden oluyor. Özetlemek gerekirse, takviyeli öğrenmedeki geri dönüş, sistemin aldığı ödüllerin indirim faktörü ile ağırlıklandırılmış toplamıdır; burada uzak gelecekteki ödüller, indirim faktörü tarafından daha yüksek bir güce yükseltilerek ağırlıklandırılır. Şimdi, negatif ödülleri olan sistemleriniz olduğunda bunun aslında ilginç bir etkisi var. İncelediğimiz örnekte, tüm ödüller sıfır veya pozitifti. Ancak herhangi bir ödül negatifse, o zaman indirim faktörü aslında sistemi negatif ödülleri olabildiğince geleceğe itmeye teşvik eder. Mali bir örnek alırsak, birine 10$ ödemek zorunda kalsaydınız, bu belki eksi 10'luk bir negatif ödüldür. faiz oranı aslında bugün ödemek zorunda olduğunuz 10 dolardan daha az değerdedir. Negatif ödülleri olan sistemler için, algoritmanın ödülleri olabildiğince geleceğe itmeye çalışmasına neden olur. Finansal uygulamalar ve diğer uygulamalar için, bu aslında sistemin yapması gereken doğru şey olarak ortaya çıkıyor. Takviyeli öğrenmede geri dönüşün ne olduğunu artık biliyorsunuz, takviyeli öğrenme algoritmasının hedefini formüle etmek için bir sonraki videoya geçelim.

## Making decisions: Policies in reinforcement learning
Bir pekiştirmeli öğrenme algoritmasının eylemleri nasıl seçtiğini resmileştirelim. Bu videoda, takviyeli öğrenme algoritmasında bir politikanın ne olduğunu öğreneceksiniz. Hadi bir bakalım. Gördüğümüz gibi, pekiştirmeli öğrenme probleminde harekete geçmenin birçok farklı yolu vardır. Örneğin, her zaman en yakın ödül için gitmeye karar verebiliriz, böylece bu en soldaki ödül daha yakınsa sola gidersiniz veya bu en sağdaki ödül daha yakınsa sağa gidersiniz. Eylemleri seçmemizin başka bir yolu da, her zaman daha büyük ödül için gitmektir veya her zaman daha küçük ödül için gidebiliriz, iyi bir fikir gibi görünmüyor, ancak bu başka bir seçenek veya sadece değilseniz sola gitmeyi seçebilirsiniz. daha az ödülden bir adım uzaklaşırsanız, bu durumda onu tercih edersiniz. Takviyeli öğrenmede amacımız, işi herhangi bir durumu girdi olarak almak ve bunu bizden yapmamızı istediği bazı eylemlere haritalamak olan, politika Pi adı verilen bir işlev bulmaktır. Örneğin, alttaki bu politika için, bu politika, eğer 2. durumdaysanız, bizi sol eyleme eşler. 3. durumdaysanız, politika sola gidin diyor. 4. durumdaysanız ayrıca sola gidin ve 5. durumdaysanız sağa gidin. S durumuna uygulanan Pi, bize bu durumda hangi eylemi yapmamızı istediğini söyler. Takviyeli öğrenmenin amacı, getiriyi en üst düzeye çıkarmak için her durumda hangi eylemi yapmanız gerektiğini size söyleyen bir Pi veya Pi of S politikası bulmaktır. Bu arada, politikanın pi'nin ne olduğunu en açıklayıcı terim olup olmadığını bilmiyorum ama pekiştirmeli öğrenmede standart hale gelen terimlerden biri. Belki Pi'ye bir politika yerine bir denetleyici demek daha doğal bir terminoloji olur, ancak politika, pekiştirmeli öğrenmedeki herkesin şimdi buna dediği şeydir. Son videoda, durumlardan ödüllere, getirilere ve politikalara kadar pek çok pekiştirmeli öğrenme konseptini inceledik. Bir sonraki videoda bunları hızlı bir şekilde gözden geçirelim ve ardından bu politikaları bulmak için algoritmalar geliştirmeye başlayacağız. Bir sonraki videoya geçelim.

## Review of key concepts
Altı durumlu Mars gezgini örneğini kullanarak bir pekiştirmeli öğrenme formalizmi geliştirdik. Anahtar kavramlara hızlıca bir göz atalım ve bu kavram setinin diğer uygulamalar için de nasıl kullanılabileceğini görelim. Tartıştığımız kavramlardan bazıları, takviyeli öğrenme probleminin durumları, eylemler dizisi, ödüller, bir indirim faktörü, ardından ödüllerin ve indirim faktörünün birlikte getiriyi hesaplamak için nasıl kullanıldığı ve son olarak, işi kimin yaptığı bir politikadır. getiriyi en üst düzeye çıkarmak için eylemleri seçmenize yardımcı olmaktır. Mars gezgini örneği için 1-6 arasında numaralandırdığımız altı durumumuz vardı ve eylemler sola veya sağa gitmek şeklindeydi. Ödüller en soldaki durum için 100, en sağdaki durum için 40 ve arada sıfırdı ve 0,5'lik bir indirim faktörü kullanıyordum. Geri dönüş bu formül tarafından verildi ve Pi'nin eylemleri tasvir ettiği, içinde bulunduğunuz duruma bağlı olarak farklı politikaları olabilir. Aynı biçimcilik veya durumlar, eylemler, ödüller vb. birçok başka uygulama için de kullanılabilir. Sorunu çözün veya otonom bir helikopter bulun. Bir durum belirlemek, helikopterin olası konumları, yönelimleri ve hızları vb. kümesi olacaktır. Olası eylemler, bir helikopterin kontrol çubuğunu hareket ettirmenin olası yolları olabilir ve ödüller, iyi uçuyorsa artı bir ve gerçekten kötü düşmüyorsa veya çarpmıyorsa eksi 1.000 olabilir. Helikopterin ne kadar iyi uçtuğunu size söyleyen ödül işlevi. İndirgeme faktörü, birden biraz daha küçük bir sayı, diyelim ki, 0.99 ve sonra ödüllere ve indirim faktörüne bağlı olarak, getiriyi aynı formülü kullanarak hesaplarsınız. Takviyeli bir öğrenme algoritmasının işi, helikopterin konumu olan girdi olarak verildiğinde, size hangi eylemi yapmanız gerektiğini söyleyen Pi'nin bir politikasını bulmak olacaktır. Yani kontrol çubuklarını nasıl hareket ettireceğinizi anlatır. İşte bir örnek daha. İşte oyun oynayan bir tane. Satranç oynamayı öğrenmek için pekiştirmeli öğrenmeyi kullanmak istediğinizi varsayalım. Bu sorunun durumu, tahtadaki tüm taşların konumu olacaktır. Bu arada, satranç oynuyorsanız ve kuralları iyi biliyorsanız, bunun satranç için sadece taşların konumundan biraz daha fazla bilgi olduğunu biliyorum ama bu video için biraz basitleştireceğim. Eylemler, oyundaki olası yasal hamlelerdir ve bu durumda, sisteminize bir oyunu kazanırsa artı bir, oyunu kaybederse eksi bir ve eğer oyunu kaybederse eksi bir ödül verirseniz, ortak bir ödül seçimi olacaktır. bir oyun bağlar. Satranç için, genellikle bire çok yakın bir indirim faktörü kullanılacaktır, bu nedenle belki 0,99, hatta 0,995 veya 0,999 olabilir ve geri dönüş, diğer uygulamalarla aynı formülü kullanır. Bir kez daha, hedefe bir politika Pi kullanarak iyi bir eylem seçmesi için bir tahta pozisyonu verilir. Bir pekiştirmeli öğrenme uygulamasının bu şekilciliğinin aslında bir adı vardır. Buna Markov karar süreci deniyor ve bunun kulağa büyük, teknik açıdan karmaşık bir terim gibi geldiğini biliyorum. Ancak bu Markov karar süreci veya kısaca MDP terimini duyarsanız, bu son birkaç videoda bahsettiğimiz biçimciliktir. MDP veya Markov karar sürecindeki Markov terimi, geleceğin mevcut duruma gelmeden önce gerçekleşmiş olabilecek herhangi bir şeye değil, yalnızca mevcut duruma bağlı olduğunu ifade eder. Başka bir deyişle, bir Markov karar sürecinde gelecek, buraya nasıl geldiğinize değil, yalnızca şu anda nerede olduğunuza bağlıdır. Markov karar süreci formalizmini düşünmenin bir başka yolu da, kontrol etmek istediğimiz bir robotumuz veya başka bir aracımız var ve yapacağımız şey a eylemlerini seçmek ve bu eylemlere dayalı olarak dünyada veya dünyada bir şeyler olacak. çevre, örneğin dünyadaki konumumuz değişiyor ya da bir kaya parçasını örnek alıp bilim görevini yerine getiriyoruz. A eylemini seçme şeklimiz Pi politikasıdır ve dünyada olup bitenlere bağlı olarak hangi durumda olduğumuzu ve hangi ödülleri aldığımızı görebilir veya gözlemleyebiliriz. Bazen farklı yazarların Markov karar sürecini veya MDP biçimciliğini temsil etmek için bunun gibi bir diyagram kullandığını görürsünüz, ancak bu, son birkaç videoda öğrendiğiniz kavramlar dizisini göstermenin başka bir yoludur. Artık pekiştirmeli öğrenme probleminin nasıl çalıştığını biliyorsunuz. Bir sonraki videoda, iyi eylemler seçmek için bir algoritma geliştirmeye başlayacağız. Buna yönelik ilk adım, durum eylem değeri işlevini tanımlamak ve sonunda hesaplamayı öğrenmek olacaktır. Bu, bir öğrenme algoritması geliştirmek istediğimizde kilit niceliklerden biri olarak ortaya çıkıyor. Bunun ne olduğunu görmek için bir sonraki videoya geçelim, durum eylem değeri fonksiyonu.


## State-action value function definition
Bu hafta ilerleyen saatlerde takviyeli öğrenmeyi geliştirmeye başladığımızda, takviyeli öğrenme oklarının hesaplamaya çalışacağı önemli bir miktar olduğunu görüyorsunuz ve buna durum eylem değeri fonksiyonu deniyor. Şimdi bu fonksiyonun ne olduğuna bir göz atalım. Durum eylem değeri işlevi, tipik olarak büyük Q harfiyle gösterilen bir işlevdir. Ve bu, içinde olabileceğiniz bir durumun yanı sıra o durumda gerçekleştirmeyi seçebileceğiniz eylemin ve QFSA'nın bir işlevidir. Dönüşe eşit bir sayı verecektir. Bu durumda başlarsanız. S ve A eylemini yalnızca bir kez gerçekleştirin ve A eylemini bir kez gerçekleştirdikten sonra, bundan sonra en uygun şekilde davranın. Bundan sonra, mümkün olan en yüksek getiriyi sağlayacak her türlü eylemi yaparsınız. Şimdi, bu tanımda biraz garip bir şeyler olduğunu düşünebilirsiniz çünkü optimal davranışın ne olduğunu nasıl bilebiliriz? Ve otomatik davranışın ne olduğunu bilseydik, her durumda yapılacak en iyi eylemin ne olduğunu zaten bilseydik, neden hala Q of SA'yı hesaplamamız gerekiyor? Çünkü zaten otomatik politikamız var. Bu yüzden, bu tanımda biraz garip bir şey olduğunu kabul etmek istiyorum. Bu tanımla ilgili neredeyse biraz döngüsel bir şey var, ancak emin olun Belirli pekiştirmeli öğrenme çıktılarına baktığımızda, daha sonra bu biraz döngüsel tanımı çözeceğiz ve Q işlevini daha biz bulmadan önce hesaplamanın bir yolunu bulacağız. optimal politika. Ama bunu daha sonraki bir videoda görüyorsunuz. O yüzden şimdilik bu konuda endişelenme. Daha önce gördüğümüz bir örneğe bakalım, bu oldukça iyi bir politika 2., 3. ve 4. aşamadan sola gidin ve Beşinci Devletten sağa gidin. Bunun mars rover uygulaması için aslında en uygun politika olduğu ortaya çıktı İndirgeme faktörü gamma 0,5 olduğunda, yani S'nin Q'su toplam getiriye eşit olacaktır. daha sonrasında. Anlamı bu politikaya göre hareket etmek. Burada gösterildiği gibi, Q of s,a'nın ne olduğunu bulalım. Birkaç farklı eyalet içindir. Diyelim ki Q of state'e de bakalım. Peki ya sağa gitmek için harekete geçersek, eğer siz ikinci durumdaysanız ve sağa giderseniz, sonra üçüncü duruma gelirsiniz ve bundan sonra en uygun şekilde davranırsanız, ST üçten sola ve sonra sola gidersiniz. eyaletten duruma ve sonunda 100'lük ödülü alırsınız. Bu durumda, aldığınız ödüller eyaletten sıfıra, ikinci duruma geri döndüğünüzde üç sıfır kaldığınızda ve sonra nihayet geldiğinizde 100 olacaktır. uç durum bir ve dönüş sıfır artı 0,5 çarpı bunun artı 0,5 kare çarpı ac artı 0,5 küp çarpı 100 olacaktır. Bunun geçtiğini unutmayın, doğru gitmenin iyi bir fikir olup olmadığına dair bir yargı yoktur. Aslında, durumdan sağa gitmek o kadar iyi bir fikir değil, ancak A eylemini gerçekleştirirseniz ve ardından en uygun şekilde davranırsanız, dönüşü sadakatle bildirir. İşte başka bir örnek. Durumdaysanız ve sola gidecekseniz, ikinci durumdayken alacağınız ödül dizisi sıfır olacak ve ardından 100 olacaktır. Ve böylece dönüş sıfır artı 0,5 çarpı 100, bu da 50'ye eşittir QSA değerlerini yazmak için. Bu şemada, sağa gidişin Q olduğunu göstermek için sağa 12.5 yazacağım. Ve sonra, bunun ST 2'nin Q'su olduğunu göstermek için sola biraz 50 yazdığımda ve sadece bir örnek daha almak için sola gittiğimde Peki ya ST 4'teysek ve sola gitmeye karar verirsek? Dördüncü aşamadaysanız sola gidersiniz, sıfır ödül alırsınız ve sonra burada sola doğru harekete geçersiniz. Yani sıfır kazanç, burada sol harekete geç, sıfır ve sonra 100. Yani dört Soldan Q, sıfır ödülle sonuçlanır çünkü ilk eylem kaldı ve ardından daha sonra en uygun politikayı izlediğimiz için 00 100 ödüllendirebilirsiniz. Ve böylece dönüş sıfırdır. artı bunun 00,5 katı. Artı 4,5 kare çarpı bunun artı 0,5 Q çarpı bunun. Bu nedenle 12.5'e eşittir. Yani Q4 kaldı 12.5. Bu yılı 12.5 olarak yazacağım. Ve bu alıştırmayı diğer tüm durumlar ve diğer tüm eylemler için yaparsanız, bunun s,a'nın Q'su olduğunu görürsünüz. Farklı durumlar ve farklı eylemler için Ve son olarak Terminal Durumunda. Ne yaptığınız önemli değil, sadece 100 veya 40 terminal ödülü alırsınız. O yüzden terminal ödüllerini buraya yazın. Yani bu Q(s,a)'dır. Birden altıya kadar her durum durumu için ve iki eylem için, eylem sol ve eylem sağ. Durum eylem değeri işlevi neredeyse her zaman Q harfiyle belirtildiği için. Buna genellikle Q işlevi de denir. Dolayısıyla, Q. işlevi ve durum eylem değeri işlevi birbirinin yerine kullanılır ve size getirilerinizin ne olduğunu veya gerçekten değerin ne olduğunu söyler. ne kadar iyi? Sadece A ve ST S eylemlerini gerçekleştirin ve ardından en uygun şekilde davranın. Şimdi, Q işlevini bir kez hesaplayabildiğiniz zaman, bunun size eylemleri seçmek için de bir yol vereceği ortaya çıktı. İşte politika ve iade. Ve işte s,a'nın iki değeri. Önceki slayttan. Farklı durumlara baktığınızda ilginç bir şey fark ediyorsunuz, o da soldaki eylemi yapmak için durum alırsanız a,q ile sonuçlanır. Değer veya durum eylem değeri 50'dir ki bu aslında o durumdan alabileceğiniz en olası getiridir. Üç iki s,a durumunda. çünkü soldaki işlem aynı zamanda size daha yüksek getiri sağlar, bu nedenle soldaki işlem size istediğiniz getiriyi verir. Ve beşinci durumda, aslında size 20'lik daha yüksek getiriyi sağlayan, sağa giden eylemdir. Böylece, herhangi bir S durumundan mümkün olan en iyi getiri olduğu ortaya çıktı. Q,F, S'nin en büyük değeridir. Bunun net olduğundan emin olmak için söylediğim şu ki, kal için kal Dört durumundan ikisi kaldı ki bu da 12.5 Ve q. Bu da 10 olur. Ve bu iki değerden büyük olan 12.5, bu durumdaki dörtten mümkün olan en iyi getiridir. Başka bir deyişle, Dördüncü Durumdan almayı umabileceğiniz en yüksek getiri 12,5'tir. Ve aslında bu iki sayıdan 12,5 ve 10'dan daha büyük olanıdır. Ayrıca, Mars Rover'ınızın 10 demek yerine 12,5'lik bir dönüşün keyfini çıkarmasını istiyorsanız, yapmanız gereken eylem A eylemidir. Q s,a. Yani mümkün olan en iyi eylem durumu, eylem A'dır. Bu aslında Q, of s,a'yı maksimize eder. Bu size Q, of s,a'nın neden hesaplandığına dair bir ipucu verebilir. Daha sonra inşa edilecek pekiştirmeli öğrenme algoritmasının önemli bir parçasıdır. Yani Q(s,a)'yı hesaplamak için bir yolunuz varsa. Her durum ve her eylem için, bazı durumlardayken tek yapmanız gereken farklı A eylemlerine bakmaktır. Ve A eylemini seçin. Bu, Q of s,a'yı maksimize eder. Ve böylece pi F. S sadece A eylemini seçebilir. Bu, s,a'nın en büyük Q değerini verir. Ve bu iyi bir eylem olacak. Aslında en uygun eylem olduğu ortaya çıktı. Bunun neden mantıklı olduğuna dair bir başka sezgi de Qof s,a'dır. Eğer ani bir durum ve eyleme geçerseniz döndürülür A. Ve bundan sonra en uygun şekilde davranın. Yani mümkün olan en yüksek getiriyi elde etmek için, gerçekten istediğiniz şey A eylemini gerçekleştirmektir. Bu, en yüksek toplam getiriyle sonuçlanır. Bu nedenle, keşke Q f s,a'yı hesaplamanın bir yolu olsa. Bu koşullar altında getiriyi en üst düzeye çıkaran eylem yardımını alan her devlet için, o eyalette yapılacak en iyi eylem gibi görünüyor. Bu, bunun için bilmeniz gereken bir şey olmasa da. Çünkü şunu da belirtmek isterim ki internete bakarsanız veya pekiştirmeli öğrenme literatürüne bakarsanız bazen bu Q fonksiyonunun Q olarak yazıldığını görürsünüz. Bu terimler, tam olarak tanımladığımız gibi Q işlevine atıfta bulunur. Pekiştirmeli öğrenme literatürüne bakarsanız ve Q. Star veya Q fonksiyonu hakkında okursanız, bu sadece bahsettiğimiz durum eylem değeri fonksiyonu anlamına gelir. Ancak bu kursun amaçları açısından bunun için endişelenmenize gerek yok. Özetlemek gerekirse, Q of s,a'yı hesaplayabilirseniz. Her durum ve her eylem için, bu bize S'nin oto politikası pi'yi hesaplamak için iyi bir yol verir. Yani bu, durum eylem değeri işlevi veya Q işlevidir. Q fonksiyonunun tanımının biraz dairesel yönüne rağmen bunları hesaplamak için nasıl bir algoritma bulacağımız hakkında daha sonra konuşacağız. Ama önce bir sonraki videoda Q of s,a değerlerinin ne olduğuna dair bazı özel örneklere bakalım. aslında benziyor


## State-action value function example
Durum-eylem değeri işlevi örneğini kullanma. QSA değerlerinin nasıl olduğunu görüyorsunuz. Takviyeli öğrenme problemleri ve probleme bağlı olarak QSA değerlerinin nasıl değiştiği konusundaki sezgilerimizi sürdürmek için isteğe bağlı bir laboratuvar sağlanacaktır. Bu, [DUYULMUYOR] örneğini değiştirerek oynamanıza ve QSA'nın nasıl değişeceğini kendiniz görmenize olanak tanır. Hadi bir bakalım. İşte bu videoyu izledikten sonra oynamanızı umduğum bir Jupyter Notebook. Bu yardımcı işlevleri çalıştıracağım, şimdi burada dikkat edin, bu iki eylemin altı sayısını belirtiyor, böylece bunları değiştirmeyeceğim. Ve bu, 140 olan terminal sağ ödüllerinde sol terminali belirtir ve daha sonra ara durumların ödülleri sıfırdır. İndirim faktörü kumar 0.5. Şimdilik yanlış adım atma olasılığını göz ardı edelim, bundan sonraki bir videoda bahsedeceğiz. Ve bu değerlerle, bu kodu çalıştırırsanız bu, optimal politikanın yanı sıra SA'nın Q işlevini hesaplar ve görselleştirir. Q of SA'yı kendiniz tahmin etmek veya hesaplamak için bir öğrenme algoritmasının nasıl geliştirileceğini daha sonra öğreneceksiniz. Şimdilik Q of SA'yı hesaplamak için hangi kodu yazdığımızı dert etmeyin. Ama burada Q of SA değerlerinin derste gördüğümüz değerler olduğunu görüyorsunuz. Şimdi eğlence burada başlıyor. Bazı değerleri değiştirelim ve bu şeylerin nasıl değiştiğini görelim. Terminal sağ ödülünü çok daha küçük bir değere güncelleyeceğim sadece 10 diyor. Şimdi kodu tekrar çalıştırırsam, Q of SA'nın nasıl değiştiğine bakın ve şimdi 5 durumundaysanız bunu düşünür. ve en uygun şekilde davranın, 6.25 alırsınız. Oysa sağa giderseniz ve bundan sonra da davranırsanız, yalnızca beşlik bir getiri elde edersiniz. Şimdi sağdaki ödül çok küçükken, sadece 10. Size çok yakın olsanız bile, sonuna kadar sola gitmeyi tercih edin. Ve aslında otomobil politikası artık her eyaletten sola gitmek. Başka değişiklikler yapalım. Terminal hakkı ödülünü tekrar 40 olarak değiştireceğim. Ama indirim faktörünü bire yakın bir indirim faktörü ile 0.9 olarak değiştireyim. Bu, Mars Gezgini'ni daha az sabırsız yapar, daha yüksek bir ödül için daha uzun süre dayanmaya isteklidir çünkü gelecekte ödüller 0,5 ile çarpılmaz - bazı yüksek güçler 0,9 ile bazı yüksek güçlerle çarpılır. Ve daha fazla sabırlı olmaya isteklidir, çünkü gelecekte ödüller iskonto edilmez veya indirim 0,5 iken olduğu kadar küçük bir sayı ile çarpılmaz. Öyleyse kodu tekrar çalıştıralım. Ve şimdi bunun farklı eyaletler için Q of SA olduğunu görüyorsunuz ve şimdi durum 5 için sola gitmek size 36'ya kıyasla 65,61 gibi daha yüksek bir ödül veriyor. Bu arada 36'nın 40'lık bu terminal ödülünün 0,9 katı olduğuna dikkat edin. mantıklı olmak. Ama küçük bir hasta sola gitmeye istekli olduğunda, siz 5 durumundayken bile. Şimdi gammayı 0 .3 gibi çok daha küçük bir sayıya değiştirelim. Yani bu, gelecekte ödüllerde çok ağır indirimler yapar. Bu onu inanılmaz derecede sabırsız yapar. Bu kodu tekrar çalıştırmama izin verin ve şimdi davranış değişti. Şu anda 4. durumda olanın daha büyük 100 ödül için gidecek sabrı olmayacağı fark edildi, çünkü indirim faktörü gamma artık çok küçük, 0,3. Çok daha küçük bir ödül daha yakın olmasına rağmen 40'lık ödülü tercih eder ve biz de bunu yapmayı seçeriz. Umarım bu sayılarla kendiniz oynayarak ve bu kodu çalıştırarak bir fikir edinebilirsiniz. Q of SA değerleri nasıl değişir, fark ettiğiniz optimal getiri bu iki QSA sayısından daha büyük olanıdır. Bunun nasıl değiştiği ve optimal politikanın nasıl değiştiği. Bu yüzden umarım gidip isteğe bağlı laboratuvarla oynarsınız ve ödül işlevini değiştirirsiniz ve indirim faktörü gama'yı değiştirir ve farklı değerler denersiniz. Ve Q of SA değerlerinin nasıl değiştiğini, farklı durumlardan optimal getirilerin nasıl değiştiğini ve bu farklı değerlere bağlı olarak otomobil politikasının nasıl değiştiğini kendiniz görün. Ve bunu yaparak, pekiştirmeli öğrenme uygulamasında ödüllere bağlı olarak bu farklı miktarların nasıl etkilendiğine dair sezginizi keskinleştireceğinizi umuyorum. Siz laboratuvarda oynadıktan sonra, geri gelip takviyeli öğrenmede muhtemelen en önemli denklemin ne olduğu hakkında konuşmaya hazır olacağız, buna bellman denklemi denir. Umarım isteğe bağlı laboratuvarda eğlenirsiniz ve ondan sonra hadi geri gelip kapıcı denklemleri hakkında konuşalım.

## Bellman Equations
Nerede olduğumuzu özetleyeyim. Q of S, A durum eylem değeri işlevini hesaplayabilirseniz, bu size her sahneden iyi bir eylem seçmenin bir yolunu sunar. Sadece size en büyük Q of S,A değerini veren A eylemini seçin. Soru şu ki, bu Q of S,A değerlerini nasıl hesaplarsınız? Takviyeli öğrenmede, durum eylem değeri fonksiyonunu hesaplamamıza yardımcı olacak Bellman denklemi adı verilen bir anahtar denklem vardır. Şimdi bu denklemin ne olduğuna bir göz atalım. Bir hatırlatma olarak, bu Q of S, A'nın tanımıdır. S durumunda başlarsak, bir kez harekete geçersek ve bundan sonra en uygun şekilde davranırsak geri döner. Bellman denklemini açıklamak için aşağıdaki gösterimi kullanacağım. Mevcut durumu belirtmek için S kullanacağım. Şimdi, mevcut durumun ödüllerini belirtmek için R of S'yi kullanacağım. Küçük MDP örneğimiz için, bir Durum1'in r'sinin 100 olduğunu alacağız. Durum 2'nin ödülü 0'dır, vb. Durum 6'nın ödülü 40'tır. Mevcut eylemi, S durumunda gerçekleştirdiğiniz eylemi belirtmek için A alfabesini kullanacağım. a eylemini gerçekleştirdikten sonra, yeni bir duruma geçersiniz. Örneğin, Durum 4'teyseniz ve soldaki eylemi yaparsanız, Durum 3'e ulaşırsınız. Mevcut durum S'den a eylemini yaptıktan sonra ulaştığınız durumu belirtmek için S üssünü kullanacağım. Ayrıca A üssünü S üssü durumunda gerçekleştirebileceğiniz eylemi, ulaştığınız yeni sabiti belirtmek için kullanacağım. Bu arada gösterim kuralı, S,A'nın mevcut duruma ve eyleme karşılık gelmesidir. Asal sayıyı eklediğimizde, bu bir sonraki durum, ardından bir sonraki eylemdir. Bellman denklemi aşağıdaki gibidir. Q(S,A), yani r(S)'ye eşit olan bu varsayımlar dizisi altındaki getiri, bu durumda olduğunuz için aldığınız ödül artı iskonto faktörü Gamma çarpı tüm olası eylemler üzerinden maksimum, a üssü q S üssünün, ulaştığınız yeni durum ve ardından bir asal sayı. Bu denklemde çok şey oluyor. Önce bazı örneklere bir göz atalım. Bu denklemin neden anlamlı olabileceğini görmek için geri geleceğiz. Bir örneğe bakalım. Q of State 2'ye ve eyleme bakalım. Bize hangi değeri verdiğini görmek için buna Bellman Denklemini uygulayın. Mevcut durum ikinci durumsa ve eylem sağa gidecekse, o zaman ertesi gün S üssünü yazdıktan sonra Durum 3 olur. Bellman denklemi Q/2 der, sağ R(S)'dir. R Durumu 2, yalnızca ödül sıfır artı bu örnekte 0,5 olarak ayarladığımız Gamma indirim faktörü, Durum 3'teki S durumundaki Q değerlerinin maksimum katıdır. Bu, 25 ve 6.25'in maksimumu olacak , çünkü bu maksimum bölü q'nun bir üssü S üssü virgül a üssüdür. Bu, 25 veya 6.25'ten büyük olanı alıyor, çünkü bunlar Durum 3 için iki seçenek. Sadece bir örneğe daha bakalım. Durum 4'ü ele alalım ve sola gitmeye karar verirseniz, Durum 4'ün Q'sunun ne olduğunu görelim. Bu durumda, mevcut durum dört geçerli eylem sola gitmektir. Bir sonraki durum, eğer dörtten başlayabilirseniz sola gidiyor. Aynı zamanda Durum 3'te olursunuz. Bu üçü tekrar asal yapalım, Bellman Denklemi, bunun R(S)'ye eşit olduğunu söyleyeceğiz. Durum dört, sıfır artı 0.5 iskonto faktörü Gamma bölü a üssü q S üssü. Bu yine Durum 3, virgül ve asal. Bir kez daha, Durum 3 için Q değerleri 25 ve 6.25'tir ve bunların büyüğü 25'tir. Bu bizim 40 artı 0.5 çarpı 25 olur, bu da yine 12.5'e eşittir. Bu nedenle, kalan işlemle birlikte q/dört de 12,5'e eşittir, sadece bir nota, eğer uç durumdaysanız, o zaman Bellman Denklemi q(SA) eşittir r(S) şeklinde sadeleşir çünkü S üssü durumu yoktur ve böylece ikinci dönem gider. Bu nedenle Q of S,A terminal durumlarında sadece 100, 100 veya 40 40'tır. Dilerseniz videoyu duraklatabilir ve Bellman Denklemini bu MDP'deki diğer herhangi bir durum eylemine uygulayabilir ve bu matematiğin doğru olup olmadığını kendiniz kontrol edebilirsiniz. çalışır. Özetlemek gerekirse, Q of S,A'yı bu şekilde tanımlamıştık. Herhangi bir S durumundan mümkün olan en iyi dönüşün, S,A'nın bir Q'su üzerinden maksimum olduğunu daha önce görmüştük. Aslında, SNA'yı yeniden adlandırmak gerekirse, bir S asal durumundan mümkün olan en iyi getiri, bir asalın maks bölü S üssüdür. S, S üssünü ve a'yı asal olarak yeniden adlandırmaktan başka bir şey yapmadım. Ancak bu, daha sonra bazı sezgileri biraz daha kolaylaştıracaktır.
Ancak, Durum 3 gibi herhangi bir S üssü durumu için, örneğin Durum 3'ten mümkün olan en iyi getiri, Q'nun S üssü E üssünün tüm olası eylemlerinin maksimumudur. İşte yine Bellman denklemi. Bunun yakaladığı sezgi, eğer s durumundan başlıyorsanız ve a eylemini yapacaksanız ve ardından bundan sonra en uygun şekilde hareket edecekseniz, o zaman zaman içinde bir dizi ödül göreceksiniz. Özellikle getiri, birinci adımdaki ödül artı ikinci adımdaki Gama çarpı ödül artı üçüncü adımdaki Gama kare çarpı ödül vb. kullanılarak hesaplanacaktır. Artı nokta, nokta, nokta terminal durumuna gelene kadar. Bellman denkleminin söylediği şey, bu ödül dizisinin, indirim faktörünün ne olduğu, iki bileşene ayrılabileceğidir. Birincisi, bu R of s, hemen alacağınız ödül. Takviyeli öğrenme literatüründe buna bazen anlık ödül de denir, ancak R_1 budur. Bazı eyaletlerde başladığınız için aldığınız ödül. O halde ikinci terim şu şekildedir; s durumunda başlayıp a eylemini gerçekleştirdikten sonra, bazı yeni s asal durumlarına ulaşırsınız. Q of s a'nın tanımı, bundan sonra optimal davranacağımızı varsayar. s asal değerine ulaştıktan sonra, optimal davranacağız ve s asal durumundan mümkün olan en iyi getiriyi alacağız. Bu nedir, a üssünün maksimumu Q (s üssü) a üssü, bu s üssü durumundan başlayarak optimal davranmanın getirisidir. Tam olarak burada yazdığımız şey, eyaletin en iyi noktasından başladığınızda mümkün olan en iyi getiridir. Bunu ifade etmenin başka bir yolu, buradaki toplam getiri de R_1 artıya eşittir ve sonra haritada Gamma'yı çıkaracağız, yani Gamma çarpı R_2 artı ve Gamma kare yerine Gama çarpı R_3 artı Gama kare çarpı R_4 artı nokta nokta nokta. Prime durumundan başlıyorsanız, alacağınız ödül sırasının R_2, R_3, ardından R_4, vb. olacağına dikkat edin. Bu yüzden buradaki ifade, s asal durumundan başlarsanız toplam getiridir. Optimal şekilde davranacak olsaydınız, o zaman bu ifade, s üssü durumundan başlamak için mümkün olan en iyi getiri olmalıdır, bu nedenle bu indirgeme ödülleri dizisi, a üssünün Q (s üssü) a üssünün maksimum değerine eşittir ve arta kalanlar da vardır. bu ekstra indirgeme faktörü Gamma, bu yüzden Q(s,a) buradaki ifadeye de eşit. Bunun oldukça karmaşık olduğunu düşünüyorsanız ve tüm detayları takip etmiyorsanız, endişelenmeyin. Bu denklemi uyguladığınız sürece doğru sonuçlara ulaşmayı başaracaksınız. Ama üst düzey sezgiyi, pekiştirmeli öğrenme probleminde elde edeceğiniz toplam geri dönüşün iki bölümden oluştuğu şeklindedir. İlk kısım, hemen aldığınız bu ödüldür ve ardından ikinci kısım, Gama çarpı bir sonraki asal durumdan başlayarak elde ettiğiniz getiridir. Bu iki bileşen birlikte olduğu için, R(s) artı Gamma çarpı bir sonraki durumdan getiri, yani mevcut durum s'den toplam getiriye eşittir. Bellman denkleminin özü budur. Bunu daha önceki örneğimiz Q of 4 ile ilişkilendirmek için sola. Durum 4'ü başlatmanın ve sola gitmenin toplam getirisi budur. Durum 4'te sola giderseniz, aldığınız ödüller Durum 4'te 0, Durum 3'te 0, Durum 2'de 0 ve ardından 100'dür, bu nedenle toplam getiri budur; 0,5'in karesi artı 0,5'in küpü, yani 12,5'ti. Bellman denkleminin söylediği şey, bunu iki parçaya ayırabileceğimizdir. Bu sıfır var, bu da dört durumunun R'si ve sonra artı 0,5 çarpı bu diğer dizi, 0 artı 0,50 artı 0,5 kare çarpı 100. Ama bu dizinin ne olduğuna bakarsanız, bu gerçekten bir sonrakinden en uygun getiri. eyalet 4'ten sola doğru eylemi gerçekleştirdikten sonra ulaştığınız asal durum. Bu nedenle bu, ödül 4 artı Durum 3'ten elde edilen optimum getirinin 0,5 katına eşittir. Çünkü, Durum 3'ten başlayacak olsaydınız, alacağınız ödüller sıfır, ardından sıfır ve ardından 100 olurdu, yani bu Durumdan en uygun getiridir. 3 ve bu yüzden bu sadece R(4) artı 0.5 max bölü Durum 3'ün bir asal Q'su, yani bir asal. Bellman denklemini biliyorum, bu, toplam getirilerinizi hemen alacağınız ödüle bölen biraz karmaşık bir denklem. Anlık ödül artı Gama çarpı bir sonraki asal durumun getirileri. Size mantıklı geliyorsa, ancak tam olarak değilse, sorun değil. Endişelenme. Takviyeli bir öğrenme algoritmasının doğru çalışmasını sağlamak için Bellman'ın denklemlerini yine de uygulayabilirsiniz, ancak en azından ödülleri neden hemen elde ettiklerinize ve gelecekte elde edeceklerinize ayırdığınıza dair yüksek düzeyde bir sezgi olduğunu umuyorum. Umarım bu mantıklıdır. Takviyeli bir öğrenme algoritması geliştirmeye geçmeden önce, Stokastik Markov karar süreçleri veya eylemlerin, eğer yaparsanız, biraz rastgele bir etkiye sahip olabileceği takviyeli öğrenme uygulamaları hakkında isteğe bağlı bir video ile karşınızdayız. Dilerseniz isteğe bağlı videoya göz atın. Ondan sonra, bir pekiştirme dosyası geliştirmeye başlayacağız.

## Random (stochastic) environment
Bazı uygulamalarda, bir işlem yaptığınızda, sonuç her zaman tam olarak güvenilir olmayabilir. Örneğin, Mars gezicinize sola gitmesini emrederseniz, belki biraz kaya kayabilir veya zemin gerçekten kaygandır ve bu nedenle kayarak yanlış yöne gidebilir. Uygulamada, birçok robot, rüzgarın esmesi ve rotadan çıkması ve tekerleğin kayması veya başka bir şey nedeniyle her zaman onlara söylediklerinizi tam olarak yapmayı başaramaz. Şimdiye kadar bahsettiğimiz, rastgele veya stokastik ortamları modelleyen takviyeli öğrenme çerçevesinin bir genellemesi var. Bu isteğe bağlı videoda, basitleştirici Mars Rover örneğimizle devam ederek, bu pekiştirmeli öğrenme problemlerinin nasıl çalıştığından bahsedeceğiz, diyelim ki harekete geçtiniz ve ona sola gitmesini emrettiniz. Çoğu zaman başarılı olursunuz, ancak ya zamanın yüzde 10'u ya da zamanın 0,1'i kazara kayarak ters yöne giderse? Sola gitmesini emrederseniz, sola doğru gitme şansı yüzde 90 veya 0,9'dur. Ama gerçekten sağa gitme şansı 0,1, yani bu örnekte yüzde 9 ihtimalle üçüncü durumda ve yüzde 10 ihtimalle beşinci durumda bitiyor. Tersine, sağa gitmesini ve harekete geçmesini emrederseniz, doğru, 0.9 ihtimalle beşinci duruma ve 0.1 ihtimalle üçüncü duruma gelir. Bu, stokastik bir ortama bir örnek olacaktır. Bakalım bu pekiştirmeli öğrenme probleminde neler oluyor. Diyelim ki burada gösterilen bu politikayı kullanıyorsunuz, 2, 3, 4. aşamalarda sola gidip sağlara gidiyorsunuz veya beşinci durumda sağa gitmeye çalışıyorsunuz. Dördüncü eyaletten başlayacaksanız ve bu politikayı izleyecekseniz, o zaman ziyaret ettiğiniz eyaletlerin gerçek sırası rastgele olabilir. Örneğin, dördüncü durumda, sola gideceksiniz ve belki döngünüz ve şanslısınız ve aslında üçüncü durumu alıyor ve sonra tekrar sola gitmeye çalışıyorsunuz ve belki gerçekten oraya varıyor. Tekrar sola gitmesini söylüyorsunuz ve o duruma geliyor. Eğer olan buysa, sonunda 000100 ödül dizisine sahip olursunuz. Ancak aynı politikayı ikinci kez denerseniz, burada ikinci kez başladığınızda belki biraz daha az şanslı olursunuz. Sola gitmeye çalışın ve başarılı olduğunu görün, böylece dördüncü durumdan sıfır, üçüncü durumdan sıfır, sola gitmesini söylediğinizi duyun, ancak bu sefer şanssızsınız ve robot kayar ve bunun yerine dördüncü duruma geri döner. Sonra size sola, sola ve sola seslenmeniz ve sonunda 100'lük ödüle ulaşmanız öğretilir. Bu durumda, gözlemlediğiniz ödüllerin sırası bu olacaktır. Bu dörtten üçe dörtten üçe iki sonra bir, hatta mümkün, eğer dördüncü eyaletten sola gitmesini söylerseniz, ilk adımda bile şanssız olabilirsiniz ve sonunda kaydığı için beşinci duruma gidersiniz. . Sonra beşi belirtin, sağa gitmesini emredersiniz ve siz buraya geldiğinizde başarılı olur. Bu durumda, gördüğünüz ödül dizisi 0040 olacaktır, çünkü dörtten beşe gitti ve sonra altı diyor, daha önce dönüşü bu indirimli ödüllerin toplamı olarak yazmıştık. Ancak takviyeli öğrenme problemi stokastik olduğunda, kesin olarak gördüğünüz bir ödül dizisi yoktur, bunun yerine bu farklı ödüller dizisini görürsünüz. Stokastik pekiştirmeli bir öğrenme probleminde, ilgilendiğimiz şey getiriyi maksimize etmek değildir çünkü bu rastgele bir sayıdır. Bizim ilgilendiğimiz şey, indirimli ödüllerin toplamının ortalama değerini maksimize etmektir. Ortalama değerden kastım, poliçenizi alıp bin kez, 100.000 kez veya bir milyon kez denerseniz, bunun gibi birçok farklı ödül dizisi elde edersiniz ve bunların hepsinin ortalamasını alırsanız. indirimli ödüllerin toplamının farklı dizileri, o zaman buna beklenen getiri diyoruz. İstatistikte, beklenen terimi, ortalama demenin başka bir yoludur. Ancak bunun anlamı, indirimli ödüllerin toplamı açısından ortalama olarak almayı umduğumuz şeyi en üst düzeye çıkarmak istiyoruz. Bunun matematiksel gösterimi, bunu E olarak yazmaktır. E, R1 artı Gamma R2 artı'nın beklenen değeri anlamına gelir, vb. Takviyeli öğrenme algoritmasının işi, indirimli ödüllerin ortalamasını veya beklenen toplamını en üst düzeye çıkarmak için bir politika Pi seçmektir. Özetlemek gerekirse, bir stokastik takviyeli öğrenme probleminiz veya bir stokastik Markov karar süreciniz olduğunda amaç, beklenen getiriyi en üst düzeye çıkarmak için S durumunda hangi eylemi yapacağımızı bize söyleyen bir politika seçmektir. Bunun en son değişme şekli, bahsettiğimiz şey, Bellman denklemini biraz değiştirmesidir. Bellman denklemi aynen yazdığımız gibi. Ama şimdi fark şu ki, s durumunda a eylemini gerçekleştirdiğinizde, bir sonraki s asal durumunun rastgele olması. 3. durumdayken ve ona bir sonraki s üssünün soluna gitmesini söylediğinizde, bu 2. durum olabilir veya 4. durum olabilir.
S üssü artık rastgele, bu yüzden buraya ortalama bir işleç veya beklenmeyen bir işleç de koyduk. s durumundan elde edilen toplam getiri, a eylemini bir kez en iyi şekilde gerçekleştirerek, hemen aldığınız ödüle eşittir, aynı zamanda anında ödül artı indirim faktörü, Gama artı ortalama olarak almayı beklediğiniz değer olarak da adlandırılır. gelecek döner. Bu stokastik pekiştirmeli öğrenme problemlerinde ne olduğu konusunda sezginizi keskinleştirmek istiyorsanız. Az önce size gösterdiğim isteğe bağlı laboratuvara geri dönerdiniz, burada bu parametre yanlış adım problemi, Mars Rover'ınızın ona emrettiğinizden ters yönde gitme olasılığıdır. İkinci adım yanlış adım 0,1 dersek ve Not Defterini yeniden çalıştırırsak ve böylece buradaki bu sayılar en uygun getiridir, eğer mümkün olan en iyi eylemleri yapacaksanız, bu optimal politikayı uygulayın, ancak robot yanlış yöne adım atacaksa yüzde 10 zamanın ve bunlar, bu stokastik NTP için q değerleridir. Robotu eskisi kadar iyi kontrol edemediğiniz için bu değerlerin artık biraz daha düşük olduğuna dikkat edin. q değerleri ve optimal getiriler biraz düştü. Aslında, yanlış adım atma olasılığını artıracak olursanız, örneğin robotun yönlere gitmediği zamanın yüzde 40'ını söyleyin. Zamanın sadece yüzde 60'ına emrediyorsun. Söylediğiniz yere gider, sonra robot üzerindeki kontrol dereceniz düştüğü için bu değerler daha da düşer. İsteğe bağlı laboratuvarla oynamanızı ve yanlış adım olasılığının değerini değiştirmenizi ve bunun for dönüşü veya otomatik beklenen getiriyi ve ayrıca Q değerlerini, q s a'yı nasıl etkilediğini görmenizi tavsiye ederim. Şimdi, şimdiye kadar yaptığımız her şeyde, bu Markov karar sürecini, sadece altı eyaletli bu Mars gezicisini kullandık. Birçok pratik uygulama için durum sayısı çok daha fazla olacaktır. Bir sonraki videoda, şimdiye kadar bahsettiğimiz pekiştirmeli öğrenme veya Markov karar süreci çerçevesini alıp çok daha büyük ve özellikle sürekli durum uzayları ile bu çok daha zengin ve belki de daha ilginç problemlere genelleştireceğiz. Bir sonraki videoda buna bir göz atalım.

## Example of continuous state space applications
Uygulama laboratuvarında üzerinde çalıştığınız aya iniş uygulaması da dahil olmak üzere birçok robotik kontrol uygulaması sürekli durum uzaylarına sahiptir. Bunun ne anlama geldiğine ve bahsettiğimiz kavramı bu sürekli durum uzaylarına nasıl genelleyebileceğimize bir göz atalım. Kullandığımız basitleştirilmiş bir Mars gezici örneği, ayrı bir durumlar kümesi kullanıyorum ve bunun anlamı, basitleştirilmiş Mars gezgini yalnızca altı olası konumdan birinde olabilir. Ancak çoğu robot, altı veya herhangi bir ayrık sayıda konumdan birden fazlasında olabilir, bunun yerine çok sayıda sürekli değer konumundan herhangi birinde olabilir. Örneğin, Mars gezici bir çizgi üzerinde herhangi bir yerde olabilirse, konumu 0-6 kilometre arasında değişen bir sayı ile belirtilir ve aradaki herhangi bir sayı geçerlidir. Bu, sürekli durum uzayına bir örnek olacaktır, çünkü konum, 2,7 kilometre veya 4,8 kilometre veya sıfır ile altı arasında herhangi bir sayı gibi bir sayı ile temsil edilecektir. Başka bir örneğe bakalım. Bu örnek için bir arabayı veya kamyonu kontrol etme uygulamasını kullanacağım. İşte bir oyuncak araba, Rusya oyuncak kamyonu. Bu kızıma ait. Kendi kendine giden bir araba veya kendi kendine giden bir kamyon yapıyorsanız ve bunu sorunsuz bir şekilde sürmek için kontrol etmek istiyorsanız, o zaman bu kamyonun durumu, x konumu, y konumu, belki yönü gibi birkaç sayı içerebilir. . Ne tarafa bakıyor? Kamyonun yerde kaldığını varsayarsak, muhtemelen ne kadar yüksek, ne kadar yüksek olduğu konusunda endişelenmenize gerek yok. Bu durum x, y ve teta açısını içerecektir, ayrıca x yönündeki hızları, y yönündeki hızı ve ne kadar hızlı dönüyor olabilir. Saniyede bir derece mi dönüyor yoksa saniyede 30 derece mi dönüyor yoksa saniyede 90 derece hızla mı dönüyor? Bir kamyon veya araba için durum, bunun bu hat üzerinde kaç kilometre olduğu gibi yalnızca bir sayı içermeyebilir, ancak altı sayı da içerebilir, x konumu, y konumu, is yönü, hangisine gidiyorum Yunan alfabesi Theta'yı ve ayrıca x yönündeki hızını x nokta kullanarak göstereceğim, yani bu x koordinatının ne kadar hızlı değiştiği, y noktasının y koordinatının ne kadar hızlı değiştiği anlamına gelir ve son olarak , Arabanın açısının ne kadar hızlı değiştiği teta noktası. Oysa 60 Mars gezgini örneğinde, durum altı olası sayıdan yalnızca biriydi. Bir, iki, üç, dört, beş veya altı olabilir. Araba için durum, altı sayıdan oluşan bu vektörü içerecektir ve bu sayılardan herhangi biri, geçerli aralık dahilinde herhangi bir değeri alabilir. Örneğin, Teta sıfır ile 360 ​​derece arasında değişmelidir. Başka bir örneğe bakalım. Otonom bir helikopteri kontrol etmek için bir pekiştirmeli öğrenme algoritması oluşturuyorsanız, bir helikopterin konumunu nasıl karakterize edersiniz? Örnek olarak, burada yanımda küçük bir oyuncak helikopter var. Helikopterin konumlandırılması, bir helikopterin ne kadar kuzey veya güney olduğu gibi x konumunu, y konumunu içerecektir. Belki helikopterin doğu-batı ekseninde ne kadar uzakta olduğu ve ayrıca z, helikopterin yerden yüksekliği. Ancak, konumun dışında, helikopterin de bir yönü vardır ve geleneksel olarak, yönünü yakalamanın bir yolu, biri helikopter sırasını yakalayan üç ek sayı kullanmaktır. Sağa mı sola mı kayıyor? Saha, ileri mi yoksa yukarı mı, geri mi dönüyor ve son olarak pusula yönünün baktığı batıya doğru sapma. Kuzeye mi yoksa doğuya mı yoksa güneye mi yoksa batıya mı bakıyorsunuz? Özetlemek gerekirse, helikopterin durumu, örneğin kuzey-güney yönündeki konumu, doğu-batı yönünde konumlandırılmış olması, y'nin yerden yüksekliği ve ayrıca helikopterin sırası, eğimi ve ayrıca yalpalamasıdır. Bunu yazmak için, durum bu nedenle x, y, z konumunu ve ardından Yunan alfabeleri Phi, Theta ve Omega ile gösterilen sıra aralığını ve sapmayı içerir. Ancak helikopteri kontrol etmek için x yönündeki, y yönündeki ve z yönündeki hızını ve açısal hız olarak da adlandırılan dönüş hızını da bilmemiz gerekir. Bu sıra ne kadar hızlı değişiyor ve bu adım ne kadar hızlı değişiyor ve sapma ne kadar hızlı değişiyor? Bu aslında otonom helikopterleri kontrol etmek için kullanılan durum. Bu 12 numara listesi bir politikaya girdi midir ve bir politikanın işi bu 12 numaraya bakmak ve helikopterde yapılacak uygun eylemin ne olduğuna karar vermektir. Yani herhangi bir sürekli durum pekiştirmeli öğrenme problemi veya sürekli bir Markov karar süreci, sürekli MTP. Problemin durumu, 1'den 6'ya kadar bir sayı gibi az sayıda olası ayrık değerden biri değildir.
Bunun yerine, herhangi biri çok sayıda değerden herhangi birini alabilen bir sayı vektörüdür. Bu haftaki uygulama laboratuvarında, simüle edilmiş bir aya iniş uygulamasına uygulanan bir pekiştirmeli öğrenme algoritmasını kendiniz uygulayacaksınız. Ay'a bir şey indirmek simülasyondur. Bir sonraki videoda bu uygulamanın neleri içerdiğine bir göz atalım, çünkü başka bir sürekli durum uygulaması olacak.

##