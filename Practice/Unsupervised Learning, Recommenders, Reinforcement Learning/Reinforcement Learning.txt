## What is Reinforcement Learning?
Makine öğrenimi uzmanlığının bu son haftasına hoş geldiniz. Bu uzmanlığın sonuna yaklaşıyor olmamız benim için biraz buruk ama bu haftayı sabırsızlıkla bekliyorum, sizinle takviyeli öğrenme hakkında bazı heyecan verici fikirleri paylaşıyorum. Makine öğreniminde, takviye öğrenimi, bugün henüz ticari uygulamalarda çok yaygın olarak uygulanmasa da makine öğreniminin temel direklerinden biri olan fikirlerden biridir. Ve onu destekleyen ve her gün geliştiren çok sayıda heyecan verici araştırma var. Pekiştirmeli öğrenmenin ne olduğuna bir göz atarak başlayalım. Bir örnekle başlayalım. İşte otonom bir helikopterin resmi. Bu aslında Stanford otonom helikopteri, 32 pound ağırlığında ve şu anda ofisimde duruyor. Diğer birçok otonom helikopter gibi, yerleşik bir bilgisayar, GPS, ivmeölçerler, jiroskoplar ve manyetik pusula ile donatılmıştır, böylece her zaman nerede olduğunu oldukça doğru bir şekilde bilir. Size bu helikopterin anahtarlarını versem ve onu uçurmak için bir program yazmanızı istesem, bunu nasıl yaparsınız? Radyo kontrollü helikopterler bunun gibi joysticklerle kontrol edilir ve bu nedenle görev saniyede on kez size helikopterin konumu, yönü ve hızı verilir. Ve helikopteri havada dengede tutabilmek için bu iki kontrol çubuğunu nasıl hareket ettireceğinize karar vermelisiniz. Bu arada, radyo kontrollü helikopterlerin yanı sıra dört rotorlu dronları kendim uçurdum. Ve radyo kontrollü helikopterleri uçurmak, havada dengede tutmak aslında biraz daha zordur. Peki bunu otomatik olarak yapacak bir programı nasıl yazarsınız? Size Stanford otonom helikopterimizle yapacağımız bir şeyin eğlenceli bir videosunu göstereyim. İşte bir takviyeli öğrenme algoritmasının kontrolü altında uçtuğu bir video. Ve videoyu oynatmama izin ver. O gün aslında kameraman bendim ve bu bilgisayar kontrolünde uçan helikopter ve videoyu uzaklaştırırsam gökyüzüne dikilmiş ağaçları görüyorsunuz. Takviyeli öğrenmeyi kullanarak, aslında baş aşağı uçmayı öğrenmek için bu helikopteri aldık. Baş aşağı uçmasını söyledik. Ve böylece güçlendirilmiş öğrenme, helikopterlerin çok çeşitli akrobasi hareketlerini uçurmasını sağlamak için kullanıldı ya da biz onlara akrobasi manevraları diyoruz. Bu arada, diğer videoları görmekle ilgileniyorsanız, bu URL'den onlara da göz atabilirsiniz. Peki pekiştirmeli öğrenmeyi kullanarak bir helikopterin kendi kendine uçmasını nasıl sağlarsınız? Görev, kontrol çubuklarını nasıl hareket ettireceğine karar vermek için helikopterin konumuna verilir. Takviyeli öğrenmede, helikopterin konumu, yönü ve hızı vb. durumlarını s olarak adlandırırız. Ve böylece görev, helikopterin durumundan eylem a'ya eşlenen bir işlev bulmaktır, yani helikopteri havada dengede tutmak, uçmak ve çarpmadan tutmak için iki kontrol çubuğunun ne kadar uzağa itilmesi gerektiği anlamına gelir. Bu sorunu denemenin bir yolu denetimli öğrenmeyi kullanmaktır. Bunun otonom helikopter uçuşu için harika bir yaklaşım olmadığı ortaya çıktı. Ama diyebilirsiniz ki, bir dizi durum gözlemi alabilirsek ve belki de uzman bir insan pilot bize yapılacak en iyi eylemin ne olduğunu söyler. Daha sonra, burada x olarak adlandırdığım durumlardan, burada y olarak adlandırdığım bir eyleme eşlemeyi doğrudan öğrenmek için denetimli öğrenmeyi kullanarak bir sinir ağını eğitebilirsiniz. Ancak, helikopter havada hareket ederken aslında çok belirsiz olduğu ortaya çıktı, yapılacak doğru eylemin tam olarak ne olduğu. Biraz sola mı yoksa çok daha fazla sola mı yatırıyorsunuz veya helikopter stresini biraz mı yoksa çok mu artırıyorsunuz? Bir x veri seti ve ideal eylem y elde etmek aslında çok zordur. Bu nedenle, helikopter ve diğer robotlar gibi bir robotu kontrol etme görevinin çoğunda, denetimli öğrenme yaklaşımı iyi çalışmıyor ve bunun yerine takviyeli öğrenme kullanıyoruz. Takviyeli öğrenme için önemli bir girdi, helikoptere ne zaman iyi ve ne zaman kötü gittiğini söyleyen ödül veya ödül işlevi olarak adlandırılan bir şeydir. Bu yüzden, ödül işlevini düşünmeyi sevdiğim şekilde, bir köpeği eğitmeye benziyor. Ben büyürken, ailemin bir köpeği vardı ve benim işim köpeği ya da yavruyu terbiye etmekti. Peki bir köpek yavrusu nasıl iyi davranır? Pekala, köpeğe bu kadarını gösteremezsin. Onun yerine işini yapmasına izin veriyorsun ve ne zaman iyi bir şey yapsa, sen gidiyorsun, ahbap. Ve ne zaman kötü bir şey yapsalar, gidersin, kötü köpek. Ve sonra umarım kendi kendine nasıl daha çok iyi köpek ve daha az kötü köpek işi yapılacağını öğrenir. Takviyeli öğrenme algoritması ile eğitim işte böyle bir şey. Helikopter iyi uçtuğunda, gidersiniz, iyi helikopter ve çarpma gibi kötü bir şey yaparsa, gidersiniz, kötü helikopter. Ve sonra, iyi helikopterden daha fazlasını ve kötü helikopter sonuçlarından daha azını nasıl elde edeceğinizi bulmak, pekiştirmeli öğrenme algoritmasının işidir.
Takviyeli öğrenmenin neden bu kadar güçlü olduğunu düşünmenin bir yolu, ona nasıl yapılacağından çok ne yapması gerektiğini söylemeniz gerektiğidir. Ve en uygun eylem yerine ödül işlevini belirtmek, sistemi nasıl tasarladığınız konusunda size çok daha fazla esneklik sağlar. Helikopteri uçurmak için somut olarak, ne zaman iyi uçuyorsa, ona iyi uçtuğu her saniye artı bir ödül verebilirsiniz. Ve belki de ne zaman kötü uçuyorsa, ona negatif bir ödül verebilirsiniz ya da düşerse, negatif 1000 gibi çok büyük bir negatif ödül verebilirsiniz. Ve bu, helikopteri iyi uçmak için çok daha fazla zaman harcamaya ve umarım asla düşmemeye teşvik eder. Ama işte başka bir eğlenceli video. Uzun yıllardır pekiştirmeli öğrenme için iyi köpek kötü köpek benzetmesini kullanıyordum. Ve sonra bir gün gerçekten robotik bir köpeğe el atmayı başardım ve bu pekiştirmeyi gerçekten iyi köpek kötü köpek öğrenme metodolojisini bir robot köpeği engelleri aşması için eğitmek için kullanabilirdim. Bu, onu ödüllendiren takviyeli öğrenmeyi kullanarak ekranın soluna doğru hareket ederek ayaklarını dikkatli bir şekilde yerleştirmeyi veya çeşitli engellerin üzerinden nasıl tırmanacağını öğrenen bir robot köpeğin videosu. Ve bir köpeği böyle programlamak için ne gerektiğini düşünürseniz, hiçbir fikrim yok, belirli bir engeli aşmak için bacaklarını yerleştirmenin en iyi yolunun ne olduğunu ona nasıl söyleyeceğimi gerçekten bilmiyorum. Bütün bunlar, robot tarafından sadece onu teşvik eden ödüller vererek, ekranın solundaki hedefe doğru ilerleme kaydederek otomatik olarak çözüldü. Günümüzde pekiştirmeli öğrenme, robotları kontrol etmekten çok çeşitli uygulamalara başarıyla uygulanmaktadır. Ve aslında bu haftanın ilerleyen saatlerinde uygulama laboratuvarında, simülasyonda bir aya iniş yapmak için kendinize bir pekiştirmeli öğrenme algoritması uyguluyorsunuz. Fabrika optimizasyonu için de kullanıldı. İş hacmini ve verimliliği ve finansal hisse senedi alım satımını en üst düzeye çıkarmak için fabrikadaki şeyleri nasıl yeniden düzenlersiniz? Örneğin, bir arkadaşım verimli stok uygulaması üzerinde çalışıyordu. Dolayısıyla, önümüzdeki birkaç gün içinde bir milyon hisse satmaya karar verdiyseniz, borsada aniden bir milyon hisseyi düşürmek istemeyebilirsiniz çünkü bu, fiyatları aleyhinize hareket ettirecektir. Öyleyse, satmak istediğiniz hisseleri satabilmeniz ve onlar için mümkün olan en iyi fiyatı alabilmeniz için alım satımlarınızı zaman içinde sıralamanın en iyi yolu nedir? Son olarak, pek çok video oyunu oynamanın yanı sıra damadan satranca, briç kart oyununa kadar oyun oynamaya kadar pek çok takviyeli öğrenme uygulaması da olmuştur. İşte bu pekiştirmeli öğrenmedir. Takviyeli öğrenme, neredeyse denetimli öğrenme kadar kullanılmasa da, günümüzde hala birkaç uygulamada kullanılmaktadır. Ve ana fikir, algoritmaya her bir girdi için doğru y çıktısının ne olduğunu söylemeniz yerine, tek yapmanız gereken ona ne zaman iyi gittiğini ve ne zaman kötü gittiğini söyleyen bir ödül işlevi belirtmektir. Ve iyi eylemlerin nasıl seçileceğini otomatik olarak bulmak algoritmanın işidir. Bununla birlikte, pekiştirmeli öğrenme problemini formüle edeceğimiz ve aynı zamanda iyi eylemleri otomatik olarak seçmek için algoritmalar geliştirmeye başlayacağımız bir sonraki videoya geçelim.

## Mars rover example
Takviyeli öğrenme şekilciliğini bitirmek için, bir helikopter veya robot köpek gibi karmaşık bir şeye bakmak yerine, genel olarak Mars gezicisinden ilham alan basitleştirilmiş bir örnek kullanabiliriz. Bu, Stanford profesörü Emma Branskill ve iş arkadaşlarımdan biri olan Jagriti Agrawal'ın şu anda Mars gezicisini gerçekten kontrol eden bir kod yazması nedeniyle örnekten uyarlandı ve bu da bu örneği geliştirmeme yardımcı oldu ve konuşmama yardımcı oldu. Hadi bir bakalım. Mars gezicisinden esinlenen basitleştirilmiş bir örnek kullanarak pekiştirmeli öğrenme geliştireceğiz. Bu uygulamada, gezici, buradaki altı kutuda gösterildiği gibi altı konumdan herhangi birinde olabilir. Gezici, örneğin, burada gösterilen dördüncü kutuya doğru yola çıkabilir. Mars gezicisinin konumu pekiştirmeli öğrenmede durum olarak adlandırılır ve ben bu altı durumu, durum 1, durum 2, durum 3, durum 4, durum 5 ve durum 6 olarak adlandıracağım ve böylece gezici başlıyor 4. durumda kapalı. Şimdi gezici, farklı bilim görevlerini yerine getirmeye çalışmak için Mars'a gönderildi. Gezegendeki farklı yerlerdeki kayayı analiz etmek için matkap, radar veya spektrometre gibi sensörlerini kullanmak üzere farklı yerlere gidebilir veya dünyadaki bilim adamlarının bakması için ilginç fotoğraflar çekmek üzere farklı yerlere gidebilir. Bu örnekte, burada soldaki durum 1, bilim adamlarının yüzey aracının örneklemesini isteyeceği çok ilginç bir yüzeye sahiptir. Durum 6 ayrıca, bilim adamlarının gezicinin örneklemesini isteyeceği oldukça ilginç bir yüzeye sahiptir, ancak durum 1 kadar ilginç değildir. Bilim görevini ve durum 1'i gerçekleştirme olasılığımız, durum 6'dan daha fazladır, ancak durum 1 daha uzaktadır. . Durum 1'in potansiyel olarak daha değerli olduğunu yansıtmanın yolu, ödül işlevidir. 1. durumdaki ödül 100'dür ve 6. aşamadaki ödül 40'tır ve aradaki diğer tüm eyaletlerdeki ödüller, sıfır ödülü olarak yazacağım çünkü bilim için o kadar ilginç bir şey yok. 2, 3, 4 ve 5 durumlarında yapılabilir. Her adımda, gezici iki eylemden birini seçer. Ya sola gidebilir ya da sağa gidebilir. Soru şu ki, gezici ne yapmalı? Takviyeli öğrenmede, ödüllere çok dikkat ederiz çünkü robotun iyi mi kötü mü yaptığını bu şekilde anlarız. Robot 4. durumdan başlayarak sola giderse ne olabileceğine dair bazı örneklere bakalım. Ardından başlangıçta 4. durumdan başlayarak, sıfır ödülü alacak ve sola gittikten sonra 3. duruma geçecek ve burada tekrar sıfır ödül alır. Sonra 2 durumuna gelir, 0 ödülünü alır ve son olarak 100'lük bir ödül aldığı 1 durumuna gelir. Bu uygulama için, ya 1. ya da 6. duruma geldiğinde, gün biter Takviyeli öğrenmede, buna bazen bir son durum diyoruz ve bunun anlamı, bu uç durumlardan birine ulaştıktan sonra, o durumda bir ödül alıyor, ancak bundan sonra hiçbir şey olmuyor. Belki robotların yakıtı bitmiştir veya o gün için zamanları kalmamıştır, bu yüzden sadece 100 veya 40 ödülünün tadını çıkarabilir, ama o gün için bu kadar. Bundan sonra ek ödüller kazanamaz. Artık robot sola gitmek yerine sağa gitmeyi de seçebilir, bu durumda 4 durumundan önce sıfır ödülü alır ve sonra sağa hareket eder ve 5 durumuna geçer, başka bir ödül alır. sıfır ve sonra sağdaki bu diğer terminal durumuna, durum 6'ya ulaşacak ve 40'lık bir ödül alacak. Ancak sola gitmek ve sağa gitmek tek seçenek. Robotun yapabileceği bir şey, 4. durumdan başlayıp sağa doğru hareket etmeye karar verebilmesidir. Durum 4-5'ten gider, durum 4'te sıfır ve durum 5'te sıfır ödülü alır ve sonra belki fikrini değiştirir ve aşağıdaki gibi sola gitmeye karar verir, bu durumda 4. durumda, 3. durumda, 2. durumda sıfır ödülü ve ardından 1. duruma geldiğinde 100 ödül. Bu eylemler ve durumlar dizisinde, robot daha iyi zaman harcıyor. Bu, harekete geçmek için harika bir yol olmayabilir, ancak algoritmanın seçebileceği bir seçimdir, ancak umarım bunu seçmezsiniz. Özetlemek gerekirse, her adımda, robot bir durumda, ben buna S diyeceğim ve bir eylem seçiyor ve ayrıca bu durumdan aldığı bazı ödüllerden, R of S'den keyif alıyor. Bu eylemin bir sonucu olarak, yeni bir S üssü durumuna geçer. Somut bir örnek olarak, robot 4. durumdayken ve sola git eylemini yaptığında, belki o 4. durumla ilişkili sıfır ödülünün tadını çıkarmadı ve yeni bir 3. durumu olmayacak. Özel pekiştirmeli öğrenme algoritmalarında, bu dört şeyin, durum, eylem, ödül ve sonraki durum olduğunu görürsünüz; bu, temel olarak her eylemde bulunduğunuzda olan şeydir ve takviyeli öğrenme algoritmalarının karar verirken bakacağı temel unsurlardır. nasıl önlemler alınır.
Açıklığa kavuşturmak için, buradaki ödül, R of S, bu, bu durumla ilişkili ödüldür. Bu sıfır ödülü, durum 3'ten ziyade durum 4 ile ilişkilendirilir. Takviyeli öğrenme uygulamasının çalışma şekli budur. Bir sonraki videoda, pekiştirmeli öğrenme algoritmasının yapmasını istediğimiz şeyi tam olarak nasıl belirttiğimize bir göz atalım. Özellikle pekiştirmeli öğrenmede geri dönüş adı verilen önemli bir fikirden bahsedeceğiz. Bunun ne anlama geldiğini görmek için bir sonraki videoya geçelim.

## The Return in reinforcement learning
Bir önceki videoda pekiştirmeli öğrenme uygulamasında durumların neler olduğunu ve yaptığınız işlemlere bağlı olarak nasıl farklı durumlardan geçtiğinizi ve farklı ödüller kazandığınızı gördünüz. Ancak belirli bir ödül setinin farklı bir ödül setinden daha iyi veya daha kötü olduğunu nasıl anlarsınız? Bu videoda tanımlayacağımız pekiştirmeli öğrenmedeki geri dönüş, bunu yakalamamızı sağlıyor. Bunu incelerken, faydalı bulabileceğiniz bir benzetme şu: Ayağınızın dibinde beş dolarlık bir banknot olduğunu hayal edin, aşağı uzanıp alabilirsiniz veya yarım saatte kasabanın öbür ucuna, yarım saatte yürüyebilir ve 10 dolarlık bir banknot al. Hangisinin peşinden gitmeyi tercih edersin? On dolar, beş dolardan çok daha iyidir, ancak gidip o 10 dolarlık banknotu almak için yarım saat yürümeniz gerekiyorsa, o zaman belki onun yerine beş dolarlık banknotu almanız daha uygun olabilir. Geri dönüş kavramı, daha hızlı alabileceğiniz ödüllerin, ulaşmanız uzun zaman alan ödüllerden belki daha çekici olduğunu yakalar. Bunun tam olarak nasıl çalıştığına bir göz atalım. İşte bir Mars Rover örneği. Durum 4'ten başlayarak sola giderseniz, aldığınız ödüllerin durum 4'ten ilk adımda sıfır, durum 3'ten sıfır, durum 2'den sıfır ve ardından uç durum olan durum 1'de 100 olacağını gördük. Getiri, bu ödüllerin toplamı olarak tanımlanır, ancak indirim faktörü olarak adlandırılan ek bir faktörle ağırlıklandırılır. İndirgeme faktörü 1'den biraz küçük bir sayıdır. İndirgeme faktörü olarak 0.9'u seçeyim. İlk adımdaki ödülü sıfır olarak ağırlıklandıracağım, ikinci adımdaki ödül bir indirim faktörü, bu ödülün 0,9 katı ve sonra artı indirim faktörü^2 çarpı bu ödül ve sonra artı indirim faktörü ^ 3 kat bu ödül. Bunu hesaplarsanız 0,729 çarpı 100 yani 72,9 çıkıyor. Geri dönüşün daha genel formülü, eğer robotunuz bir dizi durumdan geçerse ve birinci adımda R_1, ikinci adımda R_2 ve üçüncü adımda R_3 vb. alırsa, o zaman dönüş R_1 olur. artı indirim faktörü Gama, bu örnekte 0,9 olarak ayarladığım bu Yunan alfabesi Gamma, Gama çarpı R_2 artı Gama^2 çarpı R_3 artı Gamma^3 çarpı R_4 vb. siz terminal durumuna gelene kadar. İndirgeme faktörü Gamma'nın yaptığı şey, takviyeli öğrenme algoritmasını biraz sabırsız hale getirme etkisine sahip olmasıdır. Çünkü geri dönüş birinci ödüle tam kredi veriyor yüzde 100 yani 1 katı R_1 ama sonra biraz daha az kredi veriyor ikinci adımda aldığınız ödül 0,9 ile çarpılıyor ve daha sonra aldığınız ödüle daha da az kredi veriyor. bir sonraki adım R_3'te vb. alın ve böylece ödülleri daha erken almak, toplam getiri için daha yüksek bir değerle sonuçlanır. Pek çok takviyeli öğrenme algoritmasında, indirgeme faktörü için ortak bir seçim, 0,9 veya 0,99 veya hatta 0,999 gibi 1'e oldukça yakın bir sayı olacaktır. Ancak kullanacağım çalışan örnekte açıklama amacıyla, aslında 0,5'lik bir indirgeme faktörü kullanacağım. Bu çok ağır bir şekilde, gelecekte ödülleri çok ağır bir şekilde düşürür, çünkü her ek ayrıştırma zaman damgasıyla, bir adım önce alacağınız ödüllerin yalnızca yarısı kadar kredi alırsınız. Gama 0,5'e eşit olsaydı, yukarıdaki örnekte getiri 0 artı 0,5 çarpı 0 olurdu, bu denklemi üste koyar, artı 0,5^2 0 artı 0,5^3 çarpı 100. Durum 1'den uç duruma geçtiği için bu ödül kaybedildi, ve bu 12.5'lik bir getiri olarak çıkıyor. Finansal uygulamalarda iskonto faktörünün de faiz oranı veya paranın zaman değeri olarak çok doğal bir yorumu vardır. Bugün bir dolarınız varsa, bu, gelecekte yalnızca bir dolarınız olacak duruma göre biraz daha değerli olabilir. Çünkü bugün bir doları bile bankaya yatırabilir, biraz faiz kazanabilir ve bundan bir yıl sonra biraz daha fazla paraya sahip olabilirsiniz. Finansal uygulamalar için, genellikle, bu iskonto faktörü, bugün bir dolarla karşılaştırdığımda gelecekte bir doların ne kadar az olduğunu temsil eder. Bazı somut getiri örneklerine bakalım. Aldığınız geri dönüş, ödüllere bağlıdır ve ödüller, yaptığınız eylemlere bağlıdır ve dolayısıyla geri dönüş, yaptığınız eylemlere bağlıdır. Her zamanki örneğimizi kullanalım ve bu örnek için diyelim ki, ben her zaman sola gideceğim. Robot 4 durumunda başlarsa, önceki slaytta hesapladığımız gibi dönüşün 12,5 olduğunu daha önce görmüştük. Üçte başlasaydı, getiri 25 olurdu çünkü 100 ödüle bir adım daha erken ulaşır ve bu nedenle daha az iskonto edilir. Durum 2'de başlasaydı, dönüş 50 olurdu. Yeni başlayıp durum 1 olsaydı, pekala, hemen 100'lük ödülü alır, yani düşük indirimli olmaz.
Durum 1'den başlarsak getiri 100 olur ve bu iki durumdaki dönüş 6.25 olur. Görünüşe göre, son durum olan 6 durumundan başlarsanız, sadece ödülü ve dolayısıyla 40'ın geri dönüşünü alırsınız. Şimdi, farklı bir eylemler dizisi yapacak olsaydınız, geri dönüşler aslında farklı olurdu. Örneğin, her zaman sağa gidersek, eylemlerimiz bunlar olsaydı, o zaman 4. durumda başlarsanız, 0 ödül alırsınız. Sonra 5. duruma gelirsiniz, 0 ödül alırsınız ve 6 durumuna gelir ve 40'lık bir ödül alır. Bu durumda getiri 0 artı 0,5 olur, iskonto faktörü çarpı 0 artı 0,5'in karesi çarpı 40 olur ve bu da 0,5'in karesinin 1/4 olduğu ortaya çıkar, yani 40'ın 1/4'ü 10'dur. Bu halden, 4. halden getirisi 10'dur. Harekete geçecekseniz daima sağa gidin. Benzer bir mantıkla bu durumdan getiri 20, bu durumdan getiri beş, bu durumdan getiri 2,5 ve sonra dönüş determinant durumu 140 olur. tamamen mantıklıysa, videoyu duraklatıp matematiği iki kez kontrol etmekten çekinmeyin ve bunların geri dönüş için uygun değerler olduğuna kendinizi ikna edip edemeyeceğinize bakın. Çünkü farklı hallerden yola çıkarsanız ve her zaman sağa gidecekseniz. Her zaman sağa gittiğini görüyoruz. Almayı beklediğiniz getiri çoğu eyalet için daha düşüktür. Belki de her zaman sağa gitmek, her zaman sola gitmek kadar iyi bir fikir değildir. Ama her zaman sola gitmek zorunda olmadığımız, her zaman sağa gittiğimiz ortaya çıktı. Ayrıca 2. durumda olup olmadığınıza da karar verebiliriz, sola gidin. Durum 3'teyseniz, sola gidin. 4. durumdaysanız sola gidin. Ancak 5. durumdaysanız, bu ödüle çok yakınsınız demektir. Sağa gidelim. Bu, içinde bulunduğunuz duruma göre yapılacak eylemleri seçmenin farklı bir yolu olacaktır. Farklı durumlardan alacağınız getiri 100, 50, 25, 12.5, 20 ve 40 olacaktır. Bir vaka. 5. durumda başlayacak olsaydınız, burada sağa giderdiniz ve böylece aldığınız ödüller önce 5 durumunda sıfır, sonra 4 olur. Getiri sıfırdır, ilk ödül artı indirim faktörü 0,5 çarpı 40 yani 20 yani burada gösterilen işlemleri yaparsanız bu durumdan 20 dönüş neden oluyor. Özetlemek gerekirse, takviyeli öğrenmedeki geri dönüş, sistemin aldığı ödüllerin indirim faktörü ile ağırlıklandırılmış toplamıdır; burada uzak gelecekteki ödüller, indirim faktörü tarafından daha yüksek bir güce yükseltilerek ağırlıklandırılır. Şimdi, negatif ödülleri olan sistemleriniz olduğunda bunun aslında ilginç bir etkisi var. İncelediğimiz örnekte, tüm ödüller sıfır veya pozitifti. Ancak herhangi bir ödül negatifse, o zaman indirim faktörü aslında sistemi negatif ödülleri olabildiğince geleceğe itmeye teşvik eder. Mali bir örnek alırsak, birine 10$ ödemek zorunda kalsaydınız, bu belki eksi 10'luk bir negatif ödüldür. faiz oranı aslında bugün ödemek zorunda olduğunuz 10 dolardan daha az değerdedir. Negatif ödülleri olan sistemler için, algoritmanın ödülleri olabildiğince geleceğe itmeye çalışmasına neden olur. Finansal uygulamalar ve diğer uygulamalar için, bu aslında sistemin yapması gereken doğru şey olarak ortaya çıkıyor. Takviyeli öğrenmede geri dönüşün ne olduğunu artık biliyorsunuz, takviyeli öğrenme algoritmasının hedefini formüle etmek için bir sonraki videoya geçelim.

## Making decisions: Policies in reinforcement learning
Bir pekiştirmeli öğrenme algoritmasının eylemleri nasıl seçtiğini resmileştirelim. Bu videoda, takviyeli öğrenme algoritmasında bir politikanın ne olduğunu öğreneceksiniz. Hadi bir bakalım. Gördüğümüz gibi, pekiştirmeli öğrenme probleminde harekete geçmenin birçok farklı yolu vardır. Örneğin, her zaman en yakın ödül için gitmeye karar verebiliriz, böylece bu en soldaki ödül daha yakınsa sola gidersiniz veya bu en sağdaki ödül daha yakınsa sağa gidersiniz. Eylemleri seçmemizin başka bir yolu da, her zaman daha büyük ödül için gitmektir veya her zaman daha küçük ödül için gidebiliriz, iyi bir fikir gibi görünmüyor, ancak bu başka bir seçenek veya sadece değilseniz sola gitmeyi seçebilirsiniz. daha az ödülden bir adım uzaklaşırsanız, bu durumda onu tercih edersiniz. Takviyeli öğrenmede amacımız, işi herhangi bir durumu girdi olarak almak ve bunu bizden yapmamızı istediği bazı eylemlere haritalamak olan, politika Pi adı verilen bir işlev bulmaktır. Örneğin, alttaki bu politika için, bu politika, eğer 2. durumdaysanız, bizi sol eyleme eşler. 3. durumdaysanız, politika sola gidin diyor. 4. durumdaysanız ayrıca sola gidin ve 5. durumdaysanız sağa gidin. S durumuna uygulanan Pi, bize bu durumda hangi eylemi yapmamızı istediğini söyler. Takviyeli öğrenmenin amacı, getiriyi en üst düzeye çıkarmak için her durumda hangi eylemi yapmanız gerektiğini size söyleyen bir Pi veya Pi of S politikası bulmaktır. Bu arada, politikanın pi'nin ne olduğunu en açıklayıcı terim olup olmadığını bilmiyorum ama pekiştirmeli öğrenmede standart hale gelen terimlerden biri. Belki Pi'ye bir politika yerine bir denetleyici demek daha doğal bir terminoloji olur, ancak politika, pekiştirmeli öğrenmedeki herkesin şimdi buna dediği şeydir. Son videoda, durumlardan ödüllere, getirilere ve politikalara kadar pek çok pekiştirmeli öğrenme konseptini inceledik. Bir sonraki videoda bunları hızlı bir şekilde gözden geçirelim ve ardından bu politikaları bulmak için algoritmalar geliştirmeye başlayacağız. Bir sonraki videoya geçelim.

## Review of key concepts
Altı durumlu Mars gezgini örneğini kullanarak bir pekiştirmeli öğrenme formalizmi geliştirdik. Anahtar kavramlara hızlıca bir göz atalım ve bu kavram setinin diğer uygulamalar için de nasıl kullanılabileceğini görelim. Tartıştığımız kavramlardan bazıları, takviyeli öğrenme probleminin durumları, eylemler dizisi, ödüller, bir indirim faktörü, ardından ödüllerin ve indirim faktörünün birlikte getiriyi hesaplamak için nasıl kullanıldığı ve son olarak, işi kimin yaptığı bir politikadır. getiriyi en üst düzeye çıkarmak için eylemleri seçmenize yardımcı olmaktır. Mars gezgini örneği için 1-6 arasında numaralandırdığımız altı durumumuz vardı ve eylemler sola veya sağa gitmek şeklindeydi. Ödüller en soldaki durum için 100, en sağdaki durum için 40 ve arada sıfırdı ve 0,5'lik bir indirim faktörü kullanıyordum. Geri dönüş bu formül tarafından verildi ve Pi'nin eylemleri tasvir ettiği, içinde bulunduğunuz duruma bağlı olarak farklı politikaları olabilir. Aynı biçimcilik veya durumlar, eylemler, ödüller vb. birçok başka uygulama için de kullanılabilir. Sorunu çözün veya otonom bir helikopter bulun. Bir durum belirlemek, helikopterin olası konumları, yönelimleri ve hızları vb. kümesi olacaktır. Olası eylemler, bir helikopterin kontrol çubuğunu hareket ettirmenin olası yolları olabilir ve ödüller, iyi uçuyorsa artı bir ve gerçekten kötü düşmüyorsa veya çarpmıyorsa eksi 1.000 olabilir. Helikopterin ne kadar iyi uçtuğunu size söyleyen ödül işlevi. İndirgeme faktörü, birden biraz daha küçük bir sayı, diyelim ki, 0.99 ve sonra ödüllere ve indirim faktörüne bağlı olarak, getiriyi aynı formülü kullanarak hesaplarsınız. Takviyeli bir öğrenme algoritmasının işi, helikopterin konumu olan girdi olarak verildiğinde, size hangi eylemi yapmanız gerektiğini söyleyen Pi'nin bir politikasını bulmak olacaktır. Yani kontrol çubuklarını nasıl hareket ettireceğinizi anlatır. İşte bir örnek daha. İşte oyun oynayan bir tane. Satranç oynamayı öğrenmek için pekiştirmeli öğrenmeyi kullanmak istediğinizi varsayalım. Bu sorunun durumu, tahtadaki tüm taşların konumu olacaktır. Bu arada, satranç oynuyorsanız ve kuralları iyi biliyorsanız, bunun satranç için sadece taşların konumundan biraz daha fazla bilgi olduğunu biliyorum ama bu video için biraz basitleştireceğim. Eylemler, oyundaki olası yasal hamlelerdir ve bu durumda, sisteminize bir oyunu kazanırsa artı bir, oyunu kaybederse eksi bir ve eğer oyunu kaybederse eksi bir ödül verirseniz, ortak bir ödül seçimi olacaktır. bir oyun bağlar. Satranç için, genellikle bire çok yakın bir indirim faktörü kullanılacaktır, bu nedenle belki 0,99, hatta 0,995 veya 0,999 olabilir ve geri dönüş, diğer uygulamalarla aynı formülü kullanır. Bir kez daha, hedefe bir politika Pi kullanarak iyi bir eylem seçmesi için bir tahta pozisyonu verilir. Bir pekiştirmeli öğrenme uygulamasının bu şekilciliğinin aslında bir adı vardır. Buna Markov karar süreci deniyor ve bunun kulağa büyük, teknik açıdan karmaşık bir terim gibi geldiğini biliyorum. Ancak bu Markov karar süreci veya kısaca MDP terimini duyarsanız, bu son birkaç videoda bahsettiğimiz biçimciliktir. MDP veya Markov karar sürecindeki Markov terimi, geleceğin mevcut duruma gelmeden önce gerçekleşmiş olabilecek herhangi bir şeye değil, yalnızca mevcut duruma bağlı olduğunu ifade eder. Başka bir deyişle, bir Markov karar sürecinde gelecek, buraya nasıl geldiğinize değil, yalnızca şu anda nerede olduğunuza bağlıdır. Markov karar süreci formalizmini düşünmenin bir başka yolu da, kontrol etmek istediğimiz bir robotumuz veya başka bir aracımız var ve yapacağımız şey a eylemlerini seçmek ve bu eylemlere dayalı olarak dünyada veya dünyada bir şeyler olacak. çevre, örneğin dünyadaki konumumuz değişiyor ya da bir kaya parçasını örnek alıp bilim görevini yerine getiriyoruz. A eylemini seçme şeklimiz Pi politikasıdır ve dünyada olup bitenlere bağlı olarak hangi durumda olduğumuzu ve hangi ödülleri aldığımızı görebilir veya gözlemleyebiliriz. Bazen farklı yazarların Markov karar sürecini veya MDP biçimciliğini temsil etmek için bunun gibi bir diyagram kullandığını görürsünüz, ancak bu, son birkaç videoda öğrendiğiniz kavramlar dizisini göstermenin başka bir yoludur. Artık pekiştirmeli öğrenme probleminin nasıl çalıştığını biliyorsunuz. Bir sonraki videoda, iyi eylemler seçmek için bir algoritma geliştirmeye başlayacağız. Buna yönelik ilk adım, durum eylem değeri işlevini tanımlamak ve sonunda hesaplamayı öğrenmek olacaktır. Bu, bir öğrenme algoritması geliştirmek istediğimizde kilit niceliklerden biri olarak ortaya çıkıyor. Bunun ne olduğunu görmek için bir sonraki videoya geçelim, durum eylem değeri fonksiyonu.


