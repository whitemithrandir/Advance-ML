## What is Reinforcement Learning?
Makine öğrenimi uzmanlığının bu son haftasına hoş geldiniz. Bu uzmanlığın sonuna yaklaşıyor olmamız benim için biraz buruk ama bu haftayı sabırsızlıkla bekliyorum, sizinle takviyeli öğrenme hakkında bazı heyecan verici fikirleri paylaşıyorum. Makine öğreniminde, takviye öğrenimi, bugün henüz ticari uygulamalarda çok yaygın olarak uygulanmasa da makine öğreniminin temel direklerinden biri olan fikirlerden biridir. Ve onu destekleyen ve her gün geliştiren çok sayıda heyecan verici araştırma var. Pekiştirmeli öğrenmenin ne olduğuna bir göz atarak başlayalım. Bir örnekle başlayalım. İşte otonom bir helikopterin resmi. Bu aslında Stanford otonom helikopteri, 32 pound ağırlığında ve şu anda ofisimde duruyor. Diğer birçok otonom helikopter gibi, yerleşik bir bilgisayar, GPS, ivmeölçerler, jiroskoplar ve manyetik pusula ile donatılmıştır, böylece her zaman nerede olduğunu oldukça doğru bir şekilde bilir. Size bu helikopterin anahtarlarını versem ve onu uçurmak için bir program yazmanızı istesem, bunu nasıl yaparsınız? Radyo kontrollü helikopterler bunun gibi joysticklerle kontrol edilir ve bu nedenle görev saniyede on kez size helikopterin konumu, yönü ve hızı verilir. Ve helikopteri havada dengede tutabilmek için bu iki kontrol çubuğunu nasıl hareket ettireceğinize karar vermelisiniz. Bu arada, radyo kontrollü helikopterlerin yanı sıra dört rotorlu dronları kendim uçurdum. Ve radyo kontrollü helikopterleri uçurmak, havada dengede tutmak aslında biraz daha zordur. Peki bunu otomatik olarak yapacak bir programı nasıl yazarsınız? Size Stanford otonom helikopterimizle yapacağımız bir şeyin eğlenceli bir videosunu göstereyim. İşte bir takviyeli öğrenme algoritmasının kontrolü altında uçtuğu bir video. Ve videoyu oynatmama izin ver. O gün aslında kameraman bendim ve bu bilgisayar kontrolünde uçan helikopter ve videoyu uzaklaştırırsam gökyüzüne dikilmiş ağaçları görüyorsunuz. Takviyeli öğrenmeyi kullanarak, aslında baş aşağı uçmayı öğrenmek için bu helikopteri aldık. Baş aşağı uçmasını söyledik. Ve böylece güçlendirilmiş öğrenme, helikopterlerin çok çeşitli akrobasi hareketlerini uçurmasını sağlamak için kullanıldı ya da biz onlara akrobasi manevraları diyoruz. Bu arada, diğer videoları görmekle ilgileniyorsanız, bu URL'den onlara da göz atabilirsiniz. Peki pekiştirmeli öğrenmeyi kullanarak bir helikopterin kendi kendine uçmasını nasıl sağlarsınız? Görev, kontrol çubuklarını nasıl hareket ettireceğine karar vermek için helikopterin konumuna verilir. Takviyeli öğrenmede, helikopterin konumu, yönü ve hızı vb. durumlarını s olarak adlandırırız. Ve böylece görev, helikopterin durumundan eylem a'ya eşlenen bir işlev bulmaktır, yani helikopteri havada dengede tutmak, uçmak ve çarpmadan tutmak için iki kontrol çubuğunun ne kadar uzağa itilmesi gerektiği anlamına gelir. Bu sorunu denemenin bir yolu denetimli öğrenmeyi kullanmaktır. Bunun otonom helikopter uçuşu için harika bir yaklaşım olmadığı ortaya çıktı. Ama diyebilirsiniz ki, bir dizi durum gözlemi alabilirsek ve belki de uzman bir insan pilot bize yapılacak en iyi eylemin ne olduğunu söyler. Daha sonra, burada x olarak adlandırdığım durumlardan, burada y olarak adlandırdığım bir eyleme eşlemeyi doğrudan öğrenmek için denetimli öğrenmeyi kullanarak bir sinir ağını eğitebilirsiniz. Ancak, helikopter havada hareket ederken aslında çok belirsiz olduğu ortaya çıktı, yapılacak doğru eylemin tam olarak ne olduğu. Biraz sola mı yoksa çok daha fazla sola mı yatırıyorsunuz veya helikopter stresini biraz mı yoksa çok mu artırıyorsunuz? Bir x veri seti ve ideal eylem y elde etmek aslında çok zordur. Bu nedenle, helikopter ve diğer robotlar gibi bir robotu kontrol etme görevinin çoğunda, denetimli öğrenme yaklaşımı iyi çalışmıyor ve bunun yerine takviyeli öğrenme kullanıyoruz. Takviyeli öğrenme için önemli bir girdi, helikoptere ne zaman iyi ve ne zaman kötü gittiğini söyleyen ödül veya ödül işlevi olarak adlandırılan bir şeydir. Bu yüzden, ödül işlevini düşünmeyi sevdiğim şekilde, bir köpeği eğitmeye benziyor. Ben büyürken, ailemin bir köpeği vardı ve benim işim köpeği ya da yavruyu terbiye etmekti. Peki bir köpek yavrusu nasıl iyi davranır? Pekala, köpeğe bu kadarını gösteremezsin. Onun yerine işini yapmasına izin veriyorsun ve ne zaman iyi bir şey yapsa, sen gidiyorsun, ahbap. Ve ne zaman kötü bir şey yapsalar, gidersin, kötü köpek. Ve sonra umarım kendi kendine nasıl daha çok iyi köpek ve daha az kötü köpek işi yapılacağını öğrenir. Takviyeli öğrenme algoritması ile eğitim işte böyle bir şey. Helikopter iyi uçtuğunda, gidersiniz, iyi helikopter ve çarpma gibi kötü bir şey yaparsa, gidersiniz, kötü helikopter. Ve sonra, iyi helikopterden daha fazlasını ve kötü helikopter sonuçlarından daha azını nasıl elde edeceğinizi bulmak, pekiştirmeli öğrenme algoritmasının işidir.
Takviyeli öğrenmenin neden bu kadar güçlü olduğunu düşünmenin bir yolu, ona nasıl yapılacağından çok ne yapması gerektiğini söylemeniz gerektiğidir. Ve en uygun eylem yerine ödül işlevini belirtmek, sistemi nasıl tasarladığınız konusunda size çok daha fazla esneklik sağlar. Helikopteri uçurmak için somut olarak, ne zaman iyi uçuyorsa, ona iyi uçtuğu her saniye artı bir ödül verebilirsiniz. Ve belki de ne zaman kötü uçuyorsa, ona negatif bir ödül verebilirsiniz ya da düşerse, negatif 1000 gibi çok büyük bir negatif ödül verebilirsiniz. Ve bu, helikopteri iyi uçmak için çok daha fazla zaman harcamaya ve umarım asla düşmemeye teşvik eder. Ama işte başka bir eğlenceli video. Uzun yıllardır pekiştirmeli öğrenme için iyi köpek kötü köpek benzetmesini kullanıyordum. Ve sonra bir gün gerçekten robotik bir köpeğe el atmayı başardım ve bu pekiştirmeyi gerçekten iyi köpek kötü köpek öğrenme metodolojisini bir robot köpeği engelleri aşması için eğitmek için kullanabilirdim. Bu, onu ödüllendiren takviyeli öğrenmeyi kullanarak ekranın soluna doğru hareket ederek ayaklarını dikkatli bir şekilde yerleştirmeyi veya çeşitli engellerin üzerinden nasıl tırmanacağını öğrenen bir robot köpeğin videosu. Ve bir köpeği böyle programlamak için ne gerektiğini düşünürseniz, hiçbir fikrim yok, belirli bir engeli aşmak için bacaklarını yerleştirmenin en iyi yolunun ne olduğunu ona nasıl söyleyeceğimi gerçekten bilmiyorum. Bütün bunlar, robot tarafından sadece onu teşvik eden ödüller vererek, ekranın solundaki hedefe doğru ilerleme kaydederek otomatik olarak çözüldü. Günümüzde pekiştirmeli öğrenme, robotları kontrol etmekten çok çeşitli uygulamalara başarıyla uygulanmaktadır. Ve aslında bu haftanın ilerleyen saatlerinde uygulama laboratuvarında, simülasyonda bir aya iniş yapmak için kendinize bir pekiştirmeli öğrenme algoritması uyguluyorsunuz. Fabrika optimizasyonu için de kullanıldı. İş hacmini ve verimliliği ve finansal hisse senedi alım satımını en üst düzeye çıkarmak için fabrikadaki şeyleri nasıl yeniden düzenlersiniz? Örneğin, bir arkadaşım verimli stok uygulaması üzerinde çalışıyordu. Dolayısıyla, önümüzdeki birkaç gün içinde bir milyon hisse satmaya karar verdiyseniz, borsada aniden bir milyon hisseyi düşürmek istemeyebilirsiniz çünkü bu, fiyatları aleyhinize hareket ettirecektir. Öyleyse, satmak istediğiniz hisseleri satabilmeniz ve onlar için mümkün olan en iyi fiyatı alabilmeniz için alım satımlarınızı zaman içinde sıralamanın en iyi yolu nedir? Son olarak, pek çok video oyunu oynamanın yanı sıra damadan satranca, briç kart oyununa kadar oyun oynamaya kadar pek çok takviyeli öğrenme uygulaması da olmuştur. İşte bu pekiştirmeli öğrenmedir. Takviyeli öğrenme, neredeyse denetimli öğrenme kadar kullanılmasa da, günümüzde hala birkaç uygulamada kullanılmaktadır. Ve ana fikir, algoritmaya her bir girdi için doğru y çıktısının ne olduğunu söylemeniz yerine, tek yapmanız gereken ona ne zaman iyi gittiğini ve ne zaman kötü gittiğini söyleyen bir ödül işlevi belirtmektir. Ve iyi eylemlerin nasıl seçileceğini otomatik olarak bulmak algoritmanın işidir. Bununla birlikte, pekiştirmeli öğrenme problemini formüle edeceğimiz ve aynı zamanda iyi eylemleri otomatik olarak seçmek için algoritmalar geliştirmeye başlayacağımız bir sonraki videoya geçelim.

## Mars rover example
Takviyeli öğrenme şekilciliğini bitirmek için, bir helikopter veya robot köpek gibi karmaşık bir şeye bakmak yerine, genel olarak Mars gezicisinden ilham alan basitleştirilmiş bir örnek kullanabiliriz. Bu, Stanford profesörü Emma Branskill ve iş arkadaşlarımdan biri olan Jagriti Agrawal'ın şu anda Mars gezicisini gerçekten kontrol eden bir kod yazması nedeniyle örnekten uyarlandı ve bu da bu örneği geliştirmeme yardımcı oldu ve konuşmama yardımcı oldu. Hadi bir bakalım. Mars gezicisinden esinlenen basitleştirilmiş bir örnek kullanarak pekiştirmeli öğrenme geliştireceğiz. Bu uygulamada, gezici, buradaki altı kutuda gösterildiği gibi altı konumdan herhangi birinde olabilir. Gezici, örneğin, burada gösterilen dördüncü kutuya doğru yola çıkabilir. Mars gezicisinin konumu pekiştirmeli öğrenmede durum olarak adlandırılır ve ben bu altı durumu, durum 1, durum 2, durum 3, durum 4, durum 5 ve durum 6 olarak adlandıracağım ve böylece gezici başlıyor 4. durumda kapalı. Şimdi gezici, farklı bilim görevlerini yerine getirmeye çalışmak için Mars'a gönderildi. Gezegendeki farklı yerlerdeki kayayı analiz etmek için matkap, radar veya spektrometre gibi sensörlerini kullanmak üzere farklı yerlere gidebilir veya dünyadaki bilim adamlarının bakması için ilginç fotoğraflar çekmek üzere farklı yerlere gidebilir. Bu örnekte, burada soldaki durum 1, bilim adamlarının yüzey aracının örneklemesini isteyeceği çok ilginç bir yüzeye sahiptir. Durum 6 ayrıca, bilim adamlarının gezicinin örneklemesini isteyeceği oldukça ilginç bir yüzeye sahiptir, ancak durum 1 kadar ilginç değildir. Bilim görevini ve durum 1'i gerçekleştirme olasılığımız, durum 6'dan daha fazladır, ancak durum 1 daha uzaktadır. . Durum 1'in potansiyel olarak daha değerli olduğunu yansıtmanın yolu, ödül işlevidir. 1. durumdaki ödül 100'dür ve 6. aşamadaki ödül 40'tır ve aradaki diğer tüm eyaletlerdeki ödüller, sıfır ödülü olarak yazacağım çünkü bilim için o kadar ilginç bir şey yok. 2, 3, 4 ve 5 durumlarında yapılabilir. Her adımda, gezici iki eylemden birini seçer. Ya sola gidebilir ya da sağa gidebilir. Soru şu ki, gezici ne yapmalı? Takviyeli öğrenmede, ödüllere çok dikkat ederiz çünkü robotun iyi mi kötü mü yaptığını bu şekilde anlarız. Robot 4. durumdan başlayarak sola giderse ne olabileceğine dair bazı örneklere bakalım. Ardından başlangıçta 4. durumdan başlayarak, sıfır ödülü alacak ve sola gittikten sonra 3. duruma geçecek ve burada tekrar sıfır ödül alır. Sonra 2 durumuna gelir, 0 ödülünü alır ve son olarak 100'lük bir ödül aldığı 1 durumuna gelir. Bu uygulama için, ya 1. ya da 6. duruma geldiğinde, gün biter Takviyeli öğrenmede, buna bazen bir son durum diyoruz ve bunun anlamı, bu uç durumlardan birine ulaştıktan sonra, o durumda bir ödül alıyor, ancak bundan sonra hiçbir şey olmuyor. Belki robotların yakıtı bitmiştir veya o gün için zamanları kalmamıştır, bu yüzden sadece 100 veya 40 ödülünün tadını çıkarabilir, ama o gün için bu kadar. Bundan sonra ek ödüller kazanamaz. Artık robot sola gitmek yerine sağa gitmeyi de seçebilir, bu durumda 4 durumundan önce sıfır ödülü alır ve sonra sağa hareket eder ve 5 durumuna geçer, başka bir ödül alır. sıfır ve sonra sağdaki bu diğer terminal durumuna, durum 6'ya ulaşacak ve 40'lık bir ödül alacak. Ancak sola gitmek ve sağa gitmek tek seçenek. Robotun yapabileceği bir şey, 4. durumdan başlayıp sağa doğru hareket etmeye karar verebilmesidir. Durum 4-5'ten gider, durum 4'te sıfır ve durum 5'te sıfır ödülü alır ve sonra belki fikrini değiştirir ve aşağıdaki gibi sola gitmeye karar verir, bu durumda 4. durumda, 3. durumda, 2. durumda sıfır ödülü ve ardından 1. duruma geldiğinde 100 ödül. Bu eylemler ve durumlar dizisinde, robot daha iyi zaman harcıyor. Bu, harekete geçmek için harika bir yol olmayabilir, ancak algoritmanın seçebileceği bir seçimdir, ancak umarım bunu seçmezsiniz. Özetlemek gerekirse, her adımda, robot bir durumda, ben buna S diyeceğim ve bir eylem seçiyor ve ayrıca bu durumdan aldığı bazı ödüllerden, R of S'den keyif alıyor. Bu eylemin bir sonucu olarak, yeni bir S üssü durumuna geçer. Somut bir örnek olarak, robot 4. durumdayken ve sola git eylemini yaptığında, belki o 4. durumla ilişkili sıfır ödülünün tadını çıkarmadı ve yeni bir 3. durumu olmayacak. Özel pekiştirmeli öğrenme algoritmalarında, bu dört şeyin, durum, eylem, ödül ve sonraki durum olduğunu görürsünüz; bu, temel olarak her eylemde bulunduğunuzda olan şeydir ve takviyeli öğrenme algoritmalarının karar verirken bakacağı temel unsurlardır. nasıl önlemler alınır.
Açıklığa kavuşturmak için, buradaki ödül, R of S, bu, bu durumla ilişkili ödüldür. Bu sıfır ödülü, durum 3'ten ziyade durum 4 ile ilişkilendirilir. Takviyeli öğrenme uygulamasının çalışma şekli budur. Bir sonraki videoda, pekiştirmeli öğrenme algoritmasının yapmasını istediğimiz şeyi tam olarak nasıl belirttiğimize bir göz atalım. Özellikle pekiştirmeli öğrenmede geri dönüş adı verilen önemli bir fikirden bahsedeceğiz. Bunun ne anlama geldiğini görmek için bir sonraki videoya geçelim.

## The Return in reinforcement learning
Bir önceki videoda pekiştirmeli öğrenme uygulamasında durumların neler olduğunu ve yaptığınız işlemlere bağlı olarak nasıl farklı durumlardan geçtiğinizi ve farklı ödüller kazandığınızı gördünüz. Ancak belirli bir ödül setinin farklı bir ödül setinden daha iyi veya daha kötü olduğunu nasıl anlarsınız? Bu videoda tanımlayacağımız pekiştirmeli öğrenmedeki geri dönüş, bunu yakalamamızı sağlıyor. Bunu incelerken, faydalı bulabileceğiniz bir benzetme şu: Ayağınızın dibinde beş dolarlık bir banknot olduğunu hayal edin, aşağı uzanıp alabilirsiniz veya yarım saatte kasabanın öbür ucuna, yarım saatte yürüyebilir ve 10 dolarlık bir banknot al. Hangisinin peşinden gitmeyi tercih edersin? On dolar, beş dolardan çok daha iyidir, ancak gidip o 10 dolarlık banknotu almak için yarım saat yürümeniz gerekiyorsa, o zaman belki onun yerine beş dolarlık banknotu almanız daha uygun olabilir. Geri dönüş kavramı, daha hızlı alabileceğiniz ödüllerin, ulaşmanız uzun zaman alan ödüllerden belki daha çekici olduğunu yakalar. Bunun tam olarak nasıl çalıştığına bir göz atalım. İşte bir Mars Rover örneği. Durum 4'ten başlayarak sola giderseniz, aldığınız ödüllerin durum 4'ten ilk adımda sıfır, durum 3'ten sıfır, durum 2'den sıfır ve ardından uç durum olan durum 1'de 100 olacağını gördük. Getiri, bu ödüllerin toplamı olarak tanımlanır, ancak indirim faktörü olarak adlandırılan ek bir faktörle ağırlıklandırılır. İndirgeme faktörü 1'den biraz küçük bir sayıdır. İndirgeme faktörü olarak 0.9'u seçeyim. İlk adımdaki ödülü sıfır olarak ağırlıklandıracağım, ikinci adımdaki ödül bir indirim faktörü, bu ödülün 0,9 katı ve sonra artı indirim faktörü^2 çarpı bu ödül ve sonra artı indirim faktörü ^ 3 kat bu ödül. Bunu hesaplarsanız 0,729 çarpı 100 yani 72,9 çıkıyor. Geri dönüşün daha genel formülü, eğer robotunuz bir dizi durumdan geçerse ve birinci adımda R_1, ikinci adımda R_2 ve üçüncü adımda R_3 vb. alırsa, o zaman dönüş R_1 olur. artı indirim faktörü Gama, bu örnekte 0,9 olarak ayarladığım bu Yunan alfabesi Gamma, Gama çarpı R_2 artı Gama^2 çarpı R_3 artı Gamma^3 çarpı R_4 vb. siz terminal durumuna gelene kadar. İndirgeme faktörü Gamma'nın yaptığı şey, takviyeli öğrenme algoritmasını biraz sabırsız hale getirme etkisine sahip olmasıdır. Çünkü geri dönüş birinci ödüle tam kredi veriyor yüzde 100 yani 1 katı R_1 ama sonra biraz daha az kredi veriyor ikinci adımda aldığınız ödül 0,9 ile çarpılıyor ve daha sonra aldığınız ödüle daha da az kredi veriyor. bir sonraki adım R_3'te vb. alın ve böylece ödülleri daha erken almak, toplam getiri için daha yüksek bir değerle sonuçlanır. Pek çok takviyeli öğrenme algoritmasında, indirgeme faktörü için ortak bir seçim, 0,9 veya 0,99 veya hatta 0,999 gibi 1'e oldukça yakın bir sayı olacaktır. Ancak kullanacağım çalışan örnekte açıklama amacıyla, aslında 0,5'lik bir indirgeme faktörü kullanacağım. Bu çok ağır bir şekilde, gelecekte ödülleri çok ağır bir şekilde düşürür, çünkü her ek ayrıştırma zaman damgasıyla, bir adım önce alacağınız ödüllerin yalnızca yarısı kadar kredi alırsınız. Gama 0,5'e eşit olsaydı, yukarıdaki örnekte getiri 0 artı 0,5 çarpı 0 olurdu, bu denklemi üste koyar, artı 0,5^2 0 artı 0,5^3 çarpı 100. Durum 1'den uç duruma geçtiği için bu ödül kaybedildi, ve bu 12.5'lik bir getiri olarak çıkıyor. Finansal uygulamalarda iskonto faktörünün de faiz oranı veya paranın zaman değeri olarak çok doğal bir yorumu vardır. Bugün bir dolarınız varsa, bu, gelecekte yalnızca bir dolarınız olacak duruma göre biraz daha değerli olabilir. Çünkü bugün bir doları bile bankaya yatırabilir, biraz faiz kazanabilir ve bundan bir yıl sonra biraz daha fazla paraya sahip olabilirsiniz. Finansal uygulamalar için, genellikle, bu iskonto faktörü, bugün bir dolarla karşılaştırdığımda gelecekte bir doların ne kadar az olduğunu temsil eder. Bazı somut getiri örneklerine bakalım. Aldığınız geri dönüş, ödüllere bağlıdır ve ödüller, yaptığınız eylemlere bağlıdır ve dolayısıyla geri dönüş, yaptığınız eylemlere bağlıdır. Her zamanki örneğimizi kullanalım ve bu örnek için diyelim ki, ben her zaman sola gideceğim. Robot 4 durumunda başlarsa, önceki slaytta hesapladığımız gibi dönüşün 12,5 olduğunu daha önce görmüştük. Üçte başlasaydı, getiri 25 olurdu çünkü 100 ödüle bir adım daha erken ulaşır ve bu nedenle daha az iskonto edilir. Durum 2'de başlasaydı, dönüş 50 olurdu. Yeni başlayıp durum 1 olsaydı, pekala, hemen 100'lük ödülü alır, yani düşük indirimli olmaz.
Durum 1'den başlarsak getiri 100 olur ve bu iki durumdaki dönüş 6.25 olur. Görünüşe göre, son durum olan 6 durumundan başlarsanız, sadece ödülü ve dolayısıyla 40'ın geri dönüşünü alırsınız. Şimdi, farklı bir eylemler dizisi yapacak olsaydınız, geri dönüşler aslında farklı olurdu. Örneğin, her zaman sağa gidersek, eylemlerimiz bunlar olsaydı, o zaman 4. durumda başlarsanız, 0 ödül alırsınız. Sonra 5. duruma gelirsiniz, 0 ödül alırsınız ve 6 durumuna gelir ve 40'lık bir ödül alır. Bu durumda getiri 0 artı 0,5 olur, iskonto faktörü çarpı 0 artı 0,5'in karesi çarpı 40 olur ve bu da 0,5'in karesinin 1/4 olduğu ortaya çıkar, yani 40'ın 1/4'ü 10'dur. Bu halden, 4. halden getirisi 10'dur. Harekete geçecekseniz daima sağa gidin. Benzer bir mantıkla bu durumdan getiri 20, bu durumdan getiri beş, bu durumdan getiri 2,5 ve sonra dönüş determinant durumu 140 olur. tamamen mantıklıysa, videoyu duraklatıp matematiği iki kez kontrol etmekten çekinmeyin ve bunların geri dönüş için uygun değerler olduğuna kendinizi ikna edip edemeyeceğinize bakın. Çünkü farklı hallerden yola çıkarsanız ve her zaman sağa gidecekseniz. Her zaman sağa gittiğini görüyoruz. Almayı beklediğiniz getiri çoğu eyalet için daha düşüktür. Belki de her zaman sağa gitmek, her zaman sola gitmek kadar iyi bir fikir değildir. Ama her zaman sola gitmek zorunda olmadığımız, her zaman sağa gittiğimiz ortaya çıktı. Ayrıca 2. durumda olup olmadığınıza da karar verebiliriz, sola gidin. Durum 3'teyseniz, sola gidin. 4. durumdaysanız sola gidin. Ancak 5. durumdaysanız, bu ödüle çok yakınsınız demektir. Sağa gidelim. Bu, içinde bulunduğunuz duruma göre yapılacak eylemleri seçmenin farklı bir yolu olacaktır. Farklı durumlardan alacağınız getiri 100, 50, 25, 12.5, 20 ve 40 olacaktır. Bir vaka. 5. durumda başlayacak olsaydınız, burada sağa giderdiniz ve böylece aldığınız ödüller önce 5 durumunda sıfır, sonra 4 olur. Getiri sıfırdır, ilk ödül artı indirim faktörü 0,5 çarpı 40 yani 20 yani burada gösterilen işlemleri yaparsanız bu durumdan 20 dönüş neden oluyor. Özetlemek gerekirse, takviyeli öğrenmedeki geri dönüş, sistemin aldığı ödüllerin indirim faktörü ile ağırlıklandırılmış toplamıdır; burada uzak gelecekteki ödüller, indirim faktörü tarafından daha yüksek bir güce yükseltilerek ağırlıklandırılır. Şimdi, negatif ödülleri olan sistemleriniz olduğunda bunun aslında ilginç bir etkisi var. İncelediğimiz örnekte, tüm ödüller sıfır veya pozitifti. Ancak herhangi bir ödül negatifse, o zaman indirim faktörü aslında sistemi negatif ödülleri olabildiğince geleceğe itmeye teşvik eder. Mali bir örnek alırsak, birine 10$ ödemek zorunda kalsaydınız, bu belki eksi 10'luk bir negatif ödüldür. faiz oranı aslında bugün ödemek zorunda olduğunuz 10 dolardan daha az değerdedir. Negatif ödülleri olan sistemler için, algoritmanın ödülleri olabildiğince geleceğe itmeye çalışmasına neden olur. Finansal uygulamalar ve diğer uygulamalar için, bu aslında sistemin yapması gereken doğru şey olarak ortaya çıkıyor. Takviyeli öğrenmede geri dönüşün ne olduğunu artık biliyorsunuz, takviyeli öğrenme algoritmasının hedefini formüle etmek için bir sonraki videoya geçelim.

## Making decisions: Policies in reinforcement learning
Bir pekiştirmeli öğrenme algoritmasının eylemleri nasıl seçtiğini resmileştirelim. Bu videoda, takviyeli öğrenme algoritmasında bir politikanın ne olduğunu öğreneceksiniz. Hadi bir bakalım. Gördüğümüz gibi, pekiştirmeli öğrenme probleminde harekete geçmenin birçok farklı yolu vardır. Örneğin, her zaman en yakın ödül için gitmeye karar verebiliriz, böylece bu en soldaki ödül daha yakınsa sola gidersiniz veya bu en sağdaki ödül daha yakınsa sağa gidersiniz. Eylemleri seçmemizin başka bir yolu da, her zaman daha büyük ödül için gitmektir veya her zaman daha küçük ödül için gidebiliriz, iyi bir fikir gibi görünmüyor, ancak bu başka bir seçenek veya sadece değilseniz sola gitmeyi seçebilirsiniz. daha az ödülden bir adım uzaklaşırsanız, bu durumda onu tercih edersiniz. Takviyeli öğrenmede amacımız, işi herhangi bir durumu girdi olarak almak ve bunu bizden yapmamızı istediği bazı eylemlere haritalamak olan, politika Pi adı verilen bir işlev bulmaktır. Örneğin, alttaki bu politika için, bu politika, eğer 2. durumdaysanız, bizi sol eyleme eşler. 3. durumdaysanız, politika sola gidin diyor. 4. durumdaysanız ayrıca sola gidin ve 5. durumdaysanız sağa gidin. S durumuna uygulanan Pi, bize bu durumda hangi eylemi yapmamızı istediğini söyler. Takviyeli öğrenmenin amacı, getiriyi en üst düzeye çıkarmak için her durumda hangi eylemi yapmanız gerektiğini size söyleyen bir Pi veya Pi of S politikası bulmaktır. Bu arada, politikanın pi'nin ne olduğunu en açıklayıcı terim olup olmadığını bilmiyorum ama pekiştirmeli öğrenmede standart hale gelen terimlerden biri. Belki Pi'ye bir politika yerine bir denetleyici demek daha doğal bir terminoloji olur, ancak politika, pekiştirmeli öğrenmedeki herkesin şimdi buna dediği şeydir. Son videoda, durumlardan ödüllere, getirilere ve politikalara kadar pek çok pekiştirmeli öğrenme konseptini inceledik. Bir sonraki videoda bunları hızlı bir şekilde gözden geçirelim ve ardından bu politikaları bulmak için algoritmalar geliştirmeye başlayacağız. Bir sonraki videoya geçelim.

## Review of key concepts
Altı durumlu Mars gezgini örneğini kullanarak bir pekiştirmeli öğrenme formalizmi geliştirdik. Anahtar kavramlara hızlıca bir göz atalım ve bu kavram setinin diğer uygulamalar için de nasıl kullanılabileceğini görelim. Tartıştığımız kavramlardan bazıları, takviyeli öğrenme probleminin durumları, eylemler dizisi, ödüller, bir indirim faktörü, ardından ödüllerin ve indirim faktörünün birlikte getiriyi hesaplamak için nasıl kullanıldığı ve son olarak, işi kimin yaptığı bir politikadır. getiriyi en üst düzeye çıkarmak için eylemleri seçmenize yardımcı olmaktır. Mars gezgini örneği için 1-6 arasında numaralandırdığımız altı durumumuz vardı ve eylemler sola veya sağa gitmek şeklindeydi. Ödüller en soldaki durum için 100, en sağdaki durum için 40 ve arada sıfırdı ve 0,5'lik bir indirim faktörü kullanıyordum. Geri dönüş bu formül tarafından verildi ve Pi'nin eylemleri tasvir ettiği, içinde bulunduğunuz duruma bağlı olarak farklı politikaları olabilir. Aynı biçimcilik veya durumlar, eylemler, ödüller vb. birçok başka uygulama için de kullanılabilir. Sorunu çözün veya otonom bir helikopter bulun. Bir durum belirlemek, helikopterin olası konumları, yönelimleri ve hızları vb. kümesi olacaktır. Olası eylemler, bir helikopterin kontrol çubuğunu hareket ettirmenin olası yolları olabilir ve ödüller, iyi uçuyorsa artı bir ve gerçekten kötü düşmüyorsa veya çarpmıyorsa eksi 1.000 olabilir. Helikopterin ne kadar iyi uçtuğunu size söyleyen ödül işlevi. İndirgeme faktörü, birden biraz daha küçük bir sayı, diyelim ki, 0.99 ve sonra ödüllere ve indirim faktörüne bağlı olarak, getiriyi aynı formülü kullanarak hesaplarsınız. Takviyeli bir öğrenme algoritmasının işi, helikopterin konumu olan girdi olarak verildiğinde, size hangi eylemi yapmanız gerektiğini söyleyen Pi'nin bir politikasını bulmak olacaktır. Yani kontrol çubuklarını nasıl hareket ettireceğinizi anlatır. İşte bir örnek daha. İşte oyun oynayan bir tane. Satranç oynamayı öğrenmek için pekiştirmeli öğrenmeyi kullanmak istediğinizi varsayalım. Bu sorunun durumu, tahtadaki tüm taşların konumu olacaktır. Bu arada, satranç oynuyorsanız ve kuralları iyi biliyorsanız, bunun satranç için sadece taşların konumundan biraz daha fazla bilgi olduğunu biliyorum ama bu video için biraz basitleştireceğim. Eylemler, oyundaki olası yasal hamlelerdir ve bu durumda, sisteminize bir oyunu kazanırsa artı bir, oyunu kaybederse eksi bir ve eğer oyunu kaybederse eksi bir ödül verirseniz, ortak bir ödül seçimi olacaktır. bir oyun bağlar. Satranç için, genellikle bire çok yakın bir indirim faktörü kullanılacaktır, bu nedenle belki 0,99, hatta 0,995 veya 0,999 olabilir ve geri dönüş, diğer uygulamalarla aynı formülü kullanır. Bir kez daha, hedefe bir politika Pi kullanarak iyi bir eylem seçmesi için bir tahta pozisyonu verilir. Bir pekiştirmeli öğrenme uygulamasının bu şekilciliğinin aslında bir adı vardır. Buna Markov karar süreci deniyor ve bunun kulağa büyük, teknik açıdan karmaşık bir terim gibi geldiğini biliyorum. Ancak bu Markov karar süreci veya kısaca MDP terimini duyarsanız, bu son birkaç videoda bahsettiğimiz biçimciliktir. MDP veya Markov karar sürecindeki Markov terimi, geleceğin mevcut duruma gelmeden önce gerçekleşmiş olabilecek herhangi bir şeye değil, yalnızca mevcut duruma bağlı olduğunu ifade eder. Başka bir deyişle, bir Markov karar sürecinde gelecek, buraya nasıl geldiğinize değil, yalnızca şu anda nerede olduğunuza bağlıdır. Markov karar süreci formalizmini düşünmenin bir başka yolu da, kontrol etmek istediğimiz bir robotumuz veya başka bir aracımız var ve yapacağımız şey a eylemlerini seçmek ve bu eylemlere dayalı olarak dünyada veya dünyada bir şeyler olacak. çevre, örneğin dünyadaki konumumuz değişiyor ya da bir kaya parçasını örnek alıp bilim görevini yerine getiriyoruz. A eylemini seçme şeklimiz Pi politikasıdır ve dünyada olup bitenlere bağlı olarak hangi durumda olduğumuzu ve hangi ödülleri aldığımızı görebilir veya gözlemleyebiliriz. Bazen farklı yazarların Markov karar sürecini veya MDP biçimciliğini temsil etmek için bunun gibi bir diyagram kullandığını görürsünüz, ancak bu, son birkaç videoda öğrendiğiniz kavramlar dizisini göstermenin başka bir yoludur. Artık pekiştirmeli öğrenme probleminin nasıl çalıştığını biliyorsunuz. Bir sonraki videoda, iyi eylemler seçmek için bir algoritma geliştirmeye başlayacağız. Buna yönelik ilk adım, durum eylem değeri işlevini tanımlamak ve sonunda hesaplamayı öğrenmek olacaktır. Bu, bir öğrenme algoritması geliştirmek istediğimizde kilit niceliklerden biri olarak ortaya çıkıyor. Bunun ne olduğunu görmek için bir sonraki videoya geçelim, durum eylem değeri fonksiyonu.


## State-action value function definition
Bu hafta ilerleyen saatlerde takviyeli öğrenmeyi geliştirmeye başladığımızda, takviyeli öğrenme oklarının hesaplamaya çalışacağı önemli bir miktar olduğunu görüyorsunuz ve buna durum eylem değeri fonksiyonu deniyor. Şimdi bu fonksiyonun ne olduğuna bir göz atalım. Durum eylem değeri işlevi, tipik olarak büyük Q harfiyle gösterilen bir işlevdir. Ve bu, içinde olabileceğiniz bir durumun yanı sıra o durumda gerçekleştirmeyi seçebileceğiniz eylemin ve QFSA'nın bir işlevidir. Dönüşe eşit bir sayı verecektir. Bu durumda başlarsanız. S ve A eylemini yalnızca bir kez gerçekleştirin ve A eylemini bir kez gerçekleştirdikten sonra, bundan sonra en uygun şekilde davranın. Bundan sonra, mümkün olan en yüksek getiriyi sağlayacak her türlü eylemi yaparsınız. Şimdi, bu tanımda biraz garip bir şeyler olduğunu düşünebilirsiniz çünkü optimal davranışın ne olduğunu nasıl bilebiliriz? Ve otomatik davranışın ne olduğunu bilseydik, her durumda yapılacak en iyi eylemin ne olduğunu zaten bilseydik, neden hala Q of SA'yı hesaplamamız gerekiyor? Çünkü zaten otomatik politikamız var. Bu yüzden, bu tanımda biraz garip bir şey olduğunu kabul etmek istiyorum. Bu tanımla ilgili neredeyse biraz döngüsel bir şey var, ancak emin olun Belirli pekiştirmeli öğrenme çıktılarına baktığımızda, daha sonra bu biraz döngüsel tanımı çözeceğiz ve Q işlevini daha biz bulmadan önce hesaplamanın bir yolunu bulacağız. optimal politika. Ama bunu daha sonraki bir videoda görüyorsunuz. O yüzden şimdilik bu konuda endişelenme. Daha önce gördüğümüz bir örneğe bakalım, bu oldukça iyi bir politika 2., 3. ve 4. aşamadan sola gidin ve Beşinci Devletten sağa gidin. Bunun mars rover uygulaması için aslında en uygun politika olduğu ortaya çıktı İndirgeme faktörü gamma 0,5 olduğunda, yani S'nin Q'su toplam getiriye eşit olacaktır. daha sonrasında. Anlamı bu politikaya göre hareket etmek. Burada gösterildiği gibi, Q of s,a'nın ne olduğunu bulalım. Birkaç farklı eyalet içindir. Diyelim ki Q of state'e de bakalım. Peki ya sağa gitmek için harekete geçersek, eğer siz ikinci durumdaysanız ve sağa giderseniz, sonra üçüncü duruma gelirsiniz ve bundan sonra en uygun şekilde davranırsanız, ST üçten sola ve sonra sola gidersiniz. eyaletten duruma ve sonunda 100'lük ödülü alırsınız. Bu durumda, aldığınız ödüller eyaletten sıfıra, ikinci duruma geri döndüğünüzde üç sıfır kaldığınızda ve sonra nihayet geldiğinizde 100 olacaktır. uç durum bir ve dönüş sıfır artı 0,5 çarpı bunun artı 0,5 kare çarpı ac artı 0,5 küp çarpı 100 olacaktır. Bunun geçtiğini unutmayın, doğru gitmenin iyi bir fikir olup olmadığına dair bir yargı yoktur. Aslında, durumdan sağa gitmek o kadar iyi bir fikir değil, ancak A eylemini gerçekleştirirseniz ve ardından en uygun şekilde davranırsanız, dönüşü sadakatle bildirir. İşte başka bir örnek. Durumdaysanız ve sola gidecekseniz, ikinci durumdayken alacağınız ödül dizisi sıfır olacak ve ardından 100 olacaktır. Ve böylece dönüş sıfır artı 0,5 çarpı 100, bu da 50'ye eşittir QSA değerlerini yazmak için. Bu şemada, sağa gidişin Q olduğunu göstermek için sağa 12.5 yazacağım. Ve sonra, bunun ST 2'nin Q'su olduğunu göstermek için sola biraz 50 yazdığımda ve sadece bir örnek daha almak için sola gittiğimde Peki ya ST 4'teysek ve sola gitmeye karar verirsek? Dördüncü aşamadaysanız sola gidersiniz, sıfır ödül alırsınız ve sonra burada sola doğru harekete geçersiniz. Yani sıfır kazanç, burada sol harekete geç, sıfır ve sonra 100. Yani dört Soldan Q, sıfır ödülle sonuçlanır çünkü ilk eylem kaldı ve ardından daha sonra en uygun politikayı izlediğimiz için 00 100 ödüllendirebilirsiniz. Ve böylece dönüş sıfırdır. artı bunun 00,5 katı. Artı 4,5 kare çarpı bunun artı 0,5 Q çarpı bunun. Bu nedenle 12.5'e eşittir. Yani Q4 kaldı 12.5. Bu yılı 12.5 olarak yazacağım. Ve bu alıştırmayı diğer tüm durumlar ve diğer tüm eylemler için yaparsanız, bunun s,a'nın Q'su olduğunu görürsünüz. Farklı durumlar ve farklı eylemler için Ve son olarak Terminal Durumunda. Ne yaptığınız önemli değil, sadece 100 veya 40 terminal ödülü alırsınız. O yüzden terminal ödüllerini buraya yazın. Yani bu Q(s,a)'dır. Birden altıya kadar her durum durumu için ve iki eylem için, eylem sol ve eylem sağ. Durum eylem değeri işlevi neredeyse her zaman Q harfiyle belirtildiği için. Buna genellikle Q işlevi de denir. Dolayısıyla, Q. işlevi ve durum eylem değeri işlevi birbirinin yerine kullanılır ve size getirilerinizin ne olduğunu veya gerçekten değerin ne olduğunu söyler. ne kadar iyi? Sadece A ve ST S eylemlerini gerçekleştirin ve ardından en uygun şekilde davranın. Şimdi, Q işlevini bir kez hesaplayabildiğiniz zaman, bunun size eylemleri seçmek için de bir yol vereceği ortaya çıktı. İşte politika ve iade. Ve işte s,a'nın iki değeri. Önceki slayttan. Farklı durumlara baktığınızda ilginç bir şey fark ediyorsunuz, o da soldaki eylemi yapmak için durum alırsanız a,q ile sonuçlanır. Değer veya durum eylem değeri 50'dir ki bu aslında o durumdan alabileceğiniz en olası getiridir. Üç iki s,a durumunda. çünkü soldaki işlem aynı zamanda size daha yüksek getiri sağlar, bu nedenle soldaki işlem size istediğiniz getiriyi verir. Ve beşinci durumda, aslında size 20'lik daha yüksek getiriyi sağlayan, sağa giden eylemdir. Böylece, herhangi bir S durumundan mümkün olan en iyi getiri olduğu ortaya çıktı. Q,F, S'nin en büyük değeridir. Bunun net olduğundan emin olmak için söylediğim şu ki, kal için kal Dört durumundan ikisi kaldı ki bu da 12.5 Ve q. Bu da 10 olur. Ve bu iki değerden büyük olan 12.5, bu durumdaki dörtten mümkün olan en iyi getiridir. Başka bir deyişle, Dördüncü Durumdan almayı umabileceğiniz en yüksek getiri 12,5'tir. Ve aslında bu iki sayıdan 12,5 ve 10'dan daha büyük olanıdır. Ayrıca, Mars Rover'ınızın 10 demek yerine 12,5'lik bir dönüşün keyfini çıkarmasını istiyorsanız, yapmanız gereken eylem A eylemidir. Q s,a. Yani mümkün olan en iyi eylem durumu, eylem A'dır. Bu aslında Q, of s,a'yı maksimize eder. Bu size Q, of s,a'nın neden hesaplandığına dair bir ipucu verebilir. Daha sonra inşa edilecek pekiştirmeli öğrenme algoritmasının önemli bir parçasıdır. Yani Q(s,a)'yı hesaplamak için bir yolunuz varsa. Her durum ve her eylem için, bazı durumlardayken tek yapmanız gereken farklı A eylemlerine bakmaktır. Ve A eylemini seçin. Bu, Q of s,a'yı maksimize eder. Ve böylece pi F. S sadece A eylemini seçebilir. Bu, s,a'nın en büyük Q değerini verir. Ve bu iyi bir eylem olacak. Aslında en uygun eylem olduğu ortaya çıktı. Bunun neden mantıklı olduğuna dair bir başka sezgi de Qof s,a'dır. Eğer ani bir durum ve eyleme geçerseniz döndürülür A. Ve bundan sonra en uygun şekilde davranın. Yani mümkün olan en yüksek getiriyi elde etmek için, gerçekten istediğiniz şey A eylemini gerçekleştirmektir. Bu, en yüksek toplam getiriyle sonuçlanır. Bu nedenle, keşke Q f s,a'yı hesaplamanın bir yolu olsa. Bu koşullar altında getiriyi en üst düzeye çıkaran eylem yardımını alan her devlet için, o eyalette yapılacak en iyi eylem gibi görünüyor. Bu, bunun için bilmeniz gereken bir şey olmasa da. Çünkü şunu da belirtmek isterim ki internete bakarsanız veya pekiştirmeli öğrenme literatürüne bakarsanız bazen bu Q fonksiyonunun Q olarak yazıldığını görürsünüz. Bu terimler, tam olarak tanımladığımız gibi Q işlevine atıfta bulunur. Pekiştirmeli öğrenme literatürüne bakarsanız ve Q. Star veya Q fonksiyonu hakkında okursanız, bu sadece bahsettiğimiz durum eylem değeri fonksiyonu anlamına gelir. Ancak bu kursun amaçları açısından bunun için endişelenmenize gerek yok. Özetlemek gerekirse, Q of s,a'yı hesaplayabilirseniz. Her durum ve her eylem için, bu bize S'nin oto politikası pi'yi hesaplamak için iyi bir yol verir. Yani bu, durum eylem değeri işlevi veya Q işlevidir. Q fonksiyonunun tanımının biraz dairesel yönüne rağmen bunları hesaplamak için nasıl bir algoritma bulacağımız hakkında daha sonra konuşacağız. Ama önce bir sonraki videoda Q of s,a değerlerinin ne olduğuna dair bazı özel örneklere bakalım. aslında benziyor


## State-action value function example
Durum-eylem değeri işlevi örneğini kullanma. QSA değerlerinin nasıl olduğunu görüyorsunuz. Takviyeli öğrenme problemleri ve probleme bağlı olarak QSA değerlerinin nasıl değiştiği konusundaki sezgilerimizi sürdürmek için isteğe bağlı bir laboratuvar sağlanacaktır. Bu, [DUYULMUYOR] örneğini değiştirerek oynamanıza ve QSA'nın nasıl değişeceğini kendiniz görmenize olanak tanır. Hadi bir bakalım. İşte bu videoyu izledikten sonra oynamanızı umduğum bir Jupyter Notebook. Bu yardımcı işlevleri çalıştıracağım, şimdi burada dikkat edin, bu iki eylemin altı sayısını belirtiyor, böylece bunları değiştirmeyeceğim. Ve bu, 140 olan terminal sağ ödüllerinde sol terminali belirtir ve daha sonra ara durumların ödülleri sıfırdır. İndirim faktörü kumar 0.5. Şimdilik yanlış adım atma olasılığını göz ardı edelim, bundan sonraki bir videoda bahsedeceğiz. Ve bu değerlerle, bu kodu çalıştırırsanız bu, optimal politikanın yanı sıra SA'nın Q işlevini hesaplar ve görselleştirir. Q of SA'yı kendiniz tahmin etmek veya hesaplamak için bir öğrenme algoritmasının nasıl geliştirileceğini daha sonra öğreneceksiniz. Şimdilik Q of SA'yı hesaplamak için hangi kodu yazdığımızı dert etmeyin. Ama burada Q of SA değerlerinin derste gördüğümüz değerler olduğunu görüyorsunuz. Şimdi eğlence burada başlıyor. Bazı değerleri değiştirelim ve bu şeylerin nasıl değiştiğini görelim. Terminal sağ ödülünü çok daha küçük bir değere güncelleyeceğim sadece 10 diyor. Şimdi kodu tekrar çalıştırırsam, Q of SA'nın nasıl değiştiğine bakın ve şimdi 5 durumundaysanız bunu düşünür. ve en uygun şekilde davranın, 6.25 alırsınız. Oysa sağa giderseniz ve bundan sonra da davranırsanız, yalnızca beşlik bir getiri elde edersiniz. Şimdi sağdaki ödül çok küçükken, sadece 10. Size çok yakın olsanız bile, sonuna kadar sola gitmeyi tercih edin. Ve aslında otomobil politikası artık her eyaletten sola gitmek. Başka değişiklikler yapalım. Terminal hakkı ödülünü tekrar 40 olarak değiştireceğim. Ama indirim faktörünü bire yakın bir indirim faktörü ile 0.9 olarak değiştireyim. Bu, Mars Gezgini'ni daha az sabırsız yapar, daha yüksek bir ödül için daha uzun süre dayanmaya isteklidir çünkü gelecekte ödüller 0,5 ile çarpılmaz - bazı yüksek güçler 0,9 ile bazı yüksek güçlerle çarpılır. Ve daha fazla sabırlı olmaya isteklidir, çünkü gelecekte ödüller iskonto edilmez veya indirim 0,5 iken olduğu kadar küçük bir sayı ile çarpılmaz. Öyleyse kodu tekrar çalıştıralım. Ve şimdi bunun farklı eyaletler için Q of SA olduğunu görüyorsunuz ve şimdi durum 5 için sola gitmek size 36'ya kıyasla 65,61 gibi daha yüksek bir ödül veriyor. Bu arada 36'nın 40'lık bu terminal ödülünün 0,9 katı olduğuna dikkat edin. mantıklı olmak. Ama küçük bir hasta sola gitmeye istekli olduğunda, siz 5 durumundayken bile. Şimdi gammayı 0 .3 gibi çok daha küçük bir sayıya değiştirelim. Yani bu, gelecekte ödüllerde çok ağır indirimler yapar. Bu onu inanılmaz derecede sabırsız yapar. Bu kodu tekrar çalıştırmama izin verin ve şimdi davranış değişti. Şu anda 4. durumda olanın daha büyük 100 ödül için gidecek sabrı olmayacağı fark edildi, çünkü indirim faktörü gamma artık çok küçük, 0,3. Çok daha küçük bir ödül daha yakın olmasına rağmen 40'lık ödülü tercih eder ve biz de bunu yapmayı seçeriz. Umarım bu sayılarla kendiniz oynayarak ve bu kodu çalıştırarak bir fikir edinebilirsiniz. Q of SA değerleri nasıl değişir, fark ettiğiniz optimal getiri bu iki QSA sayısından daha büyük olanıdır. Bunun nasıl değiştiği ve optimal politikanın nasıl değiştiği. Bu yüzden umarım gidip isteğe bağlı laboratuvarla oynarsınız ve ödül işlevini değiştirirsiniz ve indirim faktörü gama'yı değiştirir ve farklı değerler denersiniz. Ve Q of SA değerlerinin nasıl değiştiğini, farklı durumlardan optimal getirilerin nasıl değiştiğini ve bu farklı değerlere bağlı olarak otomobil politikasının nasıl değiştiğini kendiniz görün. Ve bunu yaparak, pekiştirmeli öğrenme uygulamasında ödüllere bağlı olarak bu farklı miktarların nasıl etkilendiğine dair sezginizi keskinleştireceğinizi umuyorum. Siz laboratuvarda oynadıktan sonra, geri gelip takviyeli öğrenmede muhtemelen en önemli denklemin ne olduğu hakkında konuşmaya hazır olacağız, buna bellman denklemi denir. Umarım isteğe bağlı laboratuvarda eğlenirsiniz ve ondan sonra hadi geri gelip kapıcı denklemleri hakkında konuşalım.

## Bellman Equations
Nerede olduğumuzu özetleyeyim. Q of S, A durum eylem değeri işlevini hesaplayabilirseniz, bu size her sahneden iyi bir eylem seçmenin bir yolunu sunar. Sadece size en büyük Q of S,A değerini veren A eylemini seçin. Soru şu ki, bu Q of S,A değerlerini nasıl hesaplarsınız? Takviyeli öğrenmede, durum eylem değeri fonksiyonunu hesaplamamıza yardımcı olacak Bellman denklemi adı verilen bir anahtar denklem vardır. Şimdi bu denklemin ne olduğuna bir göz atalım. Bir hatırlatma olarak, bu Q of S, A'nın tanımıdır. S durumunda başlarsak, bir kez harekete geçersek ve bundan sonra en uygun şekilde davranırsak geri döner. Bellman denklemini açıklamak için aşağıdaki gösterimi kullanacağım. Mevcut durumu belirtmek için S kullanacağım. Şimdi, mevcut durumun ödüllerini belirtmek için R of S'yi kullanacağım. Küçük MDP örneğimiz için, bir Durum1'in r'sinin 100 olduğunu alacağız. Durum 2'nin ödülü 0'dır, vb. Durum 6'nın ödülü 40'tır. Mevcut eylemi, S durumunda gerçekleştirdiğiniz eylemi belirtmek için A alfabesini kullanacağım. a eylemini gerçekleştirdikten sonra, yeni bir duruma geçersiniz. Örneğin, Durum 4'teyseniz ve soldaki eylemi yaparsanız, Durum 3'e ulaşırsınız. Mevcut durum S'den a eylemini yaptıktan sonra ulaştığınız durumu belirtmek için S üssünü kullanacağım. Ayrıca A üssünü S üssü durumunda gerçekleştirebileceğiniz eylemi, ulaştığınız yeni sabiti belirtmek için kullanacağım. Bu arada gösterim kuralı, S,A'nın mevcut duruma ve eyleme karşılık gelmesidir. Asal sayıyı eklediğimizde, bu bir sonraki durum, ardından bir sonraki eylemdir. Bellman denklemi aşağıdaki gibidir. Q(S,A), yani r(S)'ye eşit olan bu varsayımlar dizisi altındaki getiri, bu durumda olduğunuz için aldığınız ödül artı iskonto faktörü Gamma çarpı tüm olası eylemler üzerinden maksimum, a üssü q S üssünün, ulaştığınız yeni durum ve ardından bir asal sayı. Bu denklemde çok şey oluyor. Önce bazı örneklere bir göz atalım. Bu denklemin neden anlamlı olabileceğini görmek için geri geleceğiz. Bir örneğe bakalım. Q of State 2'ye ve eyleme bakalım. Bize hangi değeri verdiğini görmek için buna Bellman Denklemini uygulayın. Mevcut durum ikinci durumsa ve eylem sağa gidecekse, o zaman ertesi gün S üssünü yazdıktan sonra Durum 3 olur. Bellman denklemi Q/2 der, sağ R(S)'dir. R Durumu 2, yalnızca ödül sıfır artı bu örnekte 0,5 olarak ayarladığımız Gamma indirim faktörü, Durum 3'teki S durumundaki Q değerlerinin maksimum katıdır. Bu, 25 ve 6.25'in maksimumu olacak , çünkü bu maksimum bölü q'nun bir üssü S üssü virgül a üssüdür. Bu, 25 veya 6.25'ten büyük olanı alıyor, çünkü bunlar Durum 3 için iki seçenek. Sadece bir örneğe daha bakalım. Durum 4'ü ele alalım ve sola gitmeye karar verirseniz, Durum 4'ün Q'sunun ne olduğunu görelim. Bu durumda, mevcut durum dört geçerli eylem sola gitmektir. Bir sonraki durum, eğer dörtten başlayabilirseniz sola gidiyor. Aynı zamanda Durum 3'te olursunuz. Bu üçü tekrar asal yapalım, Bellman Denklemi, bunun R(S)'ye eşit olduğunu söyleyeceğiz. Durum dört, sıfır artı 0.5 iskonto faktörü Gamma bölü a üssü q S üssü. Bu yine Durum 3, virgül ve asal. Bir kez daha, Durum 3 için Q değerleri 25 ve 6.25'tir ve bunların büyüğü 25'tir. Bu bizim 40 artı 0.5 çarpı 25 olur, bu da yine 12.5'e eşittir. Bu nedenle, kalan işlemle birlikte q/dört de 12,5'e eşittir, sadece bir nota, eğer uç durumdaysanız, o zaman Bellman Denklemi q(SA) eşittir r(S) şeklinde sadeleşir çünkü S üssü durumu yoktur ve böylece ikinci dönem gider. Bu nedenle Q of S,A terminal durumlarında sadece 100, 100 veya 40 40'tır. Dilerseniz videoyu duraklatabilir ve Bellman Denklemini bu MDP'deki diğer herhangi bir durum eylemine uygulayabilir ve bu matematiğin doğru olup olmadığını kendiniz kontrol edebilirsiniz. çalışır. Özetlemek gerekirse, Q of S,A'yı bu şekilde tanımlamıştık. Herhangi bir S durumundan mümkün olan en iyi dönüşün, S,A'nın bir Q'su üzerinden maksimum olduğunu daha önce görmüştük. Aslında, SNA'yı yeniden adlandırmak gerekirse, bir S asal durumundan mümkün olan en iyi getiri, bir asalın maks bölü S üssüdür. S, S üssünü ve a'yı asal olarak yeniden adlandırmaktan başka bir şey yapmadım. Ancak bu, daha sonra bazı sezgileri biraz daha kolaylaştıracaktır.
Ancak, Durum 3 gibi herhangi bir S üssü durumu için, örneğin Durum 3'ten mümkün olan en iyi getiri, Q'nun S üssü E üssünün tüm olası eylemlerinin maksimumudur. İşte yine Bellman denklemi. Bunun yakaladığı sezgi, eğer s durumundan başlıyorsanız ve a eylemini yapacaksanız ve ardından bundan sonra en uygun şekilde hareket edecekseniz, o zaman zaman içinde bir dizi ödül göreceksiniz. Özellikle getiri, birinci adımdaki ödül artı ikinci adımdaki Gama çarpı ödül artı üçüncü adımdaki Gama kare çarpı ödül vb. kullanılarak hesaplanacaktır. Artı nokta, nokta, nokta terminal durumuna gelene kadar. Bellman denkleminin söylediği şey, bu ödül dizisinin, indirim faktörünün ne olduğu, iki bileşene ayrılabileceğidir. Birincisi, bu R of s, hemen alacağınız ödül. Takviyeli öğrenme literatüründe buna bazen anlık ödül de denir, ancak R_1 budur. Bazı eyaletlerde başladığınız için aldığınız ödül. O halde ikinci terim şu şekildedir; s durumunda başlayıp a eylemini gerçekleştirdikten sonra, bazı yeni s asal durumlarına ulaşırsınız. Q of s a'nın tanımı, bundan sonra optimal davranacağımızı varsayar. s asal değerine ulaştıktan sonra, optimal davranacağız ve s asal durumundan mümkün olan en iyi getiriyi alacağız. Bu nedir, a üssünün maksimumu Q (s üssü) a üssü, bu s üssü durumundan başlayarak optimal davranmanın getirisidir. Tam olarak burada yazdığımız şey, eyaletin en iyi noktasından başladığınızda mümkün olan en iyi getiridir. Bunu ifade etmenin başka bir yolu, buradaki toplam getiri de R_1 artıya eşittir ve sonra haritada Gamma'yı çıkaracağız, yani Gamma çarpı R_2 artı ve Gamma kare yerine Gama çarpı R_3 artı Gama kare çarpı R_4 artı nokta nokta nokta. Prime durumundan başlıyorsanız, alacağınız ödül sırasının R_2, R_3, ardından R_4, vb. olacağına dikkat edin. Bu yüzden buradaki ifade, s asal durumundan başlarsanız toplam getiridir. Optimal şekilde davranacak olsaydınız, o zaman bu ifade, s üssü durumundan başlamak için mümkün olan en iyi getiri olmalıdır, bu nedenle bu indirgeme ödülleri dizisi, a üssünün Q (s üssü) a üssünün maksimum değerine eşittir ve arta kalanlar da vardır. bu ekstra indirgeme faktörü Gamma, bu yüzden Q(s,a) buradaki ifadeye de eşit. Bunun oldukça karmaşık olduğunu düşünüyorsanız ve tüm detayları takip etmiyorsanız, endişelenmeyin. Bu denklemi uyguladığınız sürece doğru sonuçlara ulaşmayı başaracaksınız. Ama üst düzey sezgiyi, pekiştirmeli öğrenme probleminde elde edeceğiniz toplam geri dönüşün iki bölümden oluştuğu şeklindedir. İlk kısım, hemen aldığınız bu ödüldür ve ardından ikinci kısım, Gama çarpı bir sonraki asal durumdan başlayarak elde ettiğiniz getiridir. Bu iki bileşen birlikte olduğu için, R(s) artı Gamma çarpı bir sonraki durumdan getiri, yani mevcut durum s'den toplam getiriye eşittir. Bellman denkleminin özü budur. Bunu daha önceki örneğimiz Q of 4 ile ilişkilendirmek için sola. Durum 4'ü başlatmanın ve sola gitmenin toplam getirisi budur. Durum 4'te sola giderseniz, aldığınız ödüller Durum 4'te 0, Durum 3'te 0, Durum 2'de 0 ve ardından 100'dür, bu nedenle toplam getiri budur; 0,5'in karesi artı 0,5'in küpü, yani 12,5'ti. Bellman denkleminin söylediği şey, bunu iki parçaya ayırabileceğimizdir. Bu sıfır var, bu da dört durumunun R'si ve sonra artı 0,5 çarpı bu diğer dizi, 0 artı 0,50 artı 0,5 kare çarpı 100. Ama bu dizinin ne olduğuna bakarsanız, bu gerçekten bir sonrakinden en uygun getiri. eyalet 4'ten sola doğru eylemi gerçekleştirdikten sonra ulaştığınız asal durum. Bu nedenle bu, ödül 4 artı Durum 3'ten elde edilen optimum getirinin 0,5 katına eşittir. Çünkü, Durum 3'ten başlayacak olsaydınız, alacağınız ödüller sıfır, ardından sıfır ve ardından 100 olurdu, yani bu Durumdan en uygun getiridir. 3 ve bu yüzden bu sadece R(4) artı 0.5 max bölü Durum 3'ün bir asal Q'su, yani bir asal. Bellman denklemini biliyorum, bu, toplam getirilerinizi hemen alacağınız ödüle bölen biraz karmaşık bir denklem. Anlık ödül artı Gama çarpı bir sonraki asal durumun getirileri. Size mantıklı geliyorsa, ancak tam olarak değilse, sorun değil. Endişelenme. Takviyeli bir öğrenme algoritmasının doğru çalışmasını sağlamak için Bellman'ın denklemlerini yine de uygulayabilirsiniz, ancak en azından ödülleri neden hemen elde ettiklerinize ve gelecekte elde edeceklerinize ayırdığınıza dair yüksek düzeyde bir sezgi olduğunu umuyorum. Umarım bu mantıklıdır. Takviyeli bir öğrenme algoritması geliştirmeye geçmeden önce, Stokastik Markov karar süreçleri veya eylemlerin, eğer yaparsanız, biraz rastgele bir etkiye sahip olabileceği takviyeli öğrenme uygulamaları hakkında isteğe bağlı bir video ile karşınızdayız. Dilerseniz isteğe bağlı videoya göz atın. Ondan sonra, bir pekiştirme dosyası geliştirmeye başlayacağız.

## Random (stochastic) environment
Bazı uygulamalarda, bir işlem yaptığınızda, sonuç her zaman tam olarak güvenilir olmayabilir. Örneğin, Mars gezicinize sola gitmesini emrederseniz, belki biraz kaya kayabilir veya zemin gerçekten kaygandır ve bu nedenle kayarak yanlış yöne gidebilir. Uygulamada, birçok robot, rüzgarın esmesi ve rotadan çıkması ve tekerleğin kayması veya başka bir şey nedeniyle her zaman onlara söylediklerinizi tam olarak yapmayı başaramaz. Şimdiye kadar bahsettiğimiz, rastgele veya stokastik ortamları modelleyen takviyeli öğrenme çerçevesinin bir genellemesi var. Bu isteğe bağlı videoda, basitleştirici Mars Rover örneğimizle devam ederek, bu pekiştirmeli öğrenme problemlerinin nasıl çalıştığından bahsedeceğiz, diyelim ki harekete geçtiniz ve ona sola gitmesini emrettiniz. Çoğu zaman başarılı olursunuz, ancak ya zamanın yüzde 10'u ya da zamanın 0,1'i kazara kayarak ters yöne giderse? Sola gitmesini emrederseniz, sola doğru gitme şansı yüzde 90 veya 0,9'dur. Ama gerçekten sağa gitme şansı 0,1, yani bu örnekte yüzde 9 ihtimalle üçüncü durumda ve yüzde 10 ihtimalle beşinci durumda bitiyor. Tersine, sağa gitmesini ve harekete geçmesini emrederseniz, doğru, 0.9 ihtimalle beşinci duruma ve 0.1 ihtimalle üçüncü duruma gelir. Bu, stokastik bir ortama bir örnek olacaktır. Bakalım bu pekiştirmeli öğrenme probleminde neler oluyor. Diyelim ki burada gösterilen bu politikayı kullanıyorsunuz, 2, 3, 4. aşamalarda sola gidip sağlara gidiyorsunuz veya beşinci durumda sağa gitmeye çalışıyorsunuz. Dördüncü eyaletten başlayacaksanız ve bu politikayı izleyecekseniz, o zaman ziyaret ettiğiniz eyaletlerin gerçek sırası rastgele olabilir. Örneğin, dördüncü durumda, sola gideceksiniz ve belki döngünüz ve şanslısınız ve aslında üçüncü durumu alıyor ve sonra tekrar sola gitmeye çalışıyorsunuz ve belki gerçekten oraya varıyor. Tekrar sola gitmesini söylüyorsunuz ve o duruma geliyor. Eğer olan buysa, sonunda 000100 ödül dizisine sahip olursunuz. Ancak aynı politikayı ikinci kez denerseniz, burada ikinci kez başladığınızda belki biraz daha az şanslı olursunuz. Sola gitmeye çalışın ve başarılı olduğunu görün, böylece dördüncü durumdan sıfır, üçüncü durumdan sıfır, sola gitmesini söylediğinizi duyun, ancak bu sefer şanssızsınız ve robot kayar ve bunun yerine dördüncü duruma geri döner. Sonra size sola, sola ve sola seslenmeniz ve sonunda 100'lük ödüle ulaşmanız öğretilir. Bu durumda, gözlemlediğiniz ödüllerin sırası bu olacaktır. Bu dörtten üçe dörtten üçe iki sonra bir, hatta mümkün, eğer dördüncü eyaletten sola gitmesini söylerseniz, ilk adımda bile şanssız olabilirsiniz ve sonunda kaydığı için beşinci duruma gidersiniz. . Sonra beşi belirtin, sağa gitmesini emredersiniz ve siz buraya geldiğinizde başarılı olur. Bu durumda, gördüğünüz ödül dizisi 0040 olacaktır, çünkü dörtten beşe gitti ve sonra altı diyor, daha önce dönüşü bu indirimli ödüllerin toplamı olarak yazmıştık. Ancak takviyeli öğrenme problemi stokastik olduğunda, kesin olarak gördüğünüz bir ödül dizisi yoktur, bunun yerine bu farklı ödüller dizisini görürsünüz. Stokastik pekiştirmeli bir öğrenme probleminde, ilgilendiğimiz şey getiriyi maksimize etmek değildir çünkü bu rastgele bir sayıdır. Bizim ilgilendiğimiz şey, indirimli ödüllerin toplamının ortalama değerini maksimize etmektir. Ortalama değerden kastım, poliçenizi alıp bin kez, 100.000 kez veya bir milyon kez denerseniz, bunun gibi birçok farklı ödül dizisi elde edersiniz ve bunların hepsinin ortalamasını alırsanız. indirimli ödüllerin toplamının farklı dizileri, o zaman buna beklenen getiri diyoruz. İstatistikte, beklenen terimi, ortalama demenin başka bir yoludur. Ancak bunun anlamı, indirimli ödüllerin toplamı açısından ortalama olarak almayı umduğumuz şeyi en üst düzeye çıkarmak istiyoruz. Bunun matematiksel gösterimi, bunu E olarak yazmaktır. E, R1 artı Gamma R2 artı'nın beklenen değeri anlamına gelir, vb. Takviyeli öğrenme algoritmasının işi, indirimli ödüllerin ortalamasını veya beklenen toplamını en üst düzeye çıkarmak için bir politika Pi seçmektir. Özetlemek gerekirse, bir stokastik takviyeli öğrenme probleminiz veya bir stokastik Markov karar süreciniz olduğunda amaç, beklenen getiriyi en üst düzeye çıkarmak için S durumunda hangi eylemi yapacağımızı bize söyleyen bir politika seçmektir. Bunun en son değişme şekli, bahsettiğimiz şey, Bellman denklemini biraz değiştirmesidir. Bellman denklemi aynen yazdığımız gibi. Ama şimdi fark şu ki, s durumunda a eylemini gerçekleştirdiğinizde, bir sonraki s asal durumunun rastgele olması. 3. durumdayken ve ona bir sonraki s üssünün soluna gitmesini söylediğinizde, bu 2. durum olabilir veya 4. durum olabilir.
S üssü artık rastgele, bu yüzden buraya ortalama bir işleç veya beklenmeyen bir işleç de koyduk. s durumundan elde edilen toplam getiri, a eylemini bir kez en iyi şekilde gerçekleştirerek, hemen aldığınız ödüle eşittir, aynı zamanda anında ödül artı indirim faktörü, Gama artı ortalama olarak almayı beklediğiniz değer olarak da adlandırılır. gelecek döner. Bu stokastik pekiştirmeli öğrenme problemlerinde ne olduğu konusunda sezginizi keskinleştirmek istiyorsanız. Az önce size gösterdiğim isteğe bağlı laboratuvara geri dönerdiniz, burada bu parametre yanlış adım problemi, Mars Rover'ınızın ona emrettiğinizden ters yönde gitme olasılığıdır. İkinci adım yanlış adım 0,1 dersek ve Not Defterini yeniden çalıştırırsak ve böylece buradaki bu sayılar en uygun getiridir, eğer mümkün olan en iyi eylemleri yapacaksanız, bu optimal politikayı uygulayın, ancak robot yanlış yöne adım atacaksa yüzde 10 zamanın ve bunlar, bu stokastik NTP için q değerleridir. Robotu eskisi kadar iyi kontrol edemediğiniz için bu değerlerin artık biraz daha düşük olduğuna dikkat edin. q değerleri ve optimal getiriler biraz düştü. Aslında, yanlış adım atma olasılığını artıracak olursanız, örneğin robotun yönlere gitmediği zamanın yüzde 40'ını söyleyin. Zamanın sadece yüzde 60'ına emrediyorsun. Söylediğiniz yere gider, sonra robot üzerindeki kontrol dereceniz düştüğü için bu değerler daha da düşer. İsteğe bağlı laboratuvarla oynamanızı ve yanlış adım olasılığının değerini değiştirmenizi ve bunun for dönüşü veya otomatik beklenen getiriyi ve ayrıca Q değerlerini, q s a'yı nasıl etkilediğini görmenizi tavsiye ederim. Şimdi, şimdiye kadar yaptığımız her şeyde, bu Markov karar sürecini, sadece altı eyaletli bu Mars gezicisini kullandık. Birçok pratik uygulama için durum sayısı çok daha fazla olacaktır. Bir sonraki videoda, şimdiye kadar bahsettiğimiz pekiştirmeli öğrenme veya Markov karar süreci çerçevesini alıp çok daha büyük ve özellikle sürekli durum uzayları ile bu çok daha zengin ve belki de daha ilginç problemlere genelleştireceğiz. Bir sonraki videoda buna bir göz atalım.

## Example of continuous state space applications
Uygulama laboratuvarında üzerinde çalıştığınız aya iniş uygulaması da dahil olmak üzere birçok robotik kontrol uygulaması sürekli durum uzaylarına sahiptir. Bunun ne anlama geldiğine ve bahsettiğimiz kavramı bu sürekli durum uzaylarına nasıl genelleyebileceğimize bir göz atalım. Kullandığımız basitleştirilmiş bir Mars gezici örneği, ayrı bir durumlar kümesi kullanıyorum ve bunun anlamı, basitleştirilmiş Mars gezgini yalnızca altı olası konumdan birinde olabilir. Ancak çoğu robot, altı veya herhangi bir ayrık sayıda konumdan birden fazlasında olabilir, bunun yerine çok sayıda sürekli değer konumundan herhangi birinde olabilir. Örneğin, Mars gezici bir çizgi üzerinde herhangi bir yerde olabilirse, konumu 0-6 kilometre arasında değişen bir sayı ile belirtilir ve aradaki herhangi bir sayı geçerlidir. Bu, sürekli durum uzayına bir örnek olacaktır, çünkü konum, 2,7 kilometre veya 4,8 kilometre veya sıfır ile altı arasında herhangi bir sayı gibi bir sayı ile temsil edilecektir. Başka bir örneğe bakalım. Bu örnek için bir arabayı veya kamyonu kontrol etme uygulamasını kullanacağım. İşte bir oyuncak araba, Rusya oyuncak kamyonu. Bu kızıma ait. Kendi kendine giden bir araba veya kendi kendine giden bir kamyon yapıyorsanız ve bunu sorunsuz bir şekilde sürmek için kontrol etmek istiyorsanız, o zaman bu kamyonun durumu, x konumu, y konumu, belki yönü gibi birkaç sayı içerebilir. . Ne tarafa bakıyor? Kamyonun yerde kaldığını varsayarsak, muhtemelen ne kadar yüksek, ne kadar yüksek olduğu konusunda endişelenmenize gerek yok. Bu durum x, y ve teta açısını içerecektir, ayrıca x yönündeki hızları, y yönündeki hızı ve ne kadar hızlı dönüyor olabilir. Saniyede bir derece mi dönüyor yoksa saniyede 30 derece mi dönüyor yoksa saniyede 90 derece hızla mı dönüyor? Bir kamyon veya araba için durum, bunun bu hat üzerinde kaç kilometre olduğu gibi yalnızca bir sayı içermeyebilir, ancak altı sayı da içerebilir, x konumu, y konumu, is yönü, hangisine gidiyorum Yunan alfabesi Theta'yı ve ayrıca x yönündeki hızını x nokta kullanarak göstereceğim, yani bu x koordinatının ne kadar hızlı değiştiği, y noktasının y koordinatının ne kadar hızlı değiştiği anlamına gelir ve son olarak , Arabanın açısının ne kadar hızlı değiştiği teta noktası. Oysa 60 Mars gezgini örneğinde, durum altı olası sayıdan yalnızca biriydi. Bir, iki, üç, dört, beş veya altı olabilir. Araba için durum, altı sayıdan oluşan bu vektörü içerecektir ve bu sayılardan herhangi biri, geçerli aralık dahilinde herhangi bir değeri alabilir. Örneğin, Teta sıfır ile 360 ​​derece arasında değişmelidir. Başka bir örneğe bakalım. Otonom bir helikopteri kontrol etmek için bir pekiştirmeli öğrenme algoritması oluşturuyorsanız, bir helikopterin konumunu nasıl karakterize edersiniz? Örnek olarak, burada yanımda küçük bir oyuncak helikopter var. Helikopterin konumlandırılması, bir helikopterin ne kadar kuzey veya güney olduğu gibi x konumunu, y konumunu içerecektir. Belki helikopterin doğu-batı ekseninde ne kadar uzakta olduğu ve ayrıca z, helikopterin yerden yüksekliği. Ancak, konumun dışında, helikopterin de bir yönü vardır ve geleneksel olarak, yönünü yakalamanın bir yolu, biri helikopter sırasını yakalayan üç ek sayı kullanmaktır. Sağa mı sola mı kayıyor? Saha, ileri mi yoksa yukarı mı, geri mi dönüyor ve son olarak pusula yönünün baktığı batıya doğru sapma. Kuzeye mi yoksa doğuya mı yoksa güneye mi yoksa batıya mı bakıyorsunuz? Özetlemek gerekirse, helikopterin durumu, örneğin kuzey-güney yönündeki konumu, doğu-batı yönünde konumlandırılmış olması, y'nin yerden yüksekliği ve ayrıca helikopterin sırası, eğimi ve ayrıca yalpalamasıdır. Bunu yazmak için, durum bu nedenle x, y, z konumunu ve ardından Yunan alfabeleri Phi, Theta ve Omega ile gösterilen sıra aralığını ve sapmayı içerir. Ancak helikopteri kontrol etmek için x yönündeki, y yönündeki ve z yönündeki hızını ve açısal hız olarak da adlandırılan dönüş hızını da bilmemiz gerekir. Bu sıra ne kadar hızlı değişiyor ve bu adım ne kadar hızlı değişiyor ve sapma ne kadar hızlı değişiyor? Bu aslında otonom helikopterleri kontrol etmek için kullanılan durum. Bu 12 numara listesi bir politikaya girdi midir ve bir politikanın işi bu 12 numaraya bakmak ve helikopterde yapılacak uygun eylemin ne olduğuna karar vermektir. Yani herhangi bir sürekli durum pekiştirmeli öğrenme problemi veya sürekli bir Markov karar süreci, sürekli MTP. Problemin durumu, 1'den 6'ya kadar bir sayı gibi az sayıda olası ayrık değerden biri değildir.
Bunun yerine, herhangi biri çok sayıda değerden herhangi birini alabilen bir sayı vektörüdür. Bu haftaki uygulama laboratuvarında, simüle edilmiş bir aya iniş uygulamasına uygulanan bir pekiştirmeli öğrenme algoritmasını kendiniz uygulayacaksınız. Ay'a bir şey indirmek simülasyondur. Bir sonraki videoda bu uygulamanın neleri içerdiğine bir göz atalım, çünkü başka bir sürekli durum uygulaması olacak.

## Lunar lander
Ay iniş aracı, aya simüle edilmiş bir araç indirmenizi sağlar. Pek çok takviyeli öğrenme araştırmacısı tarafından kullanılan eğlenceli küçük bir video oyunu gibi. Ne olduğuna bir göz atalım. Bu uygulamada, ayın yüzeyine hızla yaklaşan bir aya iniş aracının komutanısınız. Ve sizin işiniz, onu iniş pistine güvenli bir şekilde indirmek için uygun zamanlarda yangın iticilerini kullanmaktır. Nasıl göründüğüne dair bir fikir vermek için. Bu, Ay'a inen araç başarılı bir şekilde iniş yapıyor ve bu iki sarı bayrak arasına inmek için iticileri aşağı, sola ve sağa doğru ateşliyor. Ya da takviyeli iniş algoritması politikası iyi sonuç vermiyorsa, iniş aracının ne yazık ki ayın yüzeyine çarptığı yerde durum böyle görünebilir. Bu uygulamada, her zaman adımında dört olası eyleminiz var. Ya hiçbir şey yapamazsınız, bu durumda atalet ve yerçekimi kuvvetleri sizi ayın yüzeyine doğru çeker. Veya solda küçük bir kırmızı nokta çıktığını gördüğünüzde sol iticiyi ateşleyebilirsiniz, bu da sola ateş eder. Ay iniş aracını sağa itme eğiliminde olacaklar ya da aşağıdan aşağı iten ana motoru burada ateşleyebilirsiniz. Ya da doğru iticiyi ateşleyebilirsiniz ve bu sizi sola itecek olan doğru iticiyi ateşlemektir ve işiniz zaman içinde eylemleri seçmeye devam etmektir. Yani iniş pistindeki bu iki bayrak arasında güvenli bir şekilde Ay'a iniş yapıyor. Eylemlere daha kısa bir isim vermek için bazen eylemlere hiçbir şey yapma anlamına gelen hiçbir şey veya sol, daha uzak sol itici veya ana, ana motoru aşağı veya sağa ateşleme anlamında diyeceğim. Bu yüzden eylemlere hiçbir şey kalmadığını söyleyeceğim. Maine ve bu videoda daha sonra kısaca yazın. Peki ya devletler bununla yüzleşir mi? Mtp durumları X ve Y konumlarıdır. Peki ne kadar sola veya sağa ve ne kadar yukarıda ve ayrıca x.y hızı yatay ve dikey yönlerde ne kadar hızlı hareket ediyor ve sonra da açı. Peki aya iniş aracı ne kadar sola ya da sağa doğru eğilmiş? Açısal hız kader değil mi? Ve son olarak, çünkü konumdaki küçük bir fark, iniş yapıp yapmama konusunda büyük bir fark yaratır. Durum vektöründe l ve r dediğimiz iki değişkenimiz daha olacak. Bu, sol bacağın yerde olup olmadığına, yani sol bacağın yere oturup oturmadığına karşılık gelir ve r, sağ bacağın yere oturup oturmadığına karşılık gelir. Yani xy x.theta theta. l ve r sayılarımız ikili değerli olacak ve sol ve sağ bacakların yere değip değmemesine bağlı olarak yalnızca sıfır veya bir değerlerini alabilir. Sonunda aya iniş yapan kişi için ödül fonksiyonu. İniş pistine ulaşmayı başarırsa, iniş pistinin merkezine geldiğinde ne kadar iyi uçtuğuna bağlı olarak 100 ile 140 arasında bir ödül almaz. Ayrıca pede yaklaşıp uzaklaşması için ona ek bir ödül veriyoruz, böylece pede daha yakın hareket ediyor, pozitif bir ödül alıyor ve uzaklaşıyor ve uzaklaşıyor. Negatif bir ödül alır. Düşerse -100 büyük bir ödül alır, yumuşak bir iniş gerçekleştirir, yani bir iniş. Başka bir çarpışma daha var, yere düştüğü her bacak, sol bacak veya sağ bağlantı için +100 ödül alıyor. +10 ödül alır ve son olarak onu çok fazla yakıt israf etmemeye teşvik etmek ve iticileri ateşlemek gerekli değildir. Ana motoru her ateşlediğinde ona -0,3 ödül veriyoruz ve sol veya sağ taraftaki iticileri her ateşlediğinde ona -0,03 ödül veriyoruz. Bunun orta derecede karmaşık bir ödül işlevi olduğuna dikkat edin. Ay'a iniş uygulamasının tasarımcıları aslında tam olarak hangi davranışı istediğinizi biraz düşündüler ve bunu ödül işlevinde kodladılar. İstediğiniz davranışlardan daha fazlasını teşvik etmek ve istemediğiniz çarpma gibi davranışlardan korkmak. Kendi takviyeli öğrenme uygulamanızı oluştururken, tam olarak ne isteyip istemediğinizi belirlemek ve bunu ödül işlevinde kodlamak için genellikle biraz düşünmeniz gerektiğini görürsünüz. Ancak, ödül işlevinin belirtilmesi, her bir durumdan alınacak tam doğru eylemi belirtmek için yine de çok daha kolay olacaktır. Hangisi bu ve diğer pek çok pekiştirmeli öğrenme uygulaması için çok daha zor. Yani aya iniş sorunu aşağıdaki gibidir. Amacımız bir politika pi öğrenmektir. Burada yazıldığı gibi bir S durumu verildiğinde, a'nın pi'ye eşit olduğu bir eylemi seçer. Böylece indirimli ödüllerin toplam getirisini en üst düzeye çıkarmak için. Ve genellikle aya iniş yapanlar için gama ra için oldukça büyük bir değer kullanırdı. Aslında, 0.985'e eşit olan kamera değerini kullanırdı, yani bire oldukça yakın. Ve bunu yapan bir pi politikası öğrenebilirseniz, o zaman bu aya iniş heyecan verici uygulamasını başarılı bir şekilde indirirsiniz ve artık bir politika bulmak için derin öğrenmeyi veya sinir ağlarını kullanacak bir öğrenme algoritması geliştirmeye hazırız. aya iniş aracını indirin.

## Learning the state-value function
Lunar Lander'ı kontrol etmek veya diğer pekiştirmeli öğrenme problemlerinde pekiştirmeli öğrenmeyi nasıl kullanabileceğimize bakalım. Temel fikir, bir sinir ağını Q(s,a) durum eylem değeri fonksiyonunu hesaplamak veya buna yaklaşmak için eğiteceğimiz ve bunun karşılığında iyi eylemler seçmemize izin vereceğidir. Nasıl çalıştığını görelim. Öğrenme algoritmasının kalbi, mevcut durumu ve mevcut eylemi giren ve Q of s, a'yı hesaplayan veya buna yaklaşan bir sinir ağı eğiteceğiz. Özellikle, Lunar Lander için, state s ve any action a'yı alıp bir araya getireceğiz. Somut olarak, durum daha önce gördüğümüz sekiz sayıdan oluşan bir listeydi, yani xy, x dot, y dot, Theta, Theta dot ve sonra bacakların topraklandığı yer için LR var, yani bu, tanımlayan sekiz sayıdan oluşan bir liste. eyalet. Son olarak, dört olası eylemimiz var: hiçbir şey, sol, ana, ana motor ve sağ. One-hot özellik vektörü kullanarak bu dört eylemden herhangi birini kodlayabiliriz. Eylem ilk eylem ise 1, 0, 0, 0 kullanarak kodlayabiliriz veya sol kümeyi bulmak için ikinci eylem ise 0, 1, 0, 0 olarak kodlayabiliriz. Bu 12 sayı listesi , durum için sekiz sayı ve sonra dört sayı, eylemin bir sıcak kodlaması, sinir ağına almamız gereken girdilerdir ve ben buna X diyeceğim. Daha sonra bu 12 sayıyı alacağız ve birinci gizli katmanda 64 birim, ikinci gizli katmanda 64 birim ve ardından çıkış katmanında tek bir çıktı ile onları bir sinir ağına besleyin. Sinir ağının işi, s, a'nın Q çıktısıdır. s ve a girdileri verildiğinde Lunar Lander için durum eylem-değeri işlevi. Birazdan sinir ağı eğitim algoritmalarını kullanacağımız için, bu değere (Q of s, a) yaklaşık olarak sinir ağını eğiten Y hedef değeri olarak da değineceğim. Takviyeli öğrenmenin denetimli öğrenmeden farklı olduğunu söylediğime dikkat edin, ancak yapacağımız şey bir durum girmek değil ve bir eylem çıktısını almak. Yapacağımız şey, bir durum eylem çifti girmek ve bunun Q(s,a) çıktısını almasını sağlamak ve pekiştirmeli öğrenme algoritmasının içinde bir sinir ağı kullanmak bu şekilde oldukça iyi sonuç verecektir. Ayrıntıları birazdan göreceğiz, bu yüzden henüz mantıklı gelmediyse endişelenmeyin. Ancak, size Q of s, a'nın iyi bir tahminini vermesi için gizli katmanlarda ve üst katmanda uygun parametre seçenekleriyle bir sinir ağını eğitebilirseniz, o zaman Lunar Lander s durumundayken, yapabilirsiniz daha sonra Q of s, a'yı hesaplamak için sinir ağını kullanın. Dört eylemin tümü için, Q of s, hiçbir şey, Q of s, left, Q of s, main, Q of s, right'ı hesaplayabilirsiniz ve son olarak, bunlardan hangisi en yüksek değere sahipse, karşılık gelen eylemi a seçersiniz. . Örneğin, bu dört değerden (Q of s, main) en büyüğü ise, o zaman gidip Lunar Lander'ın ana motorunu ateşlemeye karar verirsiniz. Soru, bir sinir ağını Q of s, a çıktısı için nasıl eğitirsiniz? Görünüşe göre yaklaşım, x ve y gibi çok sayıda örnek içeren bir eğitim seti oluşturmak için Bellman'ın denklemlerini kullanmak olacak ve ardından denetimli öğrenmeyi tam olarak ikinci derste sinir ağları hakkında konuştuğumuzda öğrendiğiniz gibi kullanacağız. Denetimli öğrenmeyi kullanarak öğrenmek için, x'ten y'ye bir eşleme, yani durum eylem çiftinden bu Q of s, a hedef değerine bir eşleme. Ancak, daha sonra bir sinir ağını eğitebileceğiniz x ve y değerleri ile eğitim setini nasıl elde edersiniz? Hadi bir bakalım. İşte Bellman denklemi, Q(s), a eşittir R(s) artı Gama, maks. Sağ taraf, Q of s, a'nın eşit olmasını istediğiniz şeydir, yani sağ taraftaki bu değere y diyeceğim ve sinir ağının girdisi bir durum ve bir eylemdir, yani ben buna x diyeceğim. Sinir ağının işi, x'i, yani durum eylem çiftini girmek ve sağdaki değerin ne olacağını doğru bir şekilde tahmin etmeye çalışmaktır. Denetimli öğrenmede, sinir ağının çeşitli katmanlarının parametreleri olan bir grup parametreye, W ve B'ye bağlı olan bir f fonksiyonunu öğrenmek için bir sinir ağını eğitiyorduk ve x'i girmek sinir ağının işiydi. . Umarım y hedef değerine yakın bir şey koyarım. Soru şu ki, öğrenilecek yeni bir ağ için x ve y değerlerine sahip bir eğitim setini nasıl bulabiliriz? İşte yapacağımız şey. Ay iniş aracını kullanacağız ve içinde farklı eylemler gerçekleştirmeyi deneyeceğiz. Henüz iyi bir politikamız yoksa, rastgele hareketler yapacağız, ileri, sol ileri, ileri, sağ ileri, ana motor, hiçbir şey yapmayın. Sadece aya iniş simülatöründe farklı şeyler deneyerek, bir durumda olduğumuz ve bazı eylemlerde bulunduğumuz, iyi bir eylem veya belki de her iki şekilde de korkunç bir eylem yaptığımız birçok örneği gözlemleyeceğiz.
Sonra bu durumda olduğumuz için bir miktar R of s ödülü aldık ve eylemimizin bir sonucu olarak yeni bir duruma, S' ulaştık. Ay'a inişte farklı eylemler gerçekleştirirken, bu S, a, R of s, S''yi görürsünüz ve Python kodunda bunlara birçok kez demetler diyoruz. Örneğin, belki bir kez S durumundasınız ve buna bir indeks vermek için ve biz buna S^1 diyoruz ve a^1 gibi bir eylemde bulunuyorsunuz, bu hiçbir şey kalmayabilir, ana itmeler orada veya sağda olabilir. . Bunun sonucunda bir miktar ödül aldınız ve bir S^'1 durumunda yükselmek istiyorsunuz. Belki farklı zamanlarda başka bir S2 durumundasınız, başka bir eylem yaptınız, iyi eylem olabilir, kötü eylem olabilir, dört eylemden herhangi biri olabilir ve ödülü aldınız ve sonra S ile çıkmak istiyorsunuz. '2 ve benzeri birçok kez. Belki bunu 10.000 kez, hatta 10.000'den fazla kez yaptınız, bu nedenle yalnızca S^1, a^1 vb. ile değil, S^10.000, a^10.000'e kadar yolu kurtarmanız gerekir. Bu demetlerin her birinin tek bir eğitim örneği, x^1, y^1 oluşturmak için yeterli olacağı ortaya çıktı. Özellikle, bunu nasıl yapacağınız aşağıda açıklanmıştır. Bu ilk demette dört öğe vardır. İlk ikisi x^1'i hesaplamak için kullanılacak ve ikinci ikisi y^1'i hesaplamak için kullanılacaktır. Özellikle, x^1 sadece S^1 olacak, a^1 bir araya getirilecek. S^1 sekiz sayı, aya iniş yapan aracın durumu, a^1 dört sayı, bu eylemin tek sıcak kodlaması ve y^1 Bellman denkleminin sağ tarafı kullanılarak hesaplanacaktı. . Özellikle, Bellman denklemi, S^1, a^1 girdiğinizde, Q of S^1, a^1'in bu sağ taraf olmasını, R of S^1 artı Gamma max bölü olmasını istediğinizi söyler. a' of Q of S^1'a' asal. Sağdaki demetin bu iki öğesinin size bunu hesaplamak için yeterli bilgi verdiğine dikkat edin. R(S^1) nedir biliyor musunuz? Bu yolu kurtardığın ödül bu. Artı iskonto faktörü gamma çarpı tüm eylemler üzerinden maks. Buna y^1 diyeceğim. Bunu hesapladığınızda, bu 12.5 veya 17 veya 0.5 veya başka bir sayı gibi bir sayı olacaktır. Bu sayıyı buraya y^1 olarak kaydedeceğiz, böylece bu x^1, y^1 çifti, hesapladığımız bu düşük veri setindeki ilk eğitim örneği olacak. Şimdi, Q of S', a' veya Q of S'^1, a' nereden geldiğini merak ediyor olabilirsiniz. Başlangıçta Q fonksiyonunun ne olduğunu bilmiyoruz. Ancak, Q fonksiyonunun ne olduğunu bilmediğinizde, tamamen rastgele bir tahminde bulunarak başlayabileceğiniz ortaya çıktı. Q işlevi nedir? Bir sonraki slaytta [duyulmuyor]'un yine de çalışacağını göreceğiz. Ancak her adımda Q burada sadece bir tahminde bulunacaktır. Zamanla daha iyi olacaklar, gerçek Q fonksiyonunun ne olduğu ortaya çıkıyor. İkinci örneğe bakalım. Eğer a^2'ye ulaşmak için S^2 durumunda olduğunuz, o ödülü aldığınız ve sonra o duruma ulaştığınız ikinci bir deneyiminiz varsa. Daha sonra bu veri kümesinde ikinci bir eğitim örneği oluşturacağız, x^2, burada girdi şimdi S^2, a^2'dir, böylece ilk iki öğe x girdisini hesaplamaya gider ve ardından y^2 şuna eşit olur: R of s^2 artı a' Q of S' to a' gamma max ve bu sayı her ne ise, y^2. Bunu küçük ama büyüyen eğitim setimize koyuyoruz ve bu böyle devam ediyor, ta ki bu x, y çiftleriyle 10.000 eğitim örneği elde edene kadar. Daha sonra göreceğimiz şey, x'lerin 12 özellikli girdiler ve y'lerin sadece sayılar olduğu bu eğitim setini alacağız. Örneğin, x girişinin bir fonksiyonu olarak y'yi tahmin etmeye çalışmak için ortalama karesel hata kaybıyla yeni bir ağ eğiteceğiz. Burada anlatacağım şey, kullanacağımız öğrenme algoritmasının sadece bir parçası. Hepsini bir sonraki slaytta bir araya getirelim ve hepsinin nasıl tek bir algoritmada bir araya geldiğini görelim. Q-fonksiyonunu öğrenmek için tam bir algoritmanın nasıl olduğuna bir göz atalım. İlk olarak, sinir ağımızı alacağız ve sinir ağının tüm parametrelerini rastgele başlatacağız. Başlangıçta, bunun bir Q işlevi olup olmadığı hakkında hiçbir fikrimiz yok, hadi genişliklerin tamamen rastgele değerlerini seçelim. Bu sinir ağının, Q-fonksiyonu için ilk rastgele tahminimiz olduğunu farz edeceğiz. Bu biraz, lineer regresyon eğitimi alırken tüm parametreleri rasgele başlattığınız ve ardından parametreleri iyileştirmek için gradyan inişini kullandığınız zamana benzer. Şimdilik rastgele başlatmak iyi. Önemli olan, algoritmanın daha iyi bir tahmin elde etmek için parametreleri yavaşça iyileştirip iyileştiremeyeceğidir. Ardından, aşağıdakileri tekrar tekrar yapacağız; Aya iniş aracında harekete geçeceğiz, bu yüzden rastgele süzülün, bazı iyi eylemlerde bulunun, bazı kötü eylemlerde bulunun. Her iki şekilde de sorun değil. Ama bir durumdayken bu demetlerden birçoğunu alıyorsunuz, bazılarını gerçekten R(S)'yi topladınız ve bir eyaletin asalına ulaştınız. Yapacağımız şey, bu demetlerin en son 10.000 örneğini depolamak.
Bu algoritmayı çalıştırdıkça, aya inişte birçok adım göreceksiniz, belki yüzbinlerce adım. Ancak aşırı bilgisayar belleği kullanmadığımızdan emin olmak için genel uygulama, MTP'de işlem yaparken gördüğümüz en son 10.000 demetin hatırlanmasıdır. Yalnızca en son örneklerin saklandığı bu tekniğe, takviyeli öğrenme algoritmasında bazen yeniden oynatma arabelleği adı verilir. Şimdilik, aya iniş aracını rastgele uçuruyoruz, bazen çarpıyor, bazen çarpmıyor ve bu demetleri öğrenme algoritmamız için deneyimli hale getiriyoruz. Arada sırada sinir ağını eğiteceğiz. Sinir ağını eğitmek için şunları yapacağız. Kaydettiğimiz bu en son 10.000 tuple'a bakacağız ve 10.000 örneklik bir eğitim seti oluşturacağız. Eğitim seti çok sayıda x ve y çiftine ihtiyaç duyar. Eğitim örneklerimiz için x, demetin bu kısmından s, a olacaktır. 12 numaradan oluşan bir liste olacak, 8 numara durum için ve 4 numara eylemin tek seferlik kodlaması için. Bir sinir ağının tahmin etmeye çalışmasını istediğimiz hedef değer, y eşittir R(S) artı Gama maks. Q'nun bu değerini nasıl elde ederiz? Başlangıçta rastgele başlattığımız bu sinir ağı. Pek iyi bir tahmin olmayabilir, ama bu bir tahmin. Bu 10.000 eğitim örneğini oluşturduktan sonra x1, y1'den x10.000, y10.000'e kadar eğitim örneklerine sahip olacağız. Bir sinir ağını eğiteceğiz ve yeni sinir ağına Q yeni diyeceğim, öyle ki Q yeni s, a y'ye yaklaşmayı öğreniyor. Bu tam olarak sinir ağının w ve b parametreleriyle f çıktısı alması, y hedef değerine yaklaşmaya çalışmak için x girişi yapması için eğitiliyor. Şimdi, bu sinir ağı, Q fonksiyonunun veya durum eylem değeri fonksiyonunun ne olması gerektiğine dair biraz daha iyi bir tahmin olmalıdır. Yapacağımız şey, Q'yu alıp az önce öğrendiğimiz bu yeni sinir ağına ayarlayacağız. Bu algoritmadaki fikirlerin çoğu Mnih ve diğerlerine aittir. Görünüşe göre bu algoritmayı Q fonksiyonunun gerçekten rastgele bir tahminiyle başladığınız yerde çalıştırırsanız, ardından Q fonksiyonunun tahminlerini iyileştirmeye çalışmak için Bellman denklemlerini tekrar tekrar kullanırsınız. Sonra bunu tekrar tekrar yaparak, birçok eylemde bulunarak, bir modeli eğiterek, Q-fonksiyonu için tahmininizi geliştireceksiniz. Eğiteceğiniz bir sonraki model için artık Q fonksiyonunun ne olduğuna dair biraz daha iyi bir tahmininiz var. O zaman eğiteceğiniz bir sonraki model daha da iyi olacaktır. Q'yu güncellediğinizde Q eşittir Q yeni. Sonra bir dahaki sefere s üssünün Q modelini eğittiğinizde asal değer daha da iyi bir tahmin olacaktır. Bu algoritmayı her yinelemede, Q of s üssünde çalıştırdıkça, bir asal sayının Q fonksiyonunun daha iyi bir tahmini haline gelmesi umulur, böylece algoritmayı yeterince uzun çalıştırdığınızda, bu aslında Q'nun gerçek değerinin oldukça iyi bir tahmini haline gelir. s, a, böylece daha sonra bunu umarım iyi eylemler veya MTP seçmek için kullanabilirsiniz. Az önce gördüğünüz algoritmaya bazen Derin Q-Ağı anlamına gelen DQN algoritması denir çünkü Q fonksiyonlarını öğrenmek için bir model eğitmek üzere derin öğrenme ve sinir ağı kullanıyorsunuz. Bu nedenle, bir sinir ağı kullanan DQN veya DQ. Algoritmayı tarif ettiğim gibi kullanırsanız, aya inişte işe yarayacaktır. Belki yakınsaması uzun zaman alacak, belki tam olarak inmeyecek ama işe yarayacak. Ancak algoritmada yapılacak birkaç iyileştirmeyle çok daha iyi çalışabileceği ortaya çıktı. Sonraki birkaç videoda, az önce gördüğünüz algoritmada yapılan bazı iyileştirmelere bir göz atalım.

## Algorithm refinement: Improved neural network architecture
Son videoda, durumu ve eylemi girecek ve Q fonksiyonu, Q of s, a'yı çıkarmaya çalışacak bir sinir ağı mimarisi gördük. Sinir ağı mimarisinde bu algoritmayı çok daha verimli hale getiren bir değişiklik olduğu ortaya çıktı. DQN'nin çoğu uygulaması aslında bu videoda göreceğimiz bu daha verimli mimariyi kullanır. Hadi bir bakalım. Bu, daha önce gördüğümüz sinir ağı mimarisiydi, burada 12 sayı giriyor ve Q of s, a çıktısını veriyordu. Ne zaman bir s durumunda olsak, bize en büyük Q değerini veren a eylemini seçmek için bu dört değeri hesaplamak için sinir ağında ayrı ayrı dört kez çıkarım yapmak zorunda kalırdık. Bu verimsiz çünkü çıkarımlarımızı her bir durumdan dört kez taşımak zorundayız. Bunun yerine, tek bir sinir ağını bu değerlerin dördünü aynı anda çıkaracak şekilde eğitmenin daha verimli olduğu ortaya çıktı. Görünüşü böyle. İşte girdinin aya iniş aracının durumuna karşılık gelen sekiz sayı olduğu değiştirilmiş bir sinir ağı mimarisi. Daha sonra birinci gizli katmanda 64 birim, ikinci gizli katmanda 64 birim ile sinir ağından geçer. Şimdi çıkış biriminin dört çıkış birimi vardır ve sinir ağının işi, dört çıkış biriminin Q s, hiçbir şey, Q s, sol, Q s, ana ve q s, sağ çıktısını sağlamaktır. Sinir ağının işi, s durumundayken dört olası eylemin tümü için Q değerini aynı anda hesaplamaktır. Bunun daha verimli olduğu ortaya çıkıyor çünkü s durumu verildiğinde yalnızca bir kez çıkarım yapabilir ve bu değerlerin dördünü de alabilir ve ardından çok hızlı bir şekilde Q(s,a)'yı maksimize eden a eylemini seçebiliriz. Bellman'ın denklemlerinde de fark ettiniz, bir asal Q bölü s üssü a üssü hesaplamamız gereken bir adım var, bu bir gama maksimize ediyor ve sonra burada artı R s vardı. Bu ikili ağ aynı zamanda bunu hesaplamayı çok daha verimli hale getiriyor çünkü aynı anda tüm eylemler için Q(s üssü) a üssünü elde ediyoruz. Daha sonra Bellman denklemlerinin sağ tarafı için bu değeri hesaplamak üzere maksimumu seçebilirsiniz. Sinir ağı mimarisindeki bu değişiklik, RN'yi çok daha verimli hale getiriyor ve bu nedenle bu mimariyi uygulama laboratuvarında kullanacağız. Ardından, algoritmaya çok yardımcı olacak bir fikir daha var, bu da Epsilon-açgözlülük politikası olarak adlandırılan ve henüz öğrenirken bile eylemleri nasıl seçtiğinizi etkileyen bir şey. Bir sonraki videoya ve bunun ne anlama geldiğine bir göz atalım.

## Algorithm refinement: ϵ-greedy policy
Geliştirdiğimiz öğrenme algoritması, hala Q(s,a)'ya nasıl yaklaşacağınızı öğrenirken bile, aya inişte bazı işlemler yapmanız gerekir. Hala öğrenirken bu eylemleri nasıl seçiyorsunuz? Bunu yapmanın en yaygın yolu, Epsilon-açgözlülük politikası adı verilen bir şey kullanmaktır. Bunun nasıl çalıştığına bir göz atalım. İşte daha önce gördüğünüz algoritma. Algoritmadaki adımlardan biri, aya inişte eylemde bulunmaktır. Öğrenme algoritması hala çalışırken, her durumda yapılacak en iyi eylemin ne olduğunu gerçekten bilmiyoruz. Yapsaydık, öğrenmeyi çoktan bitirmiş olurduk. Ancak hala öğreniyor olsak ve henüz çok iyi bir Q(s,a) tahminimiz olmasa bile, öğrenme algoritmasının bu adımında nasıl harekete geçebiliriz? Bazı seçeneklere bakalım. Bazı durumlarda olduğunuzda, tamamen rastgele eylemler yapmak istemeyebiliriz çünkü bu genellikle kötü bir eylem olacaktır. Doğal bir seçenek, s durumundayken, Q(s,a)'yı maksimize eden bir a eylemi seçmek olacaktır. Q(s,a), Q fonksiyonunun harika bir tahmini olmasa bile, elimizden gelenin en iyisini yapalım ve mevcut Q(s,a) tahminimizi kullanalım ve onu maksimize eden a eylemini seçelim diyebiliriz. Görünüşe göre bu işe yarayabilir, ancak en iyi seçenek değil. Bunun yerine, yaygın olarak yapılan şey şudur. İşte seçenek 2, çoğu zaman, diyelim ki 0,95 olasılıkla, Q(s,a)'yı maksimize eden eylemi seçin. Çoğu zaman mevcut Q(s,a) tahminimizi kullanarak iyi bir eylem seçmeye çalışırız. Ama zamanın küçük bir kısmında, diyelim ki zamanın yüzde beşinde rastgele bir eylem seçeceğiz. Neden ara sıra rastgele bir eylem seçmek istiyoruz? İşte nedeni. Öğrenme algoritmasının ana iticiyi ateşlemenin asla iyi bir fikir olmadığını düşünmesi için Q(s,a)'nın rastgele başlatılmasının garip bir nedeni olduğunu varsayalım. Belki sinir ağı parametreleri, Q(s, main) her zaman çok düşük olacak şekilde başlatıldı. Durum buysa, o zaman sinir ağı, Q(s,a)'yı maksimize eden a eylemini seçmeye çalıştığı için, asla ana iticiyi ateşlemeyi denemeyecektir. Ana iticiyi ateşlemeyi asla denemediği için, ana iticiyi ateşlemenin aslında bazen iyi bir fikir olduğunu asla öğrenemez. Rastgele başlatma nedeniyle, sinir ağı bir şekilde başlangıçta bazı şeylerin kötü bir fikir olduğu fikrine takılırsa, o zaman 1. seçenek, bu eylemleri asla denemeyeceği ve belki de aslında iyi bir şey olduğunu keşfedemeyeceği anlamına gelir. Bazen ana iticileri ateşlemek gibi bu eylemi gerçekleştirme fikri. Her adımda 2. seçenek altında, sinir ağının neyin kötü bir fikir olabileceğine dair kendi olası önyargılarının üstesinden gelmeyi öğrenebilmesi için farklı eylemler deneme olasılığımız biraz düşük. Eylemleri rastgele seçme fikrine bazen keşif adımı denir. Çünkü en iyi fikir olmayabilecek bir şey deneyeceğiz, ancak bazı durumlarda sadece bazı eylemler deneyeceğiz, çok fazla sahip olamayabileceğimiz bir durumda bir eylem hakkında daha fazla şey keşfedeceğiz ve öğreneceğiz. önce deneyim. Q(s,a)'yı maksimuma çıkaran bir eylemde bulunmak, bazen buna açgözlü eylem denir çünkü aslında bunu seçerek getirimizi en üst düzeye çıkarmaya çalışıyoruz. Ya da pekiştirmeli öğrenme literatüründe bazen bunu bir istismar adımı olarak da duyacaksınız. Sömürünün iyi bir şey olmadığını biliyorum, kimse kimseyi keşfetmemeli. Ancak tarihsel olarak, bu, pekiştirmeli öğrenmede, hadi elimizden gelenin en iyisini yapmak için öğrendiğimiz her şeyi kullanalım demek için kullanılan terimdi. Takviyeli öğrenme literatüründe, bazen insanların keşfetmeye karşı sömürü değiş tokuşu hakkında konuştuğunu duyarsınız; bu, ne sıklıkla rastgele eylemlerde bulunduğunuzu veya daha fazlasını öğrenmek için en iyi olmayabilecek eylemleri gerçekleştirmenizi ifade eder. Q(s,a)'yı maksimize eden eylemi yaparak geri dön. Seçenek 2 olan bu yaklaşımın bir adı vardır ve Epsilon açgözlü politikası olarak adlandırılır, burada Epsilon 0,05'tir, rastgele bir eylem seçme olasılığıdır. Bu, takviyeli öğrenme algoritmanızın ara sıra veya belki de çoğu zaman açgözlü eylemlerde bulunurken bile biraz keşfetmesini sağlamanın en yaygın yoludur. Bu arada, pek çok kişi Epsilon-açgözlülük politikası adının kafa karıştırıcı olduğu yorumunu yaptı çünkü siz aslında zamanın yüzde beşi değil, yüzde 95'i açgözlü davranıyorsunuz. Yani belki 1 eksi Epsilon açgözlü politikası, çünkü yüzde 95 açgözlü, yüzde beş keşfediyor, bu aslında algoritmanın daha doğru bir tanımı. Ancak tarihsel nedenlerden ötürü, Epsilon-açgözlü politikası adı sıkışıp kaldı. Bu, insanların zamanın bu açgözlü Epsilon kısmını değil, aslında zamanın Epsilon kısmını araştıran politikaya atıfta bulunmak için kullandıkları addır.
Son olarak, pekiştirmeli öğrenmede bazen kullanılan numaralardan biri, Epsilon yüksekten başlamaktır. Başlangıçta, bir seferde çok fazla rastgele eylemde bulunursunuz ve sonra bunu kademeli olarak azaltırsınız, böylece zamanla rastgele eylemde bulunma olasılığınız azalır ve iyi eylemleri seçmek için Q-fonksiyonuna ilişkin gelişen tahminlerinizi kullanma olasılığınız artar. Örneğin, aya iniş egzersizinde Epsilon ile çok çok yüksek başlayabilirsiniz, hatta belki Epsilon 1.0'a eşittir. Başlangıçta tamamen rastgele eylemler seçiyorsunuz ve sonra kademeli olarak 0,01'e kadar azaltıyorsunuz, böylece sonunda zamanın yüzde 99'unda açgözlü eylemlerde bulunuyorsunuz ve zamanın yalnızca çok küçük bir yüzdesinde rastgele hareket ediyorsunuz. Bu karmaşık görünüyorsa, endişelenmeyin. Bunu nasıl yapacağınızı gösteren kodu Jüpiter laboratuvarında sağlayacağız. Algoritmayı, tanımladığımız gibi, daha verimli sinir ağı mimarisi ve Epsilon-açgözlü bir keşif politikası ile uygularsanız, bunların aya inişte oldukça iyi çalıştığını görürsünüz. Takviyeli öğrenme algoritması için fark ettiğim şeylerden biri, denetimli öğrenmeye kıyasla, hiper parametre seçimi açısından daha titiz olmaları. Örneğin, denetimli öğrenmede, öğrenme hızını biraz fazla küçük ayarlarsanız, algoritmanın öğrenmesi daha uzun sürebilir. Belki antrenman yapmak üç kat daha uzun sürüyor ki bu sinir bozucu ama belki o kadar da kötü değil. Takviyeli öğrenmede ise, Epsilon'un değerini tam olarak iyi ayarlamazsanız veya diğer parametreleri tam olarak iyi ayarlamazsanız, öğrenmenin üç kat daha uzun sürmediğini bulun. Öğrenmek 10 kat da olabilir, 100 kat daha uzun da sürebilir. Takviyeli öğrenme algoritmaları, bence denetimli öğrenme algoritmalarından daha az olgun oldukları için, bunun gibi küçük parametre seçimlerinde çok daha titizler ve aslında bazen bu parametreleri takviyeli öğrenme algoritmasıyla ayarlamak, denetimli bir algoritmaya kıyasla açıkçası daha sinir bozucu oluyor. öğrenme algoritması Ama yine de, uygulama laboratuvarı, program alıştırması hakkında endişeleriniz varsa, size program alıştırmasında kullanmanız için iyi parametreler hakkında bir fikir vereceğiz, böylece bunu yapabilmeniz ve aya iniş yapmayı başarılı bir şekilde öğrenebilmeniz için umut ediyoruz. çok fazla sorun olmadan. Bir sonraki isteğe bağlı videoda, birkaç algoritma iyileştirmesi, mini gruplama ve ayrıca yumuşak güncellemeler kullanmamızı istiyorum. Bu ek iyileştirmeler olmasa bile, algoritma iyi çalışacaktır, ancak bunlar, algoritmanın çok daha hızlı çalışmasını sağlayan ek iyileştirmelerdir. Bu videoyu atlarsanız sorun olmaz, umarım başarılı bir şekilde tamamlamanız için uygulama laboratuvarında ihtiyacınız olan her şeyi sağladık. Ancak, iki takviyeli öğrenme algoritmasının bu ayrıntıları hakkında daha fazlasını öğrenmekle ilgileniyorsanız, o zaman benimle gelin ve bir sonraki videoda, mini gruplama ve yumuşak güncellemelerde görelim.


## Algorithm refinement: Mini-batch and soft updates
Bu videoda, görmüş olduğunuz pekiştirmeli öğrenme algoritmasında iki iyileştirmeye daha bakacağız. İlk fikre mini partiler kullanmak denir ve bu, hem takviyeli öğrenme algoritmanızı hızlandırabilecekleri hem de denetimli öğrenmeye uygulanabilecekleri bir fikir olarak ortaya çıkıyor. Bir sinir ağını eğitmek veya doğrusal bir regresyon veya lojistik regresyon modelini eğitmek gibi, denetimli öğrenme algoritmanızı da hızlandırmanıza yardımcı olabilirler. Bakacağımız ikinci fikir, takviyeli öğrenme algoritmanızın iyi bir çözüme yaklaşmak için daha iyi bir iş çıkarmasına yardımcı olacağı ortaya çıkan yumuşak güncellemelerdir. Mini toplu işlere ve yumuşak güncellemelere bir göz atalım. Mini partileri anlamak için, başlamak için denetimli öğrenmeye bakalım. Burada, konut fiyatlarını tahmin etmek için doğrusal regresyonun kullanılmasıyla ilgili bu uzmanlığın ilk kursunda çok önceleri görmüş olduğunuz konut boyutları ve fiyatlarına ilişkin veri kümesi bulunmaktadır. Orada w ve b parametreleri için bu maliyet fonksiyonunu bulmuştuk, 1 bölü 2m, tahminin toplamı eksi gerçek değer y^​2 idi. Bu algoritmadaki gradyan, w'yi w eksi [duyulmuyor] alfa çarpı wb'nin J maliyetinin w'ye göre kısmi türevi olarak tekrar tekrar güncellemek ve benzer şekilde b'yi aşağıdaki gibi güncellemekti. J(wb)'nin bu tanımını alıp yerine koyayım. Şimdi, bu örneğe baktığımızda, lineer regresyon ve denetimli öğrenme hakkında konuşmaya başladığımız çok eski zamanlarda, m boyutundaki eğitim seti oldukça küçüktü. Sanırım 47 eğitim örneğimiz vardı. Peki ya çok büyük bir eğitim setiniz varsa? M'nin 100 milyona eşit olduğunu söyleyin. Amerika Birleşik Devletleri de dahil olmak üzere 100 milyondan fazla konut birimine sahip birçok ülke var ve bu nedenle ulusal bir nüfus sayımı size bu büyüklükte veya büyüklükte bir veri seti verecektir. Veri kümeniz bu kadar büyük olduğunda bu algoritmanın sorunu, gradyan inişinin her bir adımının bu ortalama 100 milyondan fazla örneğin hesaplanmasını gerektirmesi ve bunun çok yavaş olduğu ortaya çıkıyor. Gradyan inişin her adımı, bu toplamı veya bu ortalamayı 100 milyon örnek üzerinde hesaplayacağınız anlamına gelir. Sonra küçük bir gradyan iniş adımı atarsınız ve geri dönersiniz ve bir sonraki adımda türevi hesaplamak için 100 milyon örnek veri kümenizin tamamını yeniden taramanız gerekir, onlar başka bir küçük gradyan iniş adımı atarlar ve bu böyle devam eder. Eğitim seti boyutu çok büyük olduğunda, bu gradyan iniş algoritmasının oldukça yavaş olduğu ortaya çıkıyor. Mini toplu gradyan iniş fikri, bu döngü boyunca her bir yinelemede 100 milyon eğitim örneğinin tamamını kullanmamaktır. Bunun yerine, daha küçük bir sayı seçebiliriz, ona m üssü eşittir 1000 diyelim. Her adımda, 100 milyon örneğin tamamını kullanmak yerine, 1000 veya m asal örnekten oluşan bir alt küme seçerdik. Bu iç terim 1 bölü 2m olur, toplam bölü toplam m asal örnek. Artık gradyan iniş yoluyla yapılan her yineleme, 100 milyon yerine yalnızca 1000 örneğe bakmayı gerektiriyor ve her adım çok daha az zaman alıyor ve yalnızca daha verimli bir algoritmaya yol açıyor. Algoritmanın ilk yinelemesinde mini toplu gradyan inişinin yaptığı şey, verilerin o alt kümesine bakıyor olabilir. Bir sonraki yinelemede, belki de verilerin o alt kümesine bakar ve bu böyle devam eder. Üçüncü yineleme vb. için, böylece her yineleme, verilerin yalnızca bir alt kümesine bakar, böylece her yineleme çok daha hızlı çalışır. Bunun neden makul bir algoritma olabileceğini görmek için konut veri kümesini burada bulabilirsiniz. İlk yinelemede sadece beş örneğe bakacak olursak, bu tüm veri kümesi değildir, ancak sonuna sığdırmak isteyebileceğiniz dize satırını biraz temsil eder ve bu nedenle algoritmayı daha iyi hale getirmek için bir gradyan iniş adımı atmak bu beş örneği sığdırmak tamamdır. Ancak bir sonraki yinelemede, burada gösterilene benzer beş farklı örnek alırsınız. Bu beş örneği kullanarak bir gradyan iniş adımı atıyorsunuz ve bir sonraki yinelemede farklı bir beş örnek kullanıyorsunuz ve bu böyle devam ediyor. Bu örnek listesini yukarıdan aşağıya tarayabilirsiniz. Bu bir yol olurdu. Başka bir yol da, her yinelemede kullanmak için tamamen farklı beş örnek seçmenizdir. Toplu gradyan iniş ile hatırlayabilirsiniz, eğer bunlar J maliyet fonksiyonunun konturlarıysa. O zaman toplu gradyan iniş, buradan başlayın ve bir adım atın, bir adım atın, bir adım atın, bir adım atın, bir adım atın derdi. Gradyan inişinin her adımı, parametrelerin burada ortada bulunan maliyet fonksiyonunun küresel minimumuna güvenilir bir şekilde yaklaşmasına neden olur. Buna karşılık, mini toplu gradyan iniş veya mini toplu öğrenme algoritması bunun gibi bir şey yapacaktır. Buradan başlarsanız, ilk yinelemede yalnızca beş örnek kullanılır.
Doğru yönde vuracak ama belki de en iyi eğimli iniş yönü değil. Sonra bir sonraki yinelemede bunu yapabilirler, sonraki yinelemede bunu yapabilirler ve bu ve bazen şans eseri, seçtiğiniz beş örnek şanssız bir seçim olabilir ve hatta küresel minimumdan yanlış yöne doğru gidebilir, vb. ileri. Ancak ortalama olarak, mini toplu gradyan iniş, güvenilir bir şekilde ve biraz gürültülü bir şekilde küresel minimuma doğru yönelecektir, ancak her yineleme, hesaplama açısından çok daha ucuzdur ve bu nedenle mini toplu öğrenme veya mini toplu gradyan iniş, çok daha hızlı bir algoritma haline gelir. çok büyük bir eğitim setiniz olduğunda. Aslında, çok büyük bir eğitim setinizin olduğu denetimli öğrenme için, mini toplu öğrenme veya mini toplu gradyan iniş ya da Atom gibi diğer optimizasyon algoritmalarına sahip bir mini toplu sürüm, toplu gradyan inişten daha yaygın olarak kullanılır. Takviyeli öğrenme algoritmamıza geri dönersek, bu daha önce gördüğümüz algoritmadır. Bunun mini toplu versiyonu, en son 10.000 demetin tekrar arabelleğinde saklanmış olsa bile, yapmayı seçebileceğiniz şey, bir modeli her eğittiğinizde 10.000'in tamamını kullanmamaktır. Bunun yerine, yapabileceğiniz şey sadece altkümeyi almaktır. Bu s, a, R of s, s asal demetlerinden yalnızca 1000 örnek seçebilir ve bunu sinir ağını eğitmek için yalnızca 1000 eğitim örneği oluşturmak için kullanabilirsiniz. Bunun, bir model eğitiminin her yinelemesini biraz daha gürültülü ama çok daha hızlı hale getireceği ve bu, genel olarak bu pekiştirmeli öğrenme algoritmasını hızlandırma eğiliminde olacağı ortaya çıktı. Mini yığın oluşturma, hem doğrusal regresyon gibi denetimli bir öğrenme algoritmasını hem de bu takviyeli öğrenme algoritmasını bu şekilde hızlandırabilir; tekrar arabelleğiniz. Son olarak, algoritmanın daha güvenilir bir şekilde yakınsamasını sağlayabilecek bir iyileştirme daha var, o da, Set Q eşittir Q_new'in bu adımını burada yazdım. Ancak bunun Q'da çok ani bir değişiklik yapabileceği ortaya çıktı. Yeni bir sinir ağını yeniye eğitirseniz, belki şans eseri çok iyi bir sinir ağı olmayabilir. Belki eskisinden biraz daha kötüdür, o zaman Q fonksiyonunuzun üzerine potansiyel olarak daha kötü gürültülü bir sinir ağı yazdınız. Yumuşak güncelleme yöntemi, Q_new'in tek bir şanssız adımla kötüye gitmesini önlemeye yardımcı olur. Özellikle, sinir ağı Q, sinir ağındaki tüm katmanlar için tüm parametreler olan W ve B gibi bazı parametrelere sahip olacaktır. Yeni sinir ağını eğittiğinizde, W_new ve B_new bazı parametreleri alırsınız. O slayttaki orijinal S [duyulmuyor] algoritmasında, W'yi W_new'e ve B'yi B_new'e eşit olarak ayarlarsınız. Q eşittir Q_new'in anlamı budur. Yumuşak güncellemeyle, bunun yerine W Kümesi 0,01 çarpı W_new artı 0,99 çarpı W'ye eşittir. Diğer bir deyişle, W'yi, W'nin eski sürümünün yüzde 99'u artı yeni W_new sürümünün yüzde biri olacak şekilde yapacağız. Buna yumuşak güncelleme denir, çünkü ne zaman yeni bir sinir ağı W_new eğitsek, yeni değerin yalnızca küçük bir kısmını kabul edeceğiz. Benzer şekilde, B, 0,01 çarpı B_yeni artı 0,99 çarpı B'ye eşittir. Bu sayılar, 0,01 ve 0,99, ayarlayabileceğiniz hiperparametrelerdir, ancak W'yi W_yeni'ye ne kadar agresif bir şekilde hareket ettireceğinizi kontrol eder ve bu iki sayının toplamının bir olması beklenir. Bir uç nokta, W'yi bir çarpı W_new artı 0 çarpı W'ye eşitlerseniz, bu durumda, W_new'i W'ye kopyaladığınız orijinal algoritmaya geri dönersiniz. Q fonksiyonu Q s, a için mevcut tahmininizi etkileyen Q veya nöral ağ parametreleri W ve B'de daha kademeli bir değişiklik yapın. Yumuşak güncelleme yöntemini kullanmanın, takviyeli öğrenme algoritmasının daha güvenilir bir şekilde yakınsamasına neden olduğu ortaya çıktı. Takviyeli öğrenme algoritmasının salınması veya yön değiştirmesi veya diğer istenmeyen özelliklere sahip olması olasılığını azaltır. Algoritmaya yapılan bu son iki iyileştirmeyle, mini kümeleme (sadece takviyeli öğrenmeyi değil, aynı zamanda öğrenmeyi denetlemek için de çok iyi uygulanır) ve ayrıca yumuşak güncellemeler fikri, ay algoritmanızın gerçekten çalışmasını sağlayabilirsiniz. Lunar Lander'da iyi. Lunar Lander aslında yeterince karmaşık, yeterince zorlayıcı bir uygulamadır ve böylece onu çalıştırabilir ve aya güvenli bir şekilde inebilirsiniz. Bence bu gerçekten harika ve umarım uygulama laboratuvarı ile oynamaktan zevk alırsınız. Takviyeli öğrenme hakkında çok konuştuk. Bitirmeden önce, takviyeli öğrenmenin durumu hakkındaki düşüncelerimi sizinle paylaşmak istiyorum, böylece dışarı çıkıp denetimli, denetimsiz, takviyeli öğrenme teknikleri aracılığıyla farklı makine öğrenimi teknikleri kullanarak uygulamalar oluştururken anlamak için bir çerçeveye sahip olursunuz. Takviyeli öğrenmenin günümüzün makine öğrenimi dünyasına uyduğu yer. Bir sonraki videoda buna bir göz atalım.

## The state of reinforcement learning
Takviyeli öğrenme, heyecan verici bir teknolojiler dizisidir. Aslında doktora tezim üzerinde çalışırken pekiştirmeli öğrenme tezimin konusuydu. Bu yüzden bu fikirler beni heyecanlandırdı ve heyecanlandırıyor. Takviyeli öğrenmenin arkasındaki tüm araştırma ivmesine ve heyecanına rağmen, bunun etrafında biraz veya belki bazen çok fazla yutturmaca olduğunu düşünüyorum. Bu yüzden, uygulamalar için faydası açısından pekiştirmeli öğrenmenin bugün nerede olduğuna dair sizinle pratik bir fikir paylaşmayı umuyorum. Takviyeli öğrenmeyle ilgili bazı abartıların nedenlerinden biri, araştırma yayınlarının çoğunun simüle edilmiş ortamlar üzerine yapılmış olmasıdır. Ve hem simülasyonlarda hem de gerçek robotlarda çalışmış biri olarak, size söyleyebilirim ki, bir simülasyonda veya video oyununda çalışmak için bir pekiştirmeli öğrenme albümü elde etmek, gerçek bir robotta çalışmaktan çok daha kolaydır. Pek çok geliştirici, onu simülasyonda çalıştırdıktan sonra bile, gerçek dünyada veya gerçek robotta çalışacak bir şey elde etmenin şaşırtıcı derecede zor olduğu yorumunu yaptı. Ve bu algoritmaları gerçek bir uygulamaya uygularsanız, yaptığınız şeyin gerçek uygulamada çalıştığından emin olmak için dikkat etmenizi umduğum bir sınırlama budur. İkincisi, takviyeli öğrenme hakkında medyada yer alan tüm haberlere rağmen, bugün gözetimli ve gözetimsiz öğrenmeye göre takviyeli öğrenmenin çok daha az uygulaması var. Bu nedenle, pratik bir uygulama oluşturuyorsanız, denetimli öğrenmeyi veya denetimsiz öğrenmeyi faydalı veya iş için doğru aracı bulma ihtimaliniz, takviyeli öğrenmeyi kullanma ihtimalinizden çok daha yüksektir. Takviyeli öğrenmeyi özellikle robotik kontrol uygulamalarında birkaç kez kullandım, ancak günlük uygulamalı çalışmalarımda, denetimli ve denetimli öğrenmeyi çok daha fazla kullanıyorum.
Takviyeli öğrenme konusunda şu anda pek çok heyecan verici araştırma var ve bence takviyeli öğrenmenin gelecekteki uygulamalar için potansiyeli çok büyük. Takviyeli öğrenme, hâlâ makine öğreniminin temel direklerinden biri olmaya devam ediyor. Ve kendi makine öğrenimi algoritmalarınızı geliştirirken bunu bir çerçeve olarak kullanmak, çalışan makine öğrenimi sistemleri oluşturmada da sizi daha etkili kılacağını umuyorum. Bu yüzden, umarım takviyeli öğrenmeyle ilgili bu haftaki materyalleri beğenmişsinizdir ve özellikle de umarım aya iniş aracını kendiniz inişe geçirirken eğlenmişsinizdir. Bir algoritmayı uyguladığınızda ve ardından yazdığınız kod sayesinde aya iniş yapan aracın aya güvenli bir şekilde indiğini gördüğünüzde tatmin edici bir deneyim olacağını umuyorum. Bu da bizi bu uzmanlığın sonuna getiriyor. Gelelim bitirdiğimiz son videoya.

