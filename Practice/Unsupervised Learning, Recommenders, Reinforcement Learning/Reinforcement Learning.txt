## What is Reinforcement Learning?
Makine öğrenimi uzmanlığının bu son haftasına hoş geldiniz. Bu uzmanlığın sonuna yaklaşıyor olmamız benim için biraz buruk ama bu haftayı sabırsızlıkla bekliyorum, sizinle takviyeli öğrenme hakkında bazı heyecan verici fikirleri paylaşıyorum. Makine öğreniminde, takviye öğrenimi, bugün henüz ticari uygulamalarda çok yaygın olarak uygulanmasa da makine öğreniminin temel direklerinden biri olan fikirlerden biridir. Ve onu destekleyen ve her gün geliştiren çok sayıda heyecan verici araştırma var. Pekiştirmeli öğrenmenin ne olduğuna bir göz atarak başlayalım. Bir örnekle başlayalım. İşte otonom bir helikopterin resmi. Bu aslında Stanford otonom helikopteri, 32 pound ağırlığında ve şu anda ofisimde duruyor. Diğer birçok otonom helikopter gibi, yerleşik bir bilgisayar, GPS, ivmeölçerler, jiroskoplar ve manyetik pusula ile donatılmıştır, böylece her zaman nerede olduğunu oldukça doğru bir şekilde bilir. Size bu helikopterin anahtarlarını versem ve onu uçurmak için bir program yazmanızı istesem, bunu nasıl yaparsınız? Radyo kontrollü helikopterler bunun gibi joysticklerle kontrol edilir ve bu nedenle görev saniyede on kez size helikopterin konumu, yönü ve hızı verilir. Ve helikopteri havada dengede tutabilmek için bu iki kontrol çubuğunu nasıl hareket ettireceğinize karar vermelisiniz. Bu arada, radyo kontrollü helikopterlerin yanı sıra dört rotorlu dronları kendim uçurdum. Ve radyo kontrollü helikopterleri uçurmak, havada dengede tutmak aslında biraz daha zordur. Peki bunu otomatik olarak yapacak bir programı nasıl yazarsınız? Size Stanford otonom helikopterimizle yapacağımız bir şeyin eğlenceli bir videosunu göstereyim. İşte bir takviyeli öğrenme algoritmasının kontrolü altında uçtuğu bir video. Ve videoyu oynatmama izin ver. O gün aslında kameraman bendim ve bu bilgisayar kontrolünde uçan helikopter ve videoyu uzaklaştırırsam gökyüzüne dikilmiş ağaçları görüyorsunuz. Takviyeli öğrenmeyi kullanarak, aslında baş aşağı uçmayı öğrenmek için bu helikopteri aldık. Baş aşağı uçmasını söyledik. Ve böylece güçlendirilmiş öğrenme, helikopterlerin çok çeşitli akrobasi hareketlerini uçurmasını sağlamak için kullanıldı ya da biz onlara akrobasi manevraları diyoruz. Bu arada, diğer videoları görmekle ilgileniyorsanız, bu URL'den onlara da göz atabilirsiniz. Peki pekiştirmeli öğrenmeyi kullanarak bir helikopterin kendi kendine uçmasını nasıl sağlarsınız? Görev, kontrol çubuklarını nasıl hareket ettireceğine karar vermek için helikopterin konumuna verilir. Takviyeli öğrenmede, helikopterin konumu, yönü ve hızı vb. durumlarını s olarak adlandırırız. Ve böylece görev, helikopterin durumundan eylem a'ya eşlenen bir işlev bulmaktır, yani helikopteri havada dengede tutmak, uçmak ve çarpmadan tutmak için iki kontrol çubuğunun ne kadar uzağa itilmesi gerektiği anlamına gelir. Bu sorunu denemenin bir yolu denetimli öğrenmeyi kullanmaktır. Bunun otonom helikopter uçuşu için harika bir yaklaşım olmadığı ortaya çıktı. Ama diyebilirsiniz ki, bir dizi durum gözlemi alabilirsek ve belki de uzman bir insan pilot bize yapılacak en iyi eylemin ne olduğunu söyler. Daha sonra, burada x olarak adlandırdığım durumlardan, burada y olarak adlandırdığım bir eyleme eşlemeyi doğrudan öğrenmek için denetimli öğrenmeyi kullanarak bir sinir ağını eğitebilirsiniz. Ancak, helikopter havada hareket ederken aslında çok belirsiz olduğu ortaya çıktı, yapılacak doğru eylemin tam olarak ne olduğu. Biraz sola mı yoksa çok daha fazla sola mı yatırıyorsunuz veya helikopter stresini biraz mı yoksa çok mu artırıyorsunuz? Bir x veri seti ve ideal eylem y elde etmek aslında çok zordur. Bu nedenle, helikopter ve diğer robotlar gibi bir robotu kontrol etme görevinin çoğunda, denetimli öğrenme yaklaşımı iyi çalışmıyor ve bunun yerine takviyeli öğrenme kullanıyoruz. Takviyeli öğrenme için önemli bir girdi, helikoptere ne zaman iyi ve ne zaman kötü gittiğini söyleyen ödül veya ödül işlevi olarak adlandırılan bir şeydir. Bu yüzden, ödül işlevini düşünmeyi sevdiğim şekilde, bir köpeği eğitmeye benziyor. Ben büyürken, ailemin bir köpeği vardı ve benim işim köpeği ya da yavruyu terbiye etmekti. Peki bir köpek yavrusu nasıl iyi davranır? Pekala, köpeğe bu kadarını gösteremezsin. Onun yerine işini yapmasına izin veriyorsun ve ne zaman iyi bir şey yapsa, sen gidiyorsun, ahbap. Ve ne zaman kötü bir şey yapsalar, gidersin, kötü köpek. Ve sonra umarım kendi kendine nasıl daha çok iyi köpek ve daha az kötü köpek işi yapılacağını öğrenir. Takviyeli öğrenme algoritması ile eğitim işte böyle bir şey. Helikopter iyi uçtuğunda, gidersiniz, iyi helikopter ve çarpma gibi kötü bir şey yaparsa, gidersiniz, kötü helikopter. Ve sonra, iyi helikopterden daha fazlasını ve kötü helikopter sonuçlarından daha azını nasıl elde edeceğinizi bulmak, pekiştirmeli öğrenme algoritmasının işidir.
Takviyeli öğrenmenin neden bu kadar güçlü olduğunu düşünmenin bir yolu, ona nasıl yapılacağından çok ne yapması gerektiğini söylemeniz gerektiğidir. Ve en uygun eylem yerine ödül işlevini belirtmek, sistemi nasıl tasarladığınız konusunda size çok daha fazla esneklik sağlar. Helikopteri uçurmak için somut olarak, ne zaman iyi uçuyorsa, ona iyi uçtuğu her saniye artı bir ödül verebilirsiniz. Ve belki de ne zaman kötü uçuyorsa, ona negatif bir ödül verebilirsiniz ya da düşerse, negatif 1000 gibi çok büyük bir negatif ödül verebilirsiniz. Ve bu, helikopteri iyi uçmak için çok daha fazla zaman harcamaya ve umarım asla düşmemeye teşvik eder. Ama işte başka bir eğlenceli video. Uzun yıllardır pekiştirmeli öğrenme için iyi köpek kötü köpek benzetmesini kullanıyordum. Ve sonra bir gün gerçekten robotik bir köpeğe el atmayı başardım ve bu pekiştirmeyi gerçekten iyi köpek kötü köpek öğrenme metodolojisini bir robot köpeği engelleri aşması için eğitmek için kullanabilirdim. Bu, onu ödüllendiren takviyeli öğrenmeyi kullanarak ekranın soluna doğru hareket ederek ayaklarını dikkatli bir şekilde yerleştirmeyi veya çeşitli engellerin üzerinden nasıl tırmanacağını öğrenen bir robot köpeğin videosu. Ve bir köpeği böyle programlamak için ne gerektiğini düşünürseniz, hiçbir fikrim yok, belirli bir engeli aşmak için bacaklarını yerleştirmenin en iyi yolunun ne olduğunu ona nasıl söyleyeceğimi gerçekten bilmiyorum. Bütün bunlar, robot tarafından sadece onu teşvik eden ödüller vererek, ekranın solundaki hedefe doğru ilerleme kaydederek otomatik olarak çözüldü. Günümüzde pekiştirmeli öğrenme, robotları kontrol etmekten çok çeşitli uygulamalara başarıyla uygulanmaktadır. Ve aslında bu haftanın ilerleyen saatlerinde uygulama laboratuvarında, simülasyonda bir aya iniş yapmak için kendinize bir pekiştirmeli öğrenme algoritması uyguluyorsunuz. Fabrika optimizasyonu için de kullanıldı. İş hacmini ve verimliliği ve finansal hisse senedi alım satımını en üst düzeye çıkarmak için fabrikadaki şeyleri nasıl yeniden düzenlersiniz? Örneğin, bir arkadaşım verimli stok uygulaması üzerinde çalışıyordu. Dolayısıyla, önümüzdeki birkaç gün içinde bir milyon hisse satmaya karar verdiyseniz, borsada aniden bir milyon hisseyi düşürmek istemeyebilirsiniz çünkü bu, fiyatları aleyhinize hareket ettirecektir. Öyleyse, satmak istediğiniz hisseleri satabilmeniz ve onlar için mümkün olan en iyi fiyatı alabilmeniz için alım satımlarınızı zaman içinde sıralamanın en iyi yolu nedir? Son olarak, pek çok video oyunu oynamanın yanı sıra damadan satranca, briç kart oyununa kadar oyun oynamaya kadar pek çok takviyeli öğrenme uygulaması da olmuştur. İşte bu pekiştirmeli öğrenmedir. Takviyeli öğrenme, neredeyse denetimli öğrenme kadar kullanılmasa da, günümüzde hala birkaç uygulamada kullanılmaktadır. Ve ana fikir, algoritmaya her bir girdi için doğru y çıktısının ne olduğunu söylemeniz yerine, tek yapmanız gereken ona ne zaman iyi gittiğini ve ne zaman kötü gittiğini söyleyen bir ödül işlevi belirtmektir. Ve iyi eylemlerin nasıl seçileceğini otomatik olarak bulmak algoritmanın işidir. Bununla birlikte, pekiştirmeli öğrenme problemini formüle edeceğimiz ve aynı zamanda iyi eylemleri otomatik olarak seçmek için algoritmalar geliştirmeye başlayacağımız bir sonraki videoya geçelim.

## Mars rover example
Takviyeli öğrenme şekilciliğini bitirmek için, bir helikopter veya robot köpek gibi karmaşık bir şeye bakmak yerine, genel olarak Mars gezicisinden ilham alan basitleştirilmiş bir örnek kullanabiliriz. Bu, Stanford profesörü Emma Branskill ve iş arkadaşlarımdan biri olan Jagriti Agrawal'ın şu anda Mars gezicisini gerçekten kontrol eden bir kod yazması nedeniyle örnekten uyarlandı ve bu da bu örneği geliştirmeme yardımcı oldu ve konuşmama yardımcı oldu. Hadi bir bakalım. Mars gezicisinden esinlenen basitleştirilmiş bir örnek kullanarak pekiştirmeli öğrenme geliştireceğiz. Bu uygulamada, gezici, buradaki altı kutuda gösterildiği gibi altı konumdan herhangi birinde olabilir. Gezici, örneğin, burada gösterilen dördüncü kutuya doğru yola çıkabilir. Mars gezicisinin konumu pekiştirmeli öğrenmede durum olarak adlandırılır ve ben bu altı durumu, durum 1, durum 2, durum 3, durum 4, durum 5 ve durum 6 olarak adlandıracağım ve böylece gezici başlıyor 4. durumda kapalı. Şimdi gezici, farklı bilim görevlerini yerine getirmeye çalışmak için Mars'a gönderildi. Gezegendeki farklı yerlerdeki kayayı analiz etmek için matkap, radar veya spektrometre gibi sensörlerini kullanmak üzere farklı yerlere gidebilir veya dünyadaki bilim adamlarının bakması için ilginç fotoğraflar çekmek üzere farklı yerlere gidebilir. Bu örnekte, burada soldaki durum 1, bilim adamlarının yüzey aracının örneklemesini isteyeceği çok ilginç bir yüzeye sahiptir. Durum 6 ayrıca, bilim adamlarının gezicinin örneklemesini isteyeceği oldukça ilginç bir yüzeye sahiptir, ancak durum 1 kadar ilginç değildir. Bilim görevini ve durum 1'i gerçekleştirme olasılığımız, durum 6'dan daha fazladır, ancak durum 1 daha uzaktadır. . Durum 1'in potansiyel olarak daha değerli olduğunu yansıtmanın yolu, ödül işlevidir. 1. durumdaki ödül 100'dür ve 6. aşamadaki ödül 40'tır ve aradaki diğer tüm eyaletlerdeki ödüller, sıfır ödülü olarak yazacağım çünkü bilim için o kadar ilginç bir şey yok. 2, 3, 4 ve 5 durumlarında yapılabilir. Her adımda, gezici iki eylemden birini seçer. Ya sola gidebilir ya da sağa gidebilir. Soru şu ki, gezici ne yapmalı? Takviyeli öğrenmede, ödüllere çok dikkat ederiz çünkü robotun iyi mi kötü mü yaptığını bu şekilde anlarız. Robot 4. durumdan başlayarak sola giderse ne olabileceğine dair bazı örneklere bakalım. Ardından başlangıçta 4. durumdan başlayarak, sıfır ödülü alacak ve sola gittikten sonra 3. duruma geçecek ve burada tekrar sıfır ödül alır. Sonra 2 durumuna gelir, 0 ödülünü alır ve son olarak 100'lük bir ödül aldığı 1 durumuna gelir. Bu uygulama için, ya 1. ya da 6. duruma geldiğinde, gün biter Takviyeli öğrenmede, buna bazen bir son durum diyoruz ve bunun anlamı, bu uç durumlardan birine ulaştıktan sonra, o durumda bir ödül alıyor, ancak bundan sonra hiçbir şey olmuyor. Belki robotların yakıtı bitmiştir veya o gün için zamanları kalmamıştır, bu yüzden sadece 100 veya 40 ödülünün tadını çıkarabilir, ama o gün için bu kadar. Bundan sonra ek ödüller kazanamaz. Artık robot sola gitmek yerine sağa gitmeyi de seçebilir, bu durumda 4 durumundan önce sıfır ödülü alır ve sonra sağa hareket eder ve 5 durumuna geçer, başka bir ödül alır. sıfır ve sonra sağdaki bu diğer terminal durumuna, durum 6'ya ulaşacak ve 40'lık bir ödül alacak. Ancak sola gitmek ve sağa gitmek tek seçenek. Robotun yapabileceği bir şey, 4. durumdan başlayıp sağa doğru hareket etmeye karar verebilmesidir. Durum 4-5'ten gider, durum 4'te sıfır ve durum 5'te sıfır ödülü alır ve sonra belki fikrini değiştirir ve aşağıdaki gibi sola gitmeye karar verir, bu durumda 4. durumda, 3. durumda, 2. durumda sıfır ödülü ve ardından 1. duruma geldiğinde 100 ödül. Bu eylemler ve durumlar dizisinde, robot daha iyi zaman harcıyor. Bu, harekete geçmek için harika bir yol olmayabilir, ancak algoritmanın seçebileceği bir seçimdir, ancak umarım bunu seçmezsiniz. Özetlemek gerekirse, her adımda, robot bir durumda, ben buna S diyeceğim ve bir eylem seçiyor ve ayrıca bu durumdan aldığı bazı ödüllerden, R of S'den keyif alıyor. Bu eylemin bir sonucu olarak, yeni bir S üssü durumuna geçer. Somut bir örnek olarak, robot 4. durumdayken ve sola git eylemini yaptığında, belki o 4. durumla ilişkili sıfır ödülünün tadını çıkarmadı ve yeni bir 3. durumu olmayacak. Özel pekiştirmeli öğrenme algoritmalarında, bu dört şeyin, durum, eylem, ödül ve sonraki durum olduğunu görürsünüz; bu, temel olarak her eylemde bulunduğunuzda olan şeydir ve takviyeli öğrenme algoritmalarının karar verirken bakacağı temel unsurlardır. nasıl önlemler alınır.
Açıklığa kavuşturmak için, buradaki ödül, R of S, bu, bu durumla ilişkili ödüldür. Bu sıfır ödülü, durum 3'ten ziyade durum 4 ile ilişkilendirilir. Takviyeli öğrenme uygulamasının çalışma şekli budur. Bir sonraki videoda, pekiştirmeli öğrenme algoritmasının yapmasını istediğimiz şeyi tam olarak nasıl belirttiğimize bir göz atalım. Özellikle pekiştirmeli öğrenmede geri dönüş adı verilen önemli bir fikirden bahsedeceğiz. Bunun ne anlama geldiğini görmek için bir sonraki videoya geçelim.

## The Return in reinforcement learning
Bir önceki videoda pekiştirmeli öğrenme uygulamasında durumların neler olduğunu ve yaptığınız işlemlere bağlı olarak nasıl farklı durumlardan geçtiğinizi ve farklı ödüller kazandığınızı gördünüz. Ancak belirli bir ödül setinin farklı bir ödül setinden daha iyi veya daha kötü olduğunu nasıl anlarsınız? Bu videoda tanımlayacağımız pekiştirmeli öğrenmedeki geri dönüş, bunu yakalamamızı sağlıyor. Bunu incelerken, faydalı bulabileceğiniz bir benzetme şu: Ayağınızın dibinde beş dolarlık bir banknot olduğunu hayal edin, aşağı uzanıp alabilirsiniz veya yarım saatte kasabanın öbür ucuna, yarım saatte yürüyebilir ve 10 dolarlık bir banknot al. Hangisinin peşinden gitmeyi tercih edersin? On dolar, beş dolardan çok daha iyidir, ancak gidip o 10 dolarlık banknotu almak için yarım saat yürümeniz gerekiyorsa, o zaman belki onun yerine beş dolarlık banknotu almanız daha uygun olabilir. Geri dönüş kavramı, daha hızlı alabileceğiniz ödüllerin, ulaşmanız uzun zaman alan ödüllerden belki daha çekici olduğunu yakalar. Bunun tam olarak nasıl çalıştığına bir göz atalım. İşte bir Mars Rover örneği. Durum 4'ten başlayarak sola giderseniz, aldığınız ödüllerin durum 4'ten ilk adımda sıfır, durum 3'ten sıfır, durum 2'den sıfır ve ardından uç durum olan durum 1'de 100 olacağını gördük. Getiri, bu ödüllerin toplamı olarak tanımlanır, ancak indirim faktörü olarak adlandırılan ek bir faktörle ağırlıklandırılır. İndirgeme faktörü 1'den biraz küçük bir sayıdır. İndirgeme faktörü olarak 0.9'u seçeyim. İlk adımdaki ödülü sıfır olarak ağırlıklandıracağım, ikinci adımdaki ödül bir indirim faktörü, bu ödülün 0,9 katı ve sonra artı indirim faktörü^2 çarpı bu ödül ve sonra artı indirim faktörü ^ 3 kat bu ödül. Bunu hesaplarsanız 0,729 çarpı 100 yani 72,9 çıkıyor. Geri dönüşün daha genel formülü, eğer robotunuz bir dizi durumdan geçerse ve birinci adımda R_1, ikinci adımda R_2 ve üçüncü adımda R_3 vb. alırsa, o zaman dönüş R_1 olur. artı indirim faktörü Gama, bu örnekte 0,9 olarak ayarladığım bu Yunan alfabesi Gamma, Gama çarpı R_2 artı Gama^2 çarpı R_3 artı Gamma^3 çarpı R_4 vb. siz terminal durumuna gelene kadar. İndirgeme faktörü Gamma'nın yaptığı şey, takviyeli öğrenme algoritmasını biraz sabırsız hale getirme etkisine sahip olmasıdır. Çünkü geri dönüş birinci ödüle tam kredi veriyor yüzde 100 yani 1 katı R_1 ama sonra biraz daha az kredi veriyor ikinci adımda aldığınız ödül 0,9 ile çarpılıyor ve daha sonra aldığınız ödüle daha da az kredi veriyor. bir sonraki adım R_3'te vb. alın ve böylece ödülleri daha erken almak, toplam getiri için daha yüksek bir değerle sonuçlanır. Pek çok takviyeli öğrenme algoritmasında, indirgeme faktörü için ortak bir seçim, 0,9 veya 0,99 veya hatta 0,999 gibi 1'e oldukça yakın bir sayı olacaktır. Ancak kullanacağım çalışan örnekte açıklama amacıyla, aslında 0,5'lik bir indirgeme faktörü kullanacağım. Bu çok ağır bir şekilde, gelecekte ödülleri çok ağır bir şekilde düşürür, çünkü her ek ayrıştırma zaman damgasıyla, bir adım önce alacağınız ödüllerin yalnızca yarısı kadar kredi alırsınız. Gama 0,5'e eşit olsaydı, yukarıdaki örnekte getiri 0 artı 0,5 çarpı 0 olurdu, bu denklemi üste koyar, artı 0,5^2 0 artı 0,5^3 çarpı 100. Durum 1'den uç duruma geçtiği için bu ödül kaybedildi, ve bu 12.5'lik bir getiri olarak çıkıyor. Finansal uygulamalarda iskonto faktörünün de faiz oranı veya paranın zaman değeri olarak çok doğal bir yorumu vardır. Bugün bir dolarınız varsa, bu, gelecekte yalnızca bir dolarınız olacak duruma göre biraz daha değerli olabilir. Çünkü bugün bir doları bile bankaya yatırabilir, biraz faiz kazanabilir ve bundan bir yıl sonra biraz daha fazla paraya sahip olabilirsiniz. Finansal uygulamalar için, genellikle, bu iskonto faktörü, bugün bir dolarla karşılaştırdığımda gelecekte bir doların ne kadar az olduğunu temsil eder. Bazı somut getiri örneklerine bakalım. Aldığınız geri dönüş, ödüllere bağlıdır ve ödüller, yaptığınız eylemlere bağlıdır ve dolayısıyla geri dönüş, yaptığınız eylemlere bağlıdır. Her zamanki örneğimizi kullanalım ve bu örnek için diyelim ki, ben her zaman sola gideceğim. Robot 4 durumunda başlarsa, önceki slaytta hesapladığımız gibi dönüşün 12,5 olduğunu daha önce görmüştük. Üçte başlasaydı, getiri 25 olurdu çünkü 100 ödüle bir adım daha erken ulaşır ve bu nedenle daha az iskonto edilir. Durum 2'de başlasaydı, dönüş 50 olurdu. Yeni başlayıp durum 1 olsaydı, pekala, hemen 100'lük ödülü alır, yani düşük indirimli olmaz.
Durum 1'den başlarsak getiri 100 olur ve bu iki durumdaki dönüş 6.25 olur. Görünüşe göre, son durum olan 6 durumundan başlarsanız, sadece ödülü ve dolayısıyla 40'ın geri dönüşünü alırsınız. Şimdi, farklı bir eylemler dizisi yapacak olsaydınız, geri dönüşler aslında farklı olurdu. Örneğin, her zaman sağa gidersek, eylemlerimiz bunlar olsaydı, o zaman 4. durumda başlarsanız, 0 ödül alırsınız. Sonra 5. duruma gelirsiniz, 0 ödül alırsınız ve 6 durumuna gelir ve 40'lık bir ödül alır. Bu durumda getiri 0 artı 0,5 olur, iskonto faktörü çarpı 0 artı 0,5'in karesi çarpı 40 olur ve bu da 0,5'in karesinin 1/4 olduğu ortaya çıkar, yani 40'ın 1/4'ü 10'dur. Bu halden, 4. halden getirisi 10'dur. Harekete geçecekseniz daima sağa gidin. Benzer bir mantıkla bu durumdan getiri 20, bu durumdan getiri beş, bu durumdan getiri 2,5 ve sonra dönüş determinant durumu 140 olur. tamamen mantıklıysa, videoyu duraklatıp matematiği iki kez kontrol etmekten çekinmeyin ve bunların geri dönüş için uygun değerler olduğuna kendinizi ikna edip edemeyeceğinize bakın. Çünkü farklı hallerden yola çıkarsanız ve her zaman sağa gidecekseniz. Her zaman sağa gittiğini görüyoruz. Almayı beklediğiniz getiri çoğu eyalet için daha düşüktür. Belki de her zaman sağa gitmek, her zaman sola gitmek kadar iyi bir fikir değildir. Ama her zaman sola gitmek zorunda olmadığımız, her zaman sağa gittiğimiz ortaya çıktı. Ayrıca 2. durumda olup olmadığınıza da karar verebiliriz, sola gidin. Durum 3'teyseniz, sola gidin. 4. durumdaysanız sola gidin. Ancak 5. durumdaysanız, bu ödüle çok yakınsınız demektir. Sağa gidelim. Bu, içinde bulunduğunuz duruma göre yapılacak eylemleri seçmenin farklı bir yolu olacaktır. Farklı durumlardan alacağınız getiri 100, 50, 25, 12.5, 20 ve 40 olacaktır. Bir vaka. 5. durumda başlayacak olsaydınız, burada sağa giderdiniz ve böylece aldığınız ödüller önce 5 durumunda sıfır, sonra 4 olur. Getiri sıfırdır, ilk ödül artı indirim faktörü 0,5 çarpı 40 yani 20 yani burada gösterilen işlemleri yaparsanız bu durumdan 20 dönüş neden oluyor. Özetlemek gerekirse, takviyeli öğrenmedeki geri dönüş, sistemin aldığı ödüllerin indirim faktörü ile ağırlıklandırılmış toplamıdır; burada uzak gelecekteki ödüller, indirim faktörü tarafından daha yüksek bir güce yükseltilerek ağırlıklandırılır. Şimdi, negatif ödülleri olan sistemleriniz olduğunda bunun aslında ilginç bir etkisi var. İncelediğimiz örnekte, tüm ödüller sıfır veya pozitifti. Ancak herhangi bir ödül negatifse, o zaman indirim faktörü aslında sistemi negatif ödülleri olabildiğince geleceğe itmeye teşvik eder. Mali bir örnek alırsak, birine 10$ ödemek zorunda kalsaydınız, bu belki eksi 10'luk bir negatif ödüldür. faiz oranı aslında bugün ödemek zorunda olduğunuz 10 dolardan daha az değerdedir. Negatif ödülleri olan sistemler için, algoritmanın ödülleri olabildiğince geleceğe itmeye çalışmasına neden olur. Finansal uygulamalar ve diğer uygulamalar için, bu aslında sistemin yapması gereken doğru şey olarak ortaya çıkıyor. Takviyeli öğrenmede geri dönüşün ne olduğunu artık biliyorsunuz, takviyeli öğrenme algoritmasının hedefini formüle etmek için bir sonraki videoya geçelim.

## Making decisions: Policies in reinforcement learning
Bir pekiştirmeli öğrenme algoritmasının eylemleri nasıl seçtiğini resmileştirelim. Bu videoda, takviyeli öğrenme algoritmasında bir politikanın ne olduğunu öğreneceksiniz. Hadi bir bakalım. Gördüğümüz gibi, pekiştirmeli öğrenme probleminde harekete geçmenin birçok farklı yolu vardır. Örneğin, her zaman en yakın ödül için gitmeye karar verebiliriz, böylece bu en soldaki ödül daha yakınsa sola gidersiniz veya bu en sağdaki ödül daha yakınsa sağa gidersiniz. Eylemleri seçmemizin başka bir yolu da, her zaman daha büyük ödül için gitmektir veya her zaman daha küçük ödül için gidebiliriz, iyi bir fikir gibi görünmüyor, ancak bu başka bir seçenek veya sadece değilseniz sola gitmeyi seçebilirsiniz. daha az ödülden bir adım uzaklaşırsanız, bu durumda onu tercih edersiniz. Takviyeli öğrenmede amacımız, işi herhangi bir durumu girdi olarak almak ve bunu bizden yapmamızı istediği bazı eylemlere haritalamak olan, politika Pi adı verilen bir işlev bulmaktır. Örneğin, alttaki bu politika için, bu politika, eğer 2. durumdaysanız, bizi sol eyleme eşler. 3. durumdaysanız, politika sola gidin diyor. 4. durumdaysanız ayrıca sola gidin ve 5. durumdaysanız sağa gidin. S durumuna uygulanan Pi, bize bu durumda hangi eylemi yapmamızı istediğini söyler. Takviyeli öğrenmenin amacı, getiriyi en üst düzeye çıkarmak için her durumda hangi eylemi yapmanız gerektiğini size söyleyen bir Pi veya Pi of S politikası bulmaktır. Bu arada, politikanın pi'nin ne olduğunu en açıklayıcı terim olup olmadığını bilmiyorum ama pekiştirmeli öğrenmede standart hale gelen terimlerden biri. Belki Pi'ye bir politika yerine bir denetleyici demek daha doğal bir terminoloji olur, ancak politika, pekiştirmeli öğrenmedeki herkesin şimdi buna dediği şeydir. Son videoda, durumlardan ödüllere, getirilere ve politikalara kadar pek çok pekiştirmeli öğrenme konseptini inceledik. Bir sonraki videoda bunları hızlı bir şekilde gözden geçirelim ve ardından bu politikaları bulmak için algoritmalar geliştirmeye başlayacağız. Bir sonraki videoya geçelim.

## Review of key concepts
Altı durumlu Mars gezgini örneğini kullanarak bir pekiştirmeli öğrenme formalizmi geliştirdik. Anahtar kavramlara hızlıca bir göz atalım ve bu kavram setinin diğer uygulamalar için de nasıl kullanılabileceğini görelim. Tartıştığımız kavramlardan bazıları, takviyeli öğrenme probleminin durumları, eylemler dizisi, ödüller, bir indirim faktörü, ardından ödüllerin ve indirim faktörünün birlikte getiriyi hesaplamak için nasıl kullanıldığı ve son olarak, işi kimin yaptığı bir politikadır. getiriyi en üst düzeye çıkarmak için eylemleri seçmenize yardımcı olmaktır. Mars gezgini örneği için 1-6 arasında numaralandırdığımız altı durumumuz vardı ve eylemler sola veya sağa gitmek şeklindeydi. Ödüller en soldaki durum için 100, en sağdaki durum için 40 ve arada sıfırdı ve 0,5'lik bir indirim faktörü kullanıyordum. Geri dönüş bu formül tarafından verildi ve Pi'nin eylemleri tasvir ettiği, içinde bulunduğunuz duruma bağlı olarak farklı politikaları olabilir. Aynı biçimcilik veya durumlar, eylemler, ödüller vb. birçok başka uygulama için de kullanılabilir. Sorunu çözün veya otonom bir helikopter bulun. Bir durum belirlemek, helikopterin olası konumları, yönelimleri ve hızları vb. kümesi olacaktır. Olası eylemler, bir helikopterin kontrol çubuğunu hareket ettirmenin olası yolları olabilir ve ödüller, iyi uçuyorsa artı bir ve gerçekten kötü düşmüyorsa veya çarpmıyorsa eksi 1.000 olabilir. Helikopterin ne kadar iyi uçtuğunu size söyleyen ödül işlevi. İndirgeme faktörü, birden biraz daha küçük bir sayı, diyelim ki, 0.99 ve sonra ödüllere ve indirim faktörüne bağlı olarak, getiriyi aynı formülü kullanarak hesaplarsınız. Takviyeli bir öğrenme algoritmasının işi, helikopterin konumu olan girdi olarak verildiğinde, size hangi eylemi yapmanız gerektiğini söyleyen Pi'nin bir politikasını bulmak olacaktır. Yani kontrol çubuklarını nasıl hareket ettireceğinizi anlatır. İşte bir örnek daha. İşte oyun oynayan bir tane. Satranç oynamayı öğrenmek için pekiştirmeli öğrenmeyi kullanmak istediğinizi varsayalım. Bu sorunun durumu, tahtadaki tüm taşların konumu olacaktır. Bu arada, satranç oynuyorsanız ve kuralları iyi biliyorsanız, bunun satranç için sadece taşların konumundan biraz daha fazla bilgi olduğunu biliyorum ama bu video için biraz basitleştireceğim. Eylemler, oyundaki olası yasal hamlelerdir ve bu durumda, sisteminize bir oyunu kazanırsa artı bir, oyunu kaybederse eksi bir ve eğer oyunu kaybederse eksi bir ödül verirseniz, ortak bir ödül seçimi olacaktır. bir oyun bağlar. Satranç için, genellikle bire çok yakın bir indirim faktörü kullanılacaktır, bu nedenle belki 0,99, hatta 0,995 veya 0,999 olabilir ve geri dönüş, diğer uygulamalarla aynı formülü kullanır. Bir kez daha, hedefe bir politika Pi kullanarak iyi bir eylem seçmesi için bir tahta pozisyonu verilir. Bir pekiştirmeli öğrenme uygulamasının bu şekilciliğinin aslında bir adı vardır. Buna Markov karar süreci deniyor ve bunun kulağa büyük, teknik açıdan karmaşık bir terim gibi geldiğini biliyorum. Ancak bu Markov karar süreci veya kısaca MDP terimini duyarsanız, bu son birkaç videoda bahsettiğimiz biçimciliktir. MDP veya Markov karar sürecindeki Markov terimi, geleceğin mevcut duruma gelmeden önce gerçekleşmiş olabilecek herhangi bir şeye değil, yalnızca mevcut duruma bağlı olduğunu ifade eder. Başka bir deyişle, bir Markov karar sürecinde gelecek, buraya nasıl geldiğinize değil, yalnızca şu anda nerede olduğunuza bağlıdır. Markov karar süreci formalizmini düşünmenin bir başka yolu da, kontrol etmek istediğimiz bir robotumuz veya başka bir aracımız var ve yapacağımız şey a eylemlerini seçmek ve bu eylemlere dayalı olarak dünyada veya dünyada bir şeyler olacak. çevre, örneğin dünyadaki konumumuz değişiyor ya da bir kaya parçasını örnek alıp bilim görevini yerine getiriyoruz. A eylemini seçme şeklimiz Pi politikasıdır ve dünyada olup bitenlere bağlı olarak hangi durumda olduğumuzu ve hangi ödülleri aldığımızı görebilir veya gözlemleyebiliriz. Bazen farklı yazarların Markov karar sürecini veya MDP biçimciliğini temsil etmek için bunun gibi bir diyagram kullandığını görürsünüz, ancak bu, son birkaç videoda öğrendiğiniz kavramlar dizisini göstermenin başka bir yoludur. Artık pekiştirmeli öğrenme probleminin nasıl çalıştığını biliyorsunuz. Bir sonraki videoda, iyi eylemler seçmek için bir algoritma geliştirmeye başlayacağız. Buna yönelik ilk adım, durum eylem değeri işlevini tanımlamak ve sonunda hesaplamayı öğrenmek olacaktır. Bu, bir öğrenme algoritması geliştirmek istediğimizde kilit niceliklerden biri olarak ortaya çıkıyor. Bunun ne olduğunu görmek için bir sonraki videoya geçelim, durum eylem değeri fonksiyonu.


## State-action value function definition
Bu hafta ilerleyen saatlerde takviyeli öğrenmeyi geliştirmeye başladığımızda, takviyeli öğrenme oklarının hesaplamaya çalışacağı önemli bir miktar olduğunu görüyorsunuz ve buna durum eylem değeri fonksiyonu deniyor. Şimdi bu fonksiyonun ne olduğuna bir göz atalım. Durum eylem değeri işlevi, tipik olarak büyük Q harfiyle gösterilen bir işlevdir. Ve bu, içinde olabileceğiniz bir durumun yanı sıra o durumda gerçekleştirmeyi seçebileceğiniz eylemin ve QFSA'nın bir işlevidir. Dönüşe eşit bir sayı verecektir. Bu durumda başlarsanız. S ve A eylemini yalnızca bir kez gerçekleştirin ve A eylemini bir kez gerçekleştirdikten sonra, bundan sonra en uygun şekilde davranın. Bundan sonra, mümkün olan en yüksek getiriyi sağlayacak her türlü eylemi yaparsınız. Şimdi, bu tanımda biraz garip bir şeyler olduğunu düşünebilirsiniz çünkü optimal davranışın ne olduğunu nasıl bilebiliriz? Ve otomatik davranışın ne olduğunu bilseydik, her durumda yapılacak en iyi eylemin ne olduğunu zaten bilseydik, neden hala Q of SA'yı hesaplamamız gerekiyor? Çünkü zaten otomatik politikamız var. Bu yüzden, bu tanımda biraz garip bir şey olduğunu kabul etmek istiyorum. Bu tanımla ilgili neredeyse biraz döngüsel bir şey var, ancak emin olun Belirli pekiştirmeli öğrenme çıktılarına baktığımızda, daha sonra bu biraz döngüsel tanımı çözeceğiz ve Q işlevini daha biz bulmadan önce hesaplamanın bir yolunu bulacağız. optimal politika. Ama bunu daha sonraki bir videoda görüyorsunuz. O yüzden şimdilik bu konuda endişelenme. Daha önce gördüğümüz bir örneğe bakalım, bu oldukça iyi bir politika 2., 3. ve 4. aşamadan sola gidin ve Beşinci Devletten sağa gidin. Bunun mars rover uygulaması için aslında en uygun politika olduğu ortaya çıktı İndirgeme faktörü gamma 0,5 olduğunda, yani S'nin Q'su toplam getiriye eşit olacaktır. daha sonrasında. Anlamı bu politikaya göre hareket etmek. Burada gösterildiği gibi, Q of s,a'nın ne olduğunu bulalım. Birkaç farklı eyalet içindir. Diyelim ki Q of state'e de bakalım. Peki ya sağa gitmek için harekete geçersek, eğer siz ikinci durumdaysanız ve sağa giderseniz, sonra üçüncü duruma gelirsiniz ve bundan sonra en uygun şekilde davranırsanız, ST üçten sola ve sonra sola gidersiniz. eyaletten duruma ve sonunda 100'lük ödülü alırsınız. Bu durumda, aldığınız ödüller eyaletten sıfıra, ikinci duruma geri döndüğünüzde üç sıfır kaldığınızda ve sonra nihayet geldiğinizde 100 olacaktır. uç durum bir ve dönüş sıfır artı 0,5 çarpı bunun artı 0,5 kare çarpı ac artı 0,5 küp çarpı 100 olacaktır. Bunun geçtiğini unutmayın, doğru gitmenin iyi bir fikir olup olmadığına dair bir yargı yoktur. Aslında, durumdan sağa gitmek o kadar iyi bir fikir değil, ancak A eylemini gerçekleştirirseniz ve ardından en uygun şekilde davranırsanız, dönüşü sadakatle bildirir. İşte başka bir örnek. Durumdaysanız ve sola gidecekseniz, ikinci durumdayken alacağınız ödül dizisi sıfır olacak ve ardından 100 olacaktır. Ve böylece dönüş sıfır artı 0,5 çarpı 100, bu da 50'ye eşittir QSA değerlerini yazmak için. Bu şemada, sağa gidişin Q olduğunu göstermek için sağa 12.5 yazacağım. Ve sonra, bunun ST 2'nin Q'su olduğunu göstermek için sola biraz 50 yazdığımda ve sadece bir örnek daha almak için sola gittiğimde Peki ya ST 4'teysek ve sola gitmeye karar verirsek? Dördüncü aşamadaysanız sola gidersiniz, sıfır ödül alırsınız ve sonra burada sola doğru harekete geçersiniz. Yani sıfır kazanç, burada sol harekete geç, sıfır ve sonra 100. Yani dört Soldan Q, sıfır ödülle sonuçlanır çünkü ilk eylem kaldı ve ardından daha sonra en uygun politikayı izlediğimiz için 00 100 ödüllendirebilirsiniz. Ve böylece dönüş sıfırdır. artı bunun 00,5 katı. Artı 4,5 kare çarpı bunun artı 0,5 Q çarpı bunun. Bu nedenle 12.5'e eşittir. Yani Q4 kaldı 12.5. Bu yılı 12.5 olarak yazacağım. Ve bu alıştırmayı diğer tüm durumlar ve diğer tüm eylemler için yaparsanız, bunun s,a'nın Q'su olduğunu görürsünüz. Farklı durumlar ve farklı eylemler için Ve son olarak Terminal Durumunda. Ne yaptığınız önemli değil, sadece 100 veya 40 terminal ödülü alırsınız. O yüzden terminal ödüllerini buraya yazın. Yani bu Q(s,a)'dır. Birden altıya kadar her durum durumu için ve iki eylem için, eylem sol ve eylem sağ. Durum eylem değeri işlevi neredeyse her zaman Q harfiyle belirtildiği için. Buna genellikle Q işlevi de denir. Dolayısıyla, Q. işlevi ve durum eylem değeri işlevi birbirinin yerine kullanılır ve size getirilerinizin ne olduğunu veya gerçekten değerin ne olduğunu söyler. ne kadar iyi? Sadece A ve ST S eylemlerini gerçekleştirin ve ardından en uygun şekilde davranın. Şimdi, Q işlevini bir kez hesaplayabildiğiniz zaman, bunun size eylemleri seçmek için de bir yol vereceği ortaya çıktı. İşte politika ve iade. Ve işte s,a'nın iki değeri. Önceki slayttan. Farklı durumlara baktığınızda ilginç bir şey fark ediyorsunuz, o da soldaki eylemi yapmak için durum alırsanız a,q ile sonuçlanır. Değer veya durum eylem değeri 50'dir ki bu aslında o durumdan alabileceğiniz en olası getiridir. Üç iki s,a durumunda. çünkü soldaki işlem aynı zamanda size daha yüksek getiri sağlar, bu nedenle soldaki işlem size istediğiniz getiriyi verir. Ve beşinci durumda, aslında size 20'lik daha yüksek getiriyi sağlayan, sağa giden eylemdir. Böylece, herhangi bir S durumundan mümkün olan en iyi getiri olduğu ortaya çıktı. Q,F, S'nin en büyük değeridir. Bunun net olduğundan emin olmak için söylediğim şu ki, kal için kal Dört durumundan ikisi kaldı ki bu da 12.5 Ve q. Bu da 10 olur. Ve bu iki değerden büyük olan 12.5, bu durumdaki dörtten mümkün olan en iyi getiridir. Başka bir deyişle, Dördüncü Durumdan almayı umabileceğiniz en yüksek getiri 12,5'tir. Ve aslında bu iki sayıdan 12,5 ve 10'dan daha büyük olanıdır. Ayrıca, Mars Rover'ınızın 10 demek yerine 12,5'lik bir dönüşün keyfini çıkarmasını istiyorsanız, yapmanız gereken eylem A eylemidir. Q s,a. Yani mümkün olan en iyi eylem durumu, eylem A'dır. Bu aslında Q, of s,a'yı maksimize eder. Bu size Q, of s,a'nın neden hesaplandığına dair bir ipucu verebilir. Daha sonra inşa edilecek pekiştirmeli öğrenme algoritmasının önemli bir parçasıdır. Yani Q(s,a)'yı hesaplamak için bir yolunuz varsa. Her durum ve her eylem için, bazı durumlardayken tek yapmanız gereken farklı A eylemlerine bakmaktır. Ve A eylemini seçin. Bu, Q of s,a'yı maksimize eder. Ve böylece pi F. S sadece A eylemini seçebilir. Bu, s,a'nın en büyük Q değerini verir. Ve bu iyi bir eylem olacak. Aslında en uygun eylem olduğu ortaya çıktı. Bunun neden mantıklı olduğuna dair bir başka sezgi de Qof s,a'dır. Eğer ani bir durum ve eyleme geçerseniz döndürülür A. Ve bundan sonra en uygun şekilde davranın. Yani mümkün olan en yüksek getiriyi elde etmek için, gerçekten istediğiniz şey A eylemini gerçekleştirmektir. Bu, en yüksek toplam getiriyle sonuçlanır. Bu nedenle, keşke Q f s,a'yı hesaplamanın bir yolu olsa. Bu koşullar altında getiriyi en üst düzeye çıkaran eylem yardımını alan her devlet için, o eyalette yapılacak en iyi eylem gibi görünüyor. Bu, bunun için bilmeniz gereken bir şey olmasa da. Çünkü şunu da belirtmek isterim ki internete bakarsanız veya pekiştirmeli öğrenme literatürüne bakarsanız bazen bu Q fonksiyonunun Q olarak yazıldığını görürsünüz. Bu terimler, tam olarak tanımladığımız gibi Q işlevine atıfta bulunur. Pekiştirmeli öğrenme literatürüne bakarsanız ve Q. Star veya Q fonksiyonu hakkında okursanız, bu sadece bahsettiğimiz durum eylem değeri fonksiyonu anlamına gelir. Ancak bu kursun amaçları açısından bunun için endişelenmenize gerek yok. Özetlemek gerekirse, Q of s,a'yı hesaplayabilirseniz. Her durum ve her eylem için, bu bize S'nin oto politikası pi'yi hesaplamak için iyi bir yol verir. Yani bu, durum eylem değeri işlevi veya Q işlevidir. Q fonksiyonunun tanımının biraz dairesel yönüne rağmen bunları hesaplamak için nasıl bir algoritma bulacağımız hakkında daha sonra konuşacağız. Ama önce bir sonraki videoda Q of s,a değerlerinin ne olduğuna dair bazı özel örneklere bakalım. aslında benziyor


## State-action value function example
Durum-eylem değeri işlevi örneğini kullanma. QSA değerlerinin nasıl olduğunu görüyorsunuz. Takviyeli öğrenme problemleri ve probleme bağlı olarak QSA değerlerinin nasıl değiştiği konusundaki sezgilerimizi sürdürmek için isteğe bağlı bir laboratuvar sağlanacaktır. Bu, [DUYULMUYOR] örneğini değiştirerek oynamanıza ve QSA'nın nasıl değişeceğini kendiniz görmenize olanak tanır. Hadi bir bakalım. İşte bu videoyu izledikten sonra oynamanızı umduğum bir Jupyter Notebook. Bu yardımcı işlevleri çalıştıracağım, şimdi burada dikkat edin, bu iki eylemin altı sayısını belirtiyor, böylece bunları değiştirmeyeceğim. Ve bu, 140 olan terminal sağ ödüllerinde sol terminali belirtir ve daha sonra ara durumların ödülleri sıfırdır. İndirim faktörü kumar 0.5. Şimdilik yanlış adım atma olasılığını göz ardı edelim, bundan sonraki bir videoda bahsedeceğiz. Ve bu değerlerle, bu kodu çalıştırırsanız bu, optimal politikanın yanı sıra SA'nın Q işlevini hesaplar ve görselleştirir. Q of SA'yı kendiniz tahmin etmek veya hesaplamak için bir öğrenme algoritmasının nasıl geliştirileceğini daha sonra öğreneceksiniz. Şimdilik Q of SA'yı hesaplamak için hangi kodu yazdığımızı dert etmeyin. Ama burada Q of SA değerlerinin derste gördüğümüz değerler olduğunu görüyorsunuz. Şimdi eğlence burada başlıyor. Bazı değerleri değiştirelim ve bu şeylerin nasıl değiştiğini görelim. Terminal sağ ödülünü çok daha küçük bir değere güncelleyeceğim sadece 10 diyor. Şimdi kodu tekrar çalıştırırsam, Q of SA'nın nasıl değiştiğine bakın ve şimdi 5 durumundaysanız bunu düşünür. ve en uygun şekilde davranın, 6.25 alırsınız. Oysa sağa giderseniz ve bundan sonra da davranırsanız, yalnızca beşlik bir getiri elde edersiniz. Şimdi sağdaki ödül çok küçükken, sadece 10. Size çok yakın olsanız bile, sonuna kadar sola gitmeyi tercih edin. Ve aslında otomobil politikası artık her eyaletten sola gitmek. Başka değişiklikler yapalım. Terminal hakkı ödülünü tekrar 40 olarak değiştireceğim. Ama indirim faktörünü bire yakın bir indirim faktörü ile 0.9 olarak değiştireyim. Bu, Mars Gezgini'ni daha az sabırsız yapar, daha yüksek bir ödül için daha uzun süre dayanmaya isteklidir çünkü gelecekte ödüller 0,5 ile çarpılmaz - bazı yüksek güçler 0,9 ile bazı yüksek güçlerle çarpılır. Ve daha fazla sabırlı olmaya isteklidir, çünkü gelecekte ödüller iskonto edilmez veya indirim 0,5 iken olduğu kadar küçük bir sayı ile çarpılmaz. Öyleyse kodu tekrar çalıştıralım. Ve şimdi bunun farklı eyaletler için Q of SA olduğunu görüyorsunuz ve şimdi durum 5 için sola gitmek size 36'ya kıyasla 65,61 gibi daha yüksek bir ödül veriyor. Bu arada 36'nın 40'lık bu terminal ödülünün 0,9 katı olduğuna dikkat edin. mantıklı olmak. Ama küçük bir hasta sola gitmeye istekli olduğunda, siz 5 durumundayken bile. Şimdi gammayı 0 .3 gibi çok daha küçük bir sayıya değiştirelim. Yani bu, gelecekte ödüllerde çok ağır indirimler yapar. Bu onu inanılmaz derecede sabırsız yapar. Bu kodu tekrar çalıştırmama izin verin ve şimdi davranış değişti. Şu anda 4. durumda olanın daha büyük 100 ödül için gidecek sabrı olmayacağı fark edildi, çünkü indirim faktörü gamma artık çok küçük, 0,3. Çok daha küçük bir ödül daha yakın olmasına rağmen 40'lık ödülü tercih eder ve biz de bunu yapmayı seçeriz. Umarım bu sayılarla kendiniz oynayarak ve bu kodu çalıştırarak bir fikir edinebilirsiniz. Q of SA değerleri nasıl değişir, fark ettiğiniz optimal getiri bu iki QSA sayısından daha büyük olanıdır. Bunun nasıl değiştiği ve optimal politikanın nasıl değiştiği. Bu yüzden umarım gidip isteğe bağlı laboratuvarla oynarsınız ve ödül işlevini değiştirirsiniz ve indirim faktörü gama'yı değiştirir ve farklı değerler denersiniz. Ve Q of SA değerlerinin nasıl değiştiğini, farklı durumlardan optimal getirilerin nasıl değiştiğini ve bu farklı değerlere bağlı olarak otomobil politikasının nasıl değiştiğini kendiniz görün. Ve bunu yaparak, pekiştirmeli öğrenme uygulamasında ödüllere bağlı olarak bu farklı miktarların nasıl etkilendiğine dair sezginizi keskinleştireceğinizi umuyorum. Siz laboratuvarda oynadıktan sonra, geri gelip takviyeli öğrenmede muhtemelen en önemli denklemin ne olduğu hakkında konuşmaya hazır olacağız, buna bellman denklemi denir. Umarım isteğe bağlı laboratuvarda eğlenirsiniz ve ondan sonra hadi geri gelip kapıcı denklemleri hakkında konuşalım.

