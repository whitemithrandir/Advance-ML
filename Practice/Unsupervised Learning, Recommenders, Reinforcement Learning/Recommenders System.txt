## Making recommendations

## Using per-item features

## Collaborative filtering algorithm

## Binary labels: favs, likes and clicks

## TensorFlow implementation of collaborative filtering
Neden bu şekilde yapmak zorunda olduğumuzu merak ediyorsanız? Neden yoğun bir katman ve ardından model derleyici ve model uyumu kullanamadık? Bu eski tarifi kullanamamamızın nedeni, teminat filtreleme algoritması ve maliyet işlevinin yoğun katmana veya TensorFlow'un diğer standart sinir ağı katmanı türlerine tam olarak uymamasıdır. Bu nedenle, maliyet fonksiyonunu kendimiz uygulayacağımız başka bir şekilde uygulamak zorunda kaldık. Ancak daha sonra, Otomatik Fark olarak da adlandırılan otomatik farklılaştırma için TensorFlow'un araçlarını kullanın. Ve maliyet işlevini optimize etme işinin çoğunu bizim için yapmasına izin vermek için TensorFlow'un adam optimizasyon algoritması uygulamasını kullanın. Sahip olduğunuz model, yoğun sinir ağı katmanları dizisi veya TensorFlow tarafından desteklenen diğer katman türleriyse ve model bileşik model uyumunun eski uygulama tarifi işe yarıyorsa. Ancak böyle olmadığında bile, TensoFlow'un bu araçları size diğer öğrenme algoritmalarını da uygulamanız için çok etkili bir yol sunar.

## Finding related items
işbirlikçi filtrelemenin birkaç sınırlamasından bahsetmek istiyorum. İşbirlikçi filtrelemede, bir dizi öğeniz vardır ve bu nedenle kullanıcılar ve kullanıcılar bazı öğe alt kümelerini derecelendirmiştir. Bu zayıflıklardan biri, soğuk çalıştırma probleminde pek iyi olmamasıdır. Örneğin, kataloğunuzda yeni bir ürün varsa, diyelim ki birisi yeni bir film yayınladı ve bu filme neredeyse hiç kimse oy vermedi, daha önce çok az kullanıcı oy verdiyse, yeni öğeyi nasıl sıralarsınız? Benzer şekilde, yalnızca birkaç öğeyi değerlendiren yeni kullanıcılar için onlara makul bir şey gösterdiğimizden nasıl emin olabiliriz? Daha önceki bir videoda, normalleşmenin bu konuda nasıl yardımcı olabileceğini ve çok yardımcı olduğunu görebilirdik. Ancak, çok az öğeyi derecelendiren kullanıcılara ilgilerini çekebilecek şeyleri göstermenin daha da iyi yolları olabilir. Buna soğuk başlatma sorunu denir, çünkü yeni bir öğeniz olduğunda, derecelendiren çok az kullanıcı vardır veya çok az öğeye oy veren yeni bir kullanıcımız olduğunda, o öğe veya o kullanıcı için işbirlikçi filtrelemenin sonuçları farklı olabilir.

İşbirlikçi bir filtreleme öneri sisteminin amacı iki vektör oluşturmaktır: Her kullanıcı için, bir kullanıcının film zevklerini somutlaştıran bir 'parametre vektörü'. Her film için, filmin bazı açıklamalarını içeren aynı boyutta bir özellik vektörü. İki vektörün iç çarpımı artı yanlılık terimi, kullanıcının o filme verebileceği derecelendirmenin bir tahminini üretmelidir.

## Collaborative filtering vs Content-based filtering
İşbirlikçi filtrelemede genel yaklaşım, sizinle benzer puanlar veren kullanıcıların puanlarına göre size öğeler önermemizdir. Bazı öğeler için bazı derecelendirmeler veren bazı kullanıcılarımız var ve algoritma, size yeni öğeler önermek için bunu nasıl kullanacağını buluyor. Bunun tersine, içerik tabanlı filtreleme, size ne önerileceğine karar vermede farklı bir yaklaşım benimser. İçerik tabanlı bir filtreleme algoritması, iyi bir eşleşme bulmak için kullanıcıların özelliklerine ve öğelerin özelliklerine göre size öğeler önerecektir. Başka bir deyişle, her bir öğenin bazı özelliklerinin yanı sıra her kullanıcının bazı özelliklerine sahip olmayı gerektirir ve bu özellikleri, hangi öğelerin ve kullanıcıların birbirleri için iyi bir eşleşme olabileceğine karar vermeye çalışmak için kullanır.

## Deep learning for content-based filtering
İçerik tabanlı bir filtreleme algoritması oluşturmak için derin öğrenmeyi bu şekilde kullanabilirsiniz. Karar ağaçlarından ve sinir ağlarına karşı karar ağaçlarının artıları ve eksilerinden bahsettiğimizi hatırlarsınız. Sinir ağlarının faydalarından birinin, daha büyük bir sistem oluşturmak için birden fazla sinir ağını konsolda çalıştıracak şekilde bir araya getirmenin daha kolay olduğundan bahsetmiştim. Az önce gördüğünüz şey aslında bunun bir örneğiydi, bir kullanıcı ağı ile film ağını alıp bir araya getirebilir ve ardından çıktıların iç çarpımını alabilirdik. İki sinir ağını bir araya getirme yeteneği, oldukça güçlü olduğu ortaya çıkan daha karmaşık bir mimariyi bu şekilde bulmayı başardık. Bu algoritmaları pratikte uyguluyorsanız, geliştiricilerin genellikle bu içerik tabanlı filtreleme algoritmalarını beslemek için gereken özellikleri dikkatli bir şekilde tasarlamak için çok fazla zaman harcadıklarını görüyorum. Sonunda bu sistemlerden birini ticari olarak kurarsak, bu uygulama için de iyi özellikler tasarlamak için biraz zaman ayırmaya değer olabilir. Bu uygulamalar açısından, algoritmanın açıkladığımız gibi bir sınırlaması, önermek isteyebileceğiniz birçok farklı filmden oluşan geniş bir kataloğunuz varsa çalıştırmanın hesaplama açısından çok pahalı olabilmesidir. Bir sonraki videoda, bazı pratik konulara ve çok büyük ürün kataloglarında bile çalışan bir ölçek oluşturmak için bu algoritmayı nasıl değiştirebileceğinize bir göz atalım. 

## Recommending from a large catalogue
Günümüzün tavsiye edilen sistemlerinin bazen önermek için bir avuç öğe seçmesi gerekebilir. Binlerce veya milyonlarca veya 10 milyonlarca veya daha fazla öğeden oluşan bir katalogdan. Bunu hesaplamalı olarak verimli bir şekilde nasıl yaparsınız, bir göz atalım. İşte ağınızda, bir kullanıcının bir öğeyi nasıl derecelendirebileceği hakkında tahminlerde bulunmak için kullanıyoruz. Bugün büyük bir film akış sitesinde binlerce film veya hangi reklamın gösterileceğine karar vermeye çalışan bir sistem olabilir. Aralarından seçim yapabileceğiniz milyonlarca reklamdan oluşan bir kataloğa sahip olabilir. Veya bir müzik akışı sitesinde, aralarından seçim yapabileceğiniz 10 milyonlarca şarkı olabilir. Ve büyük çevrimiçi alışveriş sitelerinde, aralarından seçim yapabileceğiniz milyonlarca hatta 10 milyonlarca ürün olabilir. Bir kullanıcı web sitenizde göründüğünde, bazı Xu özelliklerine sahiptir. Ancak, üründe hesaplama yapmak için bu sinir ağı aracılığıyla beslenmek için binlerce milyonlarca öğe almanız gerekiyorsa. Hangi ürünleri önermeniz gerektiğini bulmak için sinir ağı çıkarımı yapmak zorunda. Bir kullanıcı web sitenizde her göründüğünde binlerce milyonlarca kez hesaplamalı ve uygulanamaz hale gelir. Birçok hukuk ölçeğinde önerilen sistem, geri alma ve sıralama adımları olarak adlandırılan iki adım olarak uygulanmaktadır. Buradaki fikir, alma adımı sırasında, makul öğe adaylarının geniş bir listesini oluşturacaktır. Bu, kullanıcıya önerebileceğiniz pek çok olası şeyi kapsamaya çalışır ve alma adımı sırasında sorun olmaz. Kullanıcının muhtemelen beğenmeyeceği pek çok öğe eklerseniz ve ardından sıralama adımı sırasında kullanıcıya önerilecek en iyi öğeleri ince ayar yapacak ve seçecektir. İşte bir örnek, alma adımı sırasında şöyle bir şey yapabiliriz. Kullanıcının izlediği son 10 filmin her biri için en benzer 10 filmi bulun. Yani bu, örneğin bir kullanıcı vektör VIM ile I filmini izlemişse buna benzer vektör VKM ile hey filmleri bulabilirsiniz anlamına gelir. Ve benzer filmleri bulmadaki son videoda gördüğünüz gibi, verilen film önceden hesaplanabilir. Bir film vermek için en benzer filmleri önceden hesapladıktan sonra, bir arama tablosunu kullanarak sonuçları alabilirsiniz. 
Bu size, web sitenizde henüz ortaya çıkan, kullanıcıya önermek için belki biraz makul filmlerden oluşan bir başlangıç seti verecektir. Ek olarak, kullanıcının en çok görüntülenen üç türü için eklemeye karar verebilirsiniz. Kullanıcının çok sayıda aşk filmi, çok sayıda komedi filmi ve çok sayıda tarihi drama izlediğini varsayalım. Ardından, bu üç türün her birindeki en iyi 10 filmi olası öğe adayları listesine eklerdik. Ve sonra belki de bu listeye kullanıcının ülkesindeki en iyi 20 filmi de ekleyeceğiz. Dolayısıyla, bu geri alma adımı çok hızlı bir şekilde yapılabilir ve sonunda 100 veya belki de 100'lerce makul filmden oluşan bir liste elde edebilirsiniz. Kullanıcıya tavsiye etmek ve umarım bu liste bazı iyi seçenekler önerecektir. Ancak kullanıcının hiç hoşlanmayacağı bazı seçenekler içeriyorsa da sorun değil. Geri alma adımının amacı, yeterli sayıda filmin en azından içinde çok sayıda iyi filmin olması için geniş kapsama alanı sağlamaktır. Son olarak, alma adımı sırasında aldığımız tüm öğeleri alır ve bunları bir liste halinde birleştiririz. İki pişiriciyi kaldırmak ve kullanıcının daha önce yıkadığı veya daha önce satın aldığı ve ona tekrar önermek istemeyebileceğiniz öğeleri kaldırmak. Bunun ikinci adımı, sıralama adımıdır. Sıralama adımı sırasında, geri alma adımı sırasında alınan listeyi alacaksınız. Yani bu sadece yüzlerce olası film olabilir ve bunları öğrenilen modeli kullanarak sıralayabilir. Bunun anlamı, kullanıcı özellik vektörünü ve film aktörünü bu sinir ağına besleyeceğinizdir. Ve kullanıcı film çiftlerinin her biri için tahmin edilen derecelendirmeyi hesaplayın. Ve buna dayanarak, artık se 100'den fazla filme sahipsiniz, kullanıcının yüksek puan vermesi en muhtemel olan filmler. Ve sonra, kullanıcının ne vereceğini düşündüğünüze bağlı olarak, kullanıcıya öğelerin sıralama listesini görüntüleyebilirsiniz. Ek bir optimizasyon için en yüksek puan, hesaplanmış VM'niz varsadır. Önceden tüm filmler için, VU'yu hesaplamak için tek yapmanız gereken sinir ağının bu bölümünde tek seferde çıkarım yapmaktır. Ardından, web sitenizdeki kullanıcı için az önce hesapladıkları VU'yu hemen şimdi alın. Ve VU ile VM arasındaki iç çarpımı alın. Alma adımı sırasında aldığınız filmler için. Yani bu hesaplama nispeten hızlı bir şekilde yapılabilir. Alma adımı sadece 100'lerce filmi getiriyorsa, bu albüm için vermeniz gereken kararlardan biri, alma adımı sırasında kaç tane öğe almak istediğinizdir? Daha doğru sıralama adımını beslemek için. Alma adımı sırasında, daha fazla öğenin alınması daha iyi performansla sonuçlanma eğiliminde olacaktır. Ancak algoritma, 100 veya 500 veya 1000 öğeyi almak için kaç öğenin alınacağı arasındaki değiş tokuşu analiz etmek veya optimize etmek için daha yavaş olacaktır. Ek öğelerin ne kadar geri alındığını görmek için çevrimdışı deneyler yapmanızı tavsiye ederim. Ne kadar ek öğe almanın daha alakalı önerilerle sonuçlandığını görmek için çevrimdışı deneyler yapmanızı öneririm. Ve özellikle, tahmin edilen olasılık ise YIJ. Sinir ağı modelinize göre bire eşittir. Ya da modelinizin tahminine göre alınan öğelerin Y'nin tahmini derecesinin yüksek olması, çok daha yüksek çıkıyorsa. Sadece 100 öğe yerine 500 öğe alacak olsaydınız, bu belki daha fazla öğe almak anlamına gelirdi. Albümü biraz yavaşlatsa da. Ancak ayrı alma adımı ve sıralama adımı ile bu, günümüzde önerilen birçok sistemin hem hızlı hem de doğru sonuçlar vermesini sağlar. Çünkü alma adımı, üzerinde daha ayrıntılı etki ve iç ürün yapmaya değmeyecek birçok öğeyi budamaya çalışır. Ve sonra, sıralama adımı, kullanıcının gerçekten zevk alması muhtemel öğelerin neler olduğu konusunda daha dikkatli bir tahmin yapar, bu yüzden bu kadar. Önerilen sisteminizin çok büyük film veya ürün kataloglarında bile verimli çalışmasını bu şekilde sağlarsınız. Şimdi, önerilen sistemlerimiz kadar ticari açıdan da önemli olduğu, bunlarla ilgili bazı önemli etik sorunların da olduğu ortaya çıktı. Ve ne yazık ki zarar yaratan tavsiye edilen sistemler olmuştur. Kendi önerilen sisteminizi oluştururken, umarım etik bir yaklaşım benimser ve kullanıcılarınıza hizmet etmek için kullanırsınız. Ve sizin ve çalıştığınız şirket kadar büyük bir toplum. Bir sonraki videoda önerilen sistemlerle ilgili etik sorunlara bir göz atalım

## TensorFlow implementation of content-based filtering
Uygulama laboratuvarında, içerik tabanlı filtrelemeyi TensorFlow'da nasıl uygulayacağınızı göreceksiniz. Bu videoda yapmak istediğim şey, sizin sayenizde oynayabileceğiniz koddaki birkaç temel kavramdır. Hadi bir bakalım. Kodumuzun bir kullanıcı ağı ve işe yarayan bir filmle başladığını hatırlayın. Bunu TensorFlow'da uygulama şekliniz, daha önce bir dizi yoğun katman içeren bir sinir ağını uygulamamıza çok benzer. Sıralı bir model kullanacağız. Daha sonra bu örnekte, burada belirtilen gizli birim sayısına sahip iki yoğun katmanımız var ve son katmanda 32 birim ve çıktının 32 sayısı var. Sonra film ağı için, ona öğe ağı diyeceğim, çünkü filmler buradaki öğeler, kod böyle görünüyor. Bir kez daha, yoğun gizli katmanları birleştirdik, ardından 32 sayı veren bu katmanı izledik. Gizli katmanlar için, relu aktivasyon fonksiyonu olan varsayılan aktivasyon fonksiyonu seçimimizi kullanacağız. Ardından, TensorFlow Keras'a kullanıcı özelliklerini veya öğe özelliklerini, yani film özelliklerini iki sinir ağına nasıl besleyeceğini söylememiz gerekiyor. Bunu yapmanın sözdizimi budur. Bu, kullanıcı için giriş özelliklerini çıkarır ve ardından onu kullanıcıya iletir ve burada kullanıcı için vektör olan vu'yu hesaplamak için tanımladığımız şeyi verir. Sonra, bu algoritmanın biraz daha iyi çalışmasını sağlayan ek bir adım, buradaki vektör vu'yu bir uzunluğa sahip olacak şekilde normalleştiren bu satırdadır. Bu, l2 normu olarak da adlandırılan uzunluğu normalleştirir, ancak temel olarak vu vektörünün uzunluğu bire eşit olur. Sonra aynı şeyi item network için, movie network için yapıyoruz. Bu, öğe özelliklerini çıkarır ve onu yukarıda tanımladığımız öğe sinir ağına besler. Bu, film vektörü vm'yi hesaplar. Son olarak, adım aynı zamanda bu vektörü bir uzunluğa sahip olacak şekilde normalleştirir. vu ve vm'yi hesapladıktan sonra, bu iki vektör arasındaki iç çarpımı almalıyız. Bunu yapmanın sözdizimi budur. Keras'ın özel bir katman türü vardır, burada yoğun keras katmanları olduğuna dikkat edin, burada tf keras katmanları nokta. Görünüşe göre özel bir Keras katmanı var, sadece iki sayı arasında bir iç çarpım alıyorlar. Bunu, vu ve vm vektörleri arasındaki iç çarpımı almak için kullanacağız. Bu, sinir ağının çıktısını verir. Bu son tahmini verir. Son olarak, keras'a modelin girdilerinin ve çıktılarının ne olduğunu söylemek için, bu satır ona genel modelin girdileri kullanıcı özellikleri ve film veya öğe özellikleri ve çıktı olan bir model olduğunu söyler, bu az önce tanımladığımız çıktıdır. yukarıda. Bu modeli eğitmek için kullanacağımız maliyet fonksiyonu, ortalama karesel hata maliyet fonksiyonu olacaktır. Bunlar, içerik tabanlı filtrelemeyi bir sinir ağı olarak uygulamak için anahtar kod parçacıklarıdır. Uygulama laboratuvarında kodun geri kalanını görüyorsunuz, ancak umarız bununla oynayabilir ve tüm bu kod parçacıklarının içerik tabanlı bir filtreleme algoritmasının çalışan TensorFlow uygulamasına nasıl uyduğunu görebilirsiniz. Görünüşe göre daha önce bahsetmediğim bir adım daha var ama bunu yaparsanız, ki bu da vektör vu'nun uzunluğunu normalleştirir, bu algoritmanın biraz daha iyi çalışmasını sağlar. TensorFlows, vektörü normalleştiren bu l2 normalleştirilmiş harekete sahiptir, buna vektörün l2 normunu normalleştirme de denir, dolayısıyla fonksiyonun adıdır. Bu kadar. Tavsiye sistemleriyle ilgili tüm bu materyal boyunca bana bağlı kaldığınız için teşekkürler, bu heyecan verici bir teknoloji. Umarım bu haftaki uygulama laboratuvarlarında bu fikirler ve kodlarla oynamaktan keyif alırsınız. Bu da bizi tavsiye sistemlerindeki bu videoların çoğuna ve bu uzmanlık için önümüzdeki haftadan son haftaya kadar götürüyor. Ben de haftaya seni görmeyi dört gözle bekliyorum. Takviyeli öğrenmenin heyecan verici teknolojisi hakkında konuşacağız. Kısa sınavlarda ve uygulama laboratuvarlarında eğlenmenizi umuyorum ve gelecek hafta sizi görmeyi dört gözle bekliyorum.


