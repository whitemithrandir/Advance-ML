Eğer bir ikili sınıflandırma problemi üzerinde çalışılıyorsa, çıktı katmanında sigmoid kullanılır.

Örneğin, yarının hisse senedi fiyatının bugünün hisse senedi fiyatına kıyasla nasıl değişeceğini tahmin etmeye çalışıyorsanız.
Pekala, yukarı veya aşağı gidebilir ve bu durumda y pozitif veya negatif olabilen bir sayı olur ve bu durumda
doğrusal aktivasyon fonksiyonunu kullanmanızı tavsiye ederim.

örneğin bir evin fiyatını tahmin ediyorsanız, bu asla negatif olamaz, o zaman en doğal seçim ReLU aktivasyon fonksiyonu
olacaktır çünkü burada gördüğünüz gibi, bu etkinleştirme işlevi yalnızca sıfır veya pozitif değerler olmak üzere negatif
olmayan değerleri alır

Gradyan iniş, makine öğreniminde yaygın olarak kullanılan bir optimizasyon algoritmasıdır ve doğrusal regresyon ve
lojistik regresyon
ve sinir ağlarının erken uygulamaları gibi birçok algoritmanın temeli olmuştur.

Adam optimizasyon algoritması tipik olarak eğimli inişten çok daha hızlı çalışır ve uygulayıcıların sinir ağlarını nasıl
eğittiği
konusunda fiili bir standart halindedir. Hangi öğrenme algoritmasının kullanılacağına, sinir ağınızı eğitmek için hangi
optimizasyon
algoritmasının kullanılacağına karar vermeye çalışılıyorsa. Güvenli bir seçim sadece Adam optimizasyon algoritmasını
kullanmak olacaktır.

##Evaluating a model
makine öğrenimi sistemlerinin nasıl oluşturulacağına dair bazı tavsiyeler;
ekiplerin kelimenin tam anlamıyla aylarca daha fazla eğitim örneği toplayarak, daha fazla eğitim verisinin yardımcı
olacağını
düşünerek harcadıklarını gördüm, ancak bazen çok yardımcı olduğu ve bazen de yaramadığı ortaya çıktı
Bir makine öğrenimi algoritması oluşturmada etkili olmanın anahtarı, zamanınızı nereye harcayacağınız konusunda iyi
seçimler yapmaktır.
Bilgisayar için genellikle yararlı olan niceliklerden biri ve eğitim hatası, albümünüzün eğitim setinde ne kadar
iyi çalıştığının
bir ölçüsüdür.

## Diagnosing bias and variance
Makine öğrenimi sistemi oluşturma sürecinin anahtarı, performansını artırmak için bir sonraki adımda ne yapılacağına
nasıl karar
verileceğidir. Bir öğrenme algoritmasının önyargı ve varyansına bakmanın, bir sonraki denemeniz konusunda size çok iyi
rehberlik sağlar.
algoritmanızın yüksek önyargı veya yüksek varyansa sahip olup olmadığını teşhis etmenin veya bulmanın sistematik bir yolu,
algoritmanızın eğitim setindeki ve çapraz doğrulama setindeki performansına bakmak olacaktır
Yüksek varyansın temel göstergesi, J_cv'nin J treninden çok daha büyük olması durumunda olacak olsa da,
matematikte işaretten iki kat daha büyüktür, bu nedenle bu daha büyüktür ve bu çok daha büyük anlamına gelir.
Çizimin bu en sağdaki kısmı, J_cv'nin J treninden çok daha büyük olduğu yerdir. Genellikle J katarı oldukça düşük olacaktır,
ancak temel gösterge J_cv'nin J katarından çok daha büyük olup olmadığıdır

## Regularization and bias/variance
algoritmanız için düzenleme parametresinin iyi bir Lambda değerini seçmek istediğinizde yardımcı olacaktır
Burada Lambda değeri, eğitim verilerini iyi uydurmaya karşı w parametrelerini küçük tutmaktan ne kadar ödün verdiğinizi
kontrol eden düzenleme parametresidir
yüksek lambda yüksek bias demektir. Lambda 0 olursa overfit olur. İdeal bir aralıkta seçmek gerekir
Normalleştirme parametresi için kullanılacak iyi bir Lambda değerinin ne olduğuna karar vermeye çalışıyorsanız, çapraz 
doğrulama size bunu yapmanın bir yolunu da sunar
Algoritmanın en iyi performansı göstermesine neden olan bazı ara Lambda değerleri olacaktır. Çapraz doğrulamanın yaptığı şey,
Lambda'nın birçok farklı değerini denemektir
Lambda düzenleme parametresinin seçimi, algoritmanızın önyargısını ve varyansını ve genel performansını etkiler ve ayrıca
Lambda düzenleme parametresi için iyi bir seçim yapmak üzere çapraz doğrulama kullanılabilir.

## Learning curves
Eğitim seti boyutu büyüdükçe, eğitim seti hatası aslında artar.
bir öğrenme algoritmasının yüksek önyargıya sahip olması durumunda, daha fazla eğitim verisi elde etmenin tek başına
o kadar fazla umut vermeyeceği sonucuna varıyor, belki biraz şaşırtıcı. Daha fazla veriye sahip olmanın iyi olduğunu
düşünmeye alıştığımızı biliyorum, ancak algoritmanız yüksek önyargıya sahipse, yaptığınız tek şey daha fazla eğitim
verisi eklemekse, bu tek başına hatayı azaltmanıza asla izin vermez. bu kadar değerlendir. Gerçekten bu nedenle,
bu şekle ne kadar çok örnek eklerseniz ekleyin, düz doğrusal uydurma o kadar iyi olmayacak. Bu nedenle, daha fazla
eğitim verisi toplamak için çok çaba harcamadan önce, öğrenme algoritmanızın yüksek önyargıya sahip olup olmadığını
kontrol etmeye değer, çünkü varsa, muhtemelen daha fazla eğitim verisi eklemekten başka şeyler yapmanız gerekir.
Varyansınız yüksek olduğunda, eğitim kümesi boyutunu artırmak çok yardımcı olabilir ve özellikle, bu eğrileri sağa doğru
tahmin edebilirsek, M katını artırabilirsek, eğitim hatası artmaya devam eder, ancak sonra çapraz- doğrulama hatası umarım
aşağı inecek ve J trenine yaklaşacaktır. Dolayısıyla bu senaryoda, çapraz doğrulama hatasını azaltmak ve algoritmanızın
daha iyi ve daha iyi performans göstermesini sağlamak için yalnızca eğitim seti boyutunu artırarak mümkün olabilir ve bu,
yüksek önyargı durumundan farklıdır, burada yaptığınız tek şey Algoritma performansınızı çok fazla öğrenmenize yardımcı
olmayacak daha fazla eğitim verisi elde etmektir. Özetlemek gerekirse, bir öğrenme algoritması yüksek varyanstan muzdaripse,
o zaman daha fazla eğitim verisi elde etmek gerçekten yardımcı olacaktır.

## Deciding what to try next revisited
Öğrenme algoritmanızın yüksek önyargılı mı yoksa yüksek varyanslı mı olduğunu anlamaya çalışabilirsiniz.
Bu, bir öğrenme algoritması eğitirken rutin olarak yaptığım prosedürdür, algoritmamın yüksek önyargı veya
yüksek varyansa sahip olup olmadığına karar vermeye çalışmak için eğitim hatasına ve çapraz doğrulama hatasına daha sık bakarım.

## Deciding what to try next revisited
algoritmanız yüksek yanlılığa sahipse, o zaman yaptığımız tek şey daha fazla eğitim verisi almaksa, bu muhtemelen tek başına
o kadar da yardımcı olmaz. Ancak bunun aksine, algoritmanız yüksek varyansa sahipse, diyelim ki çok küçük bir eğitim setine
fazla uyuyorsa, o zaman daha fazla eğitim örneği almak çok yardımcı olacaktır
öğrenme algoritmanızın çok fazla özelliği varsa, algoritmanıza çok karmaşık modellere uyması için çok fazla esneklik sağlar.
Bu biraz x, x squared, x cubed, x^4, x^5, vb. gibi. Bunlardan sadece birkaçını ortadan kaldırsaydınız, modeliniz
çok karmaşık olmayacak ve bu kadar yüksek varyansa sahip olmayacaktı.
Lambdayı arttırmak overfit(high variance) problemini çözer.
Lambdayı azaltmak bias problemini çözer.
algoritmanızın yüksek varyansa sahip olduğunu tespit ederseniz, bunu düzeltmenin iki ana yolu;
ne daha fazla eğitim verisi alın ne de modelinizi basitleştirin. Modeli basitleştirerek demek istediğim,
ya daha küçük bir dizi özellik elde edin ya da Lambda düzenleme parametresini artırın. Algoritmanızın çok karmaşık,
çok hareketli eğrilere uyma esnekliği daha azdır. Tersine, algoritmanız yüksek önyargıya sahipse,
bu eğitim setinde bile iyi gitmiyor demektir. Durum buysa, ana düzeltmeler, modelinizi daha güçlü hale getirmek
veya onlara daha karmaşık veya daha fazla işleve uymaları için daha fazla esneklik kazandırmaktır.
Bunu yapmanın bazı yolları, ona ek özellikler vermek veya bu polinom özelliklerini eklemek veya
Lambda düzenleme parametresini azaltmaktır.
eğitim seti boyutunu küçülterek yüksek önyargıyı düzeltmeniz gerekip gerekmediğini merak ediyorsanız,
bu aslında yardımcı olmuyor. Eğitim seti boyutunu küçültürseniz, eğitim setine daha iyi uyarsınız, ancak bu,
çapraz doğrulama hatanızı ve öğrenme algoritmanızın performansını kötüleştirme eğilimindedir, bu nedenle,
yalnızca yüksek bir değeri düzeltmeye çalışmak için eğitim örneklerini rastgele atmayın

## Bias/variance and neural networks
eğer sinir ağınızı yeterince genişletirseniz, neredeyse her zaman eğitim setinize iyi uyum sağlayabilirsiniz.
Eğitim setiniz çok büyük olmadığı sürece. Ve bunun anlamı, ikisi arasında gerçekten değiş tokuş yapmaya gerek kalmadan
önyargıyı azaltmaya veya varyansı gerektiği gibi azaltmaya çalışmak için bize yeni bir reçete veriyor.
önyargıyı azaltmanın bir yolu, sadece daha büyük bir sinir ağı kullanmaktır ve daha büyük sinir ağı ile,
katman başına daha fazla gizli katman veya daha fazla gizli birim demek istiyorum. Ve daha sonra bu döngüden geçmeye devam edebilir
ve sinir ağınızı eğitim setinde başarılı olana kadar daha da büyütebilirsiniz
Eğitim setine düştükten sonra, bu sorunun cevabı evet. Daha sonra, trans doğrulama setinde iyi sonuç vermediğini sorarsınız.
Tekrar eğittikten sonra;
algoritmanın yüksek varyansa sahip olduğu sonucuna varabilirsiniz çünkü çapraz doğrulama setinde set eğitmek istemiyor.
Dolayısıyla, Jcv ve Jtrain'deki bu büyük boşluk, muhtemelen yüksek bir varyans sorununuz olduğunu gösterir ve
yüksek bir varyans sorununuz varsa, bunu düzeltmenin bir yolu daha fazla veri elde etmektir.
Daha fazla veri almak ve geri dönüp modeli yeniden eğitmek ve iki kez kontrol etmek

## Iterative loop of ML development