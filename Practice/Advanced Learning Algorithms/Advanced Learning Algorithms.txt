Eğer bir ikili sınıflandırma problemi üzerinde çalışılıyorsa, çıktı katmanında sigmoid kullanılır.

Örneğin, yarının hisse senedi fiyatının bugünün hisse senedi fiyatına kıyasla nasıl değişeceğini tahmin etmeye çalışıyorsanız.
Pekala, yukarı veya aşağı gidebilir ve bu durumda y pozitif veya negatif olabilen bir sayı olur ve bu durumda
doğrusal aktivasyon fonksiyonunu kullanmanızı tavsiye ederim.

örneğin bir evin fiyatını tahmin ediyorsanız, bu asla negatif olamaz, o zaman en doğal seçim ReLU aktivasyon fonksiyonu
olacaktır çünkü burada gördüğünüz gibi, bu etkinleştirme işlevi yalnızca sıfır veya pozitif değerler olmak üzere negatif
olmayan değerleri alır

Gradyan iniş, makine öğreniminde yaygın olarak kullanılan bir optimizasyon algoritmasıdır ve doğrusal regresyon ve
lojistik regresyon
ve sinir ağlarının erken uygulamaları gibi birçok algoritmanın temeli olmuştur.

Adam optimizasyon algoritması tipik olarak eğimli inişten çok daha hızlı çalışır ve uygulayıcıların sinir ağlarını nasıl
eğittiği
konusunda fiili bir standart halindedir. Hangi öğrenme algoritmasının kullanılacağına, sinir ağınızı eğitmek için hangi
optimizasyon
algoritmasının kullanılacağına karar vermeye çalışılıyorsa. Güvenli bir seçim sadece Adam optimizasyon algoritmasını
kullanmak olacaktır.

##Evaluating a model
makine öğrenimi sistemlerinin nasıl oluşturulacağına dair bazı tavsiyeler;
ekiplerin kelimenin tam anlamıyla aylarca daha fazla eğitim örneği toplayarak, daha fazla eğitim verisinin yardımcı
olacağını
düşünerek harcadıklarını gördüm, ancak bazen çok yardımcı olduğu ve bazen de yaramadığı ortaya çıktı
Bir makine öğrenimi algoritması oluşturmada etkili olmanın anahtarı, zamanınızı nereye harcayacağınız konusunda iyi
seçimler yapmaktır.
Bilgisayar için genellikle yararlı olan niceliklerden biri ve eğitim hatası, albümünüzün eğitim setinde ne kadar
iyi çalıştığının
bir ölçüsüdür.

## Diagnosing bias and variance
Makine öğrenimi sistemi oluşturma sürecinin anahtarı, performansını artırmak için bir sonraki adımda ne yapılacağına
nasıl karar
verileceğidir. Bir öğrenme algoritmasının önyargı ve varyansına bakmanın, bir sonraki denemeniz konusunda size çok iyi
rehberlik sağlar.
algoritmanızın yüksek önyargı veya yüksek varyansa sahip olup olmadığını teşhis etmenin veya bulmanın sistematik bir yolu,
algoritmanızın eğitim setindeki ve çapraz doğrulama setindeki performansına bakmak olacaktır
Yüksek varyansın temel göstergesi, J_cv'nin J treninden çok daha büyük olması durumunda olacak olsa da,
matematikte işaretten iki kat daha büyüktür, bu nedenle bu daha büyüktür ve bu çok daha büyük anlamına gelir.
Çizimin bu en sağdaki kısmı, J_cv'nin J treninden çok daha büyük olduğu yerdir. Genellikle J katarı oldukça düşük olacaktır,
ancak temel gösterge J_cv'nin J katarından çok daha büyük olup olmadığıdır

## Regularization and bias/variance
algoritmanız için düzenleme parametresinin iyi bir Lambda değerini seçmek istediğinizde yardımcı olacaktır
Burada Lambda değeri, eğitim verilerini iyi uydurmaya karşı w parametrelerini küçük tutmaktan ne kadar ödün verdiğinizi
kontrol eden düzenleme parametresidir
yüksek lambda yüksek bias demektir. Lambda 0 olursa overfit olur. İdeal bir aralıkta seçmek gerekir
Normalleştirme parametresi için kullanılacak iyi bir Lambda değerinin ne olduğuna karar vermeye çalışıyorsanız, çapraz 
doğrulama size bunu yapmanın bir yolunu da sunar
Algoritmanın en iyi performansı göstermesine neden olan bazı ara Lambda değerleri olacaktır. Çapraz doğrulamanın yaptığı şey,
Lambda'nın birçok farklı değerini denemektir
Lambda düzenleme parametresinin seçimi, algoritmanızın önyargısını ve varyansını ve genel performansını etkiler ve ayrıca
Lambda düzenleme parametresi için iyi bir seçim yapmak üzere çapraz doğrulama kullanılabilir.

## Learning curves
Eğitim seti boyutu büyüdükçe, eğitim seti hatası aslında artar.
bir öğrenme algoritmasının yüksek önyargıya sahip olması durumunda, daha fazla eğitim verisi elde etmenin tek başına
o kadar fazla umut vermeyeceği sonucuna varıyor, belki biraz şaşırtıcı. Daha fazla veriye sahip olmanın iyi olduğunu
düşünmeye alıştığımızı biliyorum, ancak algoritmanız yüksek önyargıya sahipse, yaptığınız tek şey daha fazla eğitim
verisi eklemekse, bu tek başına hatayı azaltmanıza asla izin vermez. bu kadar değerlendir. Gerçekten bu nedenle,
bu şekle ne kadar çok örnek eklerseniz ekleyin, düz doğrusal uydurma o kadar iyi olmayacak. Bu nedenle, daha fazla
eğitim verisi toplamak için çok çaba harcamadan önce, öğrenme algoritmanızın yüksek önyargıya sahip olup olmadığını
kontrol etmeye değer, çünkü varsa, muhtemelen daha fazla eğitim verisi eklemekten başka şeyler yapmanız gerekir.
Varyansınız yüksek olduğunda, eğitim kümesi boyutunu artırmak çok yardımcı olabilir ve özellikle, bu eğrileri sağa doğru
tahmin edebilirsek, M katını artırabilirsek, eğitim hatası artmaya devam eder, ancak sonra çapraz- doğrulama hatası umarım
aşağı inecek ve J trenine yaklaşacaktır. Dolayısıyla bu senaryoda, çapraz doğrulama hatasını azaltmak ve algoritmanızın
daha iyi ve daha iyi performans göstermesini sağlamak için yalnızca eğitim seti boyutunu artırarak mümkün olabilir ve bu,
yüksek önyargı durumundan farklıdır, burada yaptığınız tek şey Algoritma performansınızı çok fazla öğrenmenize yardımcı
olmayacak daha fazla eğitim verisi elde etmektir. Özetlemek gerekirse, bir öğrenme algoritması yüksek varyanstan muzdaripse,
o zaman daha fazla eğitim verisi elde etmek gerçekten yardımcı olacaktır.

## Deciding what to try next revisited
Öğrenme algoritmanızın yüksek önyargılı mı yoksa yüksek varyanslı mı olduğunu anlamaya çalışabilirsiniz.
Bu, bir öğrenme algoritması eğitirken rutin olarak yaptığım prosedürdür, algoritmamın yüksek önyargı veya
yüksek varyansa sahip olup olmadığına karar vermeye çalışmak için eğitim hatasına ve çapraz doğrulama hatasına daha sık bakarım.

## Deciding what to try next revisited
algoritmanız yüksek yanlılığa sahipse, o zaman yaptığımız tek şey daha fazla eğitim verisi almaksa, bu muhtemelen tek başına
o kadar da yardımcı olmaz. Ancak bunun aksine, algoritmanız yüksek varyansa sahipse, diyelim ki çok küçük bir eğitim setine
fazla uyuyorsa, o zaman daha fazla eğitim örneği almak çok yardımcı olacaktır
öğrenme algoritmanızın çok fazla özelliği varsa, algoritmanıza çok karmaşık modellere uyması için çok fazla esneklik sağlar.
Bu biraz x, x squared, x cubed, x^4, x^5, vb. gibi. Bunlardan sadece birkaçını ortadan kaldırsaydınız, modeliniz
çok karmaşık olmayacak ve bu kadar yüksek varyansa sahip olmayacaktı.
Lambdayı arttırmak overfit(high variance) problemini çözer.
Lambdayı azaltmak bias problemini çözer.
algoritmanızın yüksek varyansa sahip olduğunu tespit ederseniz, bunu düzeltmenin iki ana yolu;
ne daha fazla eğitim verisi alın ne de modelinizi basitleştirin. Modeli basitleştirerek demek istediğim,
ya daha küçük bir dizi özellik elde edin ya da Lambda düzenleme parametresini artırın. Algoritmanızın çok karmaşık,
çok hareketli eğrilere uyma esnekliği daha azdır. Tersine, algoritmanız yüksek önyargıya sahipse,
bu eğitim setinde bile iyi gitmiyor demektir. Durum buysa, ana düzeltmeler, modelinizi daha güçlü hale getirmek
veya onlara daha karmaşık veya daha fazla işleve uymaları için daha fazla esneklik kazandırmaktır.
Bunu yapmanın bazı yolları, ona ek özellikler vermek veya bu polinom özelliklerini eklemek veya
Lambda düzenleme parametresini azaltmaktır.
eğitim seti boyutunu küçülterek yüksek önyargıyı düzeltmeniz gerekip gerekmediğini merak ediyorsanız,
bu aslında yardımcı olmuyor. Eğitim seti boyutunu küçültürseniz, eğitim setine daha iyi uyarsınız, ancak bu,
çapraz doğrulama hatanızı ve öğrenme algoritmanızın performansını kötüleştirme eğilimindedir, bu nedenle,
yalnızca yüksek bir değeri düzeltmeye çalışmak için eğitim örneklerini rastgele atmayın

## Bias/variance and neural networks
eğer sinir ağınızı yeterince genişletirseniz, neredeyse her zaman eğitim setinize iyi uyum sağlayabilirsiniz.
Eğitim setiniz çok büyük olmadığı sürece. Ve bunun anlamı, ikisi arasında gerçekten değiş tokuş yapmaya gerek kalmadan
önyargıyı azaltmaya veya varyansı gerektiği gibi azaltmaya çalışmak için bize yeni bir reçete veriyor.
önyargıyı azaltmanın bir yolu, sadece daha büyük bir sinir ağı kullanmaktır ve daha büyük sinir ağı ile,
katman başına daha fazla gizli katman veya daha fazla gizli birim demek istiyorum. Ve daha sonra bu döngüden geçmeye devam edebilir
ve sinir ağınızı eğitim setinde başarılı olana kadar daha da büyütebilirsiniz
Eğitim setine düştükten sonra, bu sorunun cevabı evet. Daha sonra, trans doğrulama setinde iyi sonuç vermediğini sorarsınız.
Tekrar eğittikten sonra;
algoritmanın yüksek varyansa sahip olduğu sonucuna varabilirsiniz çünkü çapraz doğrulama setinde set eğitmek istemiyor.
Dolayısıyla, Jcv ve Jtrain'deki bu büyük boşluk, muhtemelen yüksek bir varyans sorununuz olduğunu gösterir ve
yüksek bir varyans sorununuz varsa, bunu düzeltmenin bir yolu daha fazla veri elde etmektir.
Daha fazla veri almak ve geri dönüp modeli yeniden eğitmek ve iki kez kontrol etmek

## Error analysis
Öğrenme algoritması performansınızı iyileştirmek için bir sonraki denemeyi seçmek için tanılama çalıştırmanıza yardımcı olacak
en önemli yollar açısından, yanlılık ve varyansın muhtemelen en önemli fikir olduğunu ve hata analizinin muhtemelen listemde ikinci
olacağını söyleyebilirim.
Örneğin, yanlış sınıflandırılmış spam e-postalarının birçoğunun ilaç satışı olduğunu, ilaç veya ilaç satmaya çalıştığını
fark ederseniz, aslında bu örnekleri inceleyeceğim ve bu sınıflandırmadaki kaç e-postanın farmasötik spam olduğunu elle
karşılayacağım ve orada olduğunu söyleyeceğim.
istenmeyen e-postaları bulmak için algoritmalar oluşturmak için çok zaman harcadım, ancak çok sonra net etkinin aslında
oldukça küçük olduğunu fark ettim. Bu, kasıtlı yazım yanlışlarını bulmaya çalışmak için çok zaman harcamadan önce daha dikkatli
hata analizi yapmayı dilediğim bir örnek
Bu analizden sonra, birçok hatanın farmasötik spam e-postaları olduğunu fark ederseniz, bu size daha sonra yapılacak şeyler
için bazı fikirler veya ilham verebilir. Örneğin, her şeyden daha fazla veri değil de daha fazla veri toplamaya karar
verebilirsiniz, ancak öğrenme algoritmasının bu farmasötik istenmeyen postaları tanımada daha iyi bir iş çıkarabilmesi için
farmasötik spam e-postalarına ilişkin daha fazla veri bulmaya çalışabilirsiniz. Veya satmaya çalıştığınız standartların belirli
ilaç adlarını veya farmasötik ürünlerin belirli adlarını söylemekle ilgili bazı yeni özellikler bulmaya karar verebilirsiniz
öğrenme algoritmanızın onları tanıma konusunda daha iyi bir iş çıkarmasına yardımcı olmak için özellikle kimlik avı e-postalarından
daha fazla veri almaya karar verebilirsiniz.
Genel olarak, hem sapma varyansı teşhisini hem de bu tür hata analizini gerçekleştirmenin taramaya veya modelde hangi
değişikliklerin bir sonraki denemede daha umut verici olduğuna karar vermeye gerçekten yardımcı olduğunu buldum.
Şimdi hata analizinin bir sınırlaması, insanların iyi olduğu problemler için bunu yapmanın çok daha kolay olmasıdır.
Özetle hata analizinde ayıklamak yerine spam olduğunu bildiğin verilerle eğitmek zamandan daha çok kazandırır.

## Adding data
Makine öğrenimi algoritmalarını eğitirken, neredeyse her zaman daha fazla veriye sahip olmayı dilemişiz gibi geliyor.
Ve bu yüzden bazen her şey hakkında daha fazla veri elde etmemize izin vermek cazip geliyor. Ancak her türden daha fazla veri
elde etmeye çalışmak yavaş ve pahalı olabilir. Bunun yerine, veri eklemenin alternatif bir yolu, analizin yardımcı olabileceğini
belirttiği türlerden daha fazla veri eklemeye odaklanmak olabilir. Önceki slaytta, hata analizinin ilaç spam'inin büyük bir
sorun olduğunu gözden geçirip geçirmediğini gördük, o zaman güneş altında her şeyi daha fazla veri almak için değil, daha fazla
farmasötik spam örneği almaya odaklanmak için daha hedefli bir çaba göstermeye karar verebilirsiniz. daha mütevazı bir maliyet bu
çok fazla etiketlenmemiş e-posta veriniz varsa, etrafta dolaşan e-postalar varsa ve henüz kimsenin spam veya spam olmayan
olarak etiketleme zahmetine girmediğini varsayalım. etiketlenmemiş veriler aracılığıyla ve özellikle ilaçla ilgili bir spam
ile ilgili daha fazla örnek bulun. Ve bu, öğrenme algoritması performansınızı, her türden e-postadan daha fazla veri eklemeye
çalışmaktan çok daha fazla artırabilir.
ses eklemek için arka plan gürültüsü veya kötü cep telefonu bağlantısı, burada test setinde beklediğinizi temsil ediyorsa, bu,
ses verilerinizde veri artırmayı gerçekleştirmenin yararlı yolları olacaktır. Aksine, verilere tamamen rastgele anlamsız gürültüde
genellikle o kadar yardımcı olmaz.
daha fazla veri toplamaktan herhangi bir şey olabilir. Eğer hata analizi bunu yapmanızı söylediyse. Daha fazla görüntü veya daha
fazla ses oluşturmak için veri büyütmeyi kullanmak veya sadece daha fazla eğitim örneği oluşturmak için veri sentezini kullanmak.
Ve bazen verilere odaklanmak, öğrenme algoritmanızın performansını iyileştirmesine yardımcı olmanın etkili bir yolu olabilir.

## Transfer learning: using data from a different task
Çok fazla veriye sahip olmadığınız bir uygulama için, transfer öğrenimi, uygulamanıza yardımcı olması için farklı bir görevdeki
verileri kullanmanıza izin veren harika bir tekniktir.
büyük bir eğitim setiniz varsa transfer öğrenme biraz daha iyi çalışabilir.
Transfer öğrenimi ile ilgili güzel bir şey de, belki de denetimli ön eğitimi gerçekleştiren kişi olmanıza gerek olmamasıdır.
Birçok sinir ağı için, zaten büyük bir görüntü üzerinde bir sinir ağını eğitmiş ve internette herkesin indirip kullanması için
ücretsiz lisanslı eğitimli bir sinir ağları yayınlamış olan araştırmacılar olacaktır.
Bunun anlamı, ilk adımı kendiniz gerçekleştirmek yerine, bir başkasının haftalarca eğitim almış olabileceği sinir ağını
indirebilir ve ardından çıktı katmanını kendi çıktı katmanınızla değiştirebilir ve ince ayar yapmak için Seçenek 1 veya
Seçenek 2'yi gerçekleştirebilirsiniz.
Adım 1, uygulamanızla aynı giriş tipine sahip büyük bir veri kümesinde önceden eğitilmiş parametrelere sahip sinir ağını
indirmektir. Bu giriş türü resimler, ses, metinler veya başka bir şey olabilir ya da sinir ağını indirmek istemiyorsanız,
belki kendinizinkini eğitebilirsiniz. Ancak pratikte, örneğin görüntüleri kullanıyorsanız, başka birinin önceden eğitilmiş
sinir ağını indirmek çok daha yaygındır. Ardından ağı kendi verilerinize göre daha fazla eğitin veya ince ayar yapın.
Büyük veri kümesi üzerinde önceden eğitilmiş bir sinir ağı elde edebilirseniz, diyelim ki bir milyon görüntü, o zaman
sinir ağına kendi başınıza ince ayar yapmak için bazen çok daha küçük bir veri kümesi, belki bin görüntü, belki daha da küçük
kullanabileceğinizi buldum. veri ve oldukça iyi sonuçlar elde edin.
GPT-3'ü, BERT'leri veya ImageNet'i duymadıysanız, duymuş olsanız da bunun için endişelenmeyin. Bunlar, makine öğrenimi
literatüründe ön eğitimin başarılı uygulamalarıdır.
Transfer öğrenimini gerçekleştirmenin iki olası yolu nedir?
Çıktı katmanları ve önceki katmanlar da dahil olmak üzere modelin tüm parametrelerini eğitmeyi seçebilirsiniz.
Yalnızca çıktı katmanlarının parametrelerini eğitmeyi seçebilir ve modelin diğer parametrelerini sabit bırakabilirsiniz.


## Full cycle of a machine learning project
makine öğrenimi sistemi oluştururken düşünülmesi ve planlanması gereken adımlar nelerdir?
MLOps adı verilen büyüyen bir alan var. Bu, Makine Öğrenimi İşlemleri anlamına gelir. Bu, makine öğrenimi sistemlerinin
sistematik olarak nasıl oluşturulacağı ve dağıtılacağı ve sürdürüleceği pratiğini ifade eder. Makine öğrenimi modelinizin güvenilir
olduğundan, iyi ölçeklendiğinden, iyi yasalara sahip olduğundan, izlendiğinden emin olmak için tüm bunları yapmak için ve
ardından, modelin iyi çalışmasını sağlamak için uygun şekilde güncellemeler yapma fırsatınız olur.

## Error metrics for skewed datasets
Çarpık veri kümeleriyle ilgili sorunlar üzerinde çalışırken, öğrenme algoritmanızın ne kadar iyi çalıştığını anlamak için
genellikle sınıflandırma hatası yerine farklı bir hata metriği kullanırız.
bu dört hücreye isim vereceğim. Gerçek sınıf bir ve tahmin edilen sınıf bir olduğunda, buna gerçek pozitif diyeceğiz çünkü
siz pozitif tahmin ettiniz ve bu doğruydu, pozitif bir örnek var. Sağ alttaki bu hücrede, gerçek sınıfın sıfır ve tahmin edilen
sınıfın sıfır olduğu yerde, buna gerçek bir negatif diyeceğiz çünkü siz negatif tahmin ettiniz ve bu doğruydu. Gerçekten olumsuz
bir örnekti. Sağ üstteki bu hücreye yanlış pozitif denir, çünkü algoritma pozitif tahmin eder, ancak yanlıştır. Aslında pozitif
değil, bu yüzden buna yanlış pozitif denir. Algoritma sıfır öngördüğü için bu hücreye yanlış negatif sayısı denir, ancak yanlıştı.

## Trading off precision and recall
Doğruluk (Accuracy) yerine F1 Score değerinin kullanılmasının en temel sebebi eşit dağılmayan veri kümelerinde hatalı bir model
seçimi yapmamaktır. Ayrıca sadece False Negative ya da False Positive değil tüm hata maliyetlerini de içerecek bir ölçme metriğine
ihtiyaç duyulduğu içinde F1 Score bizim için çok önemlidir.

## Decision tree model
## Learning Process
Karar ağacı öğrenmenin ilk adımı, kök düğümde hangi özelliğin kullanılacağına karar vermemizdir.
Bu, karar ağacının en üstündeki ilk düğümdür.
Bir karar ağacını öğrenirken vermemiz gereken ilk karar, hangi özelliği, salonu ve her bir düğümü nasıl
seçeceğimizdir. Bir karar ağacı oluştururken vermeniz gereken ikinci önemli karar, bölmeyi ne zaman
durduracağınıza karar vermektir

## Measuring purity
entropi işlevi, bir dizi verinin safsızlığının bir ölçüsüdür. Sıfırdan başlar, bire çıkar ve sonra numunenizdeki pozitif örneklerin kesrinin bir fonksiyonu olarak sıfıra geri döner. Buna benzeyen başka fonksiyonlar da var, sıfırdan bire gidiyorlar ve sonra geri düşüyorlar.

## Choosing a split: Information Gain
Bir karar ağacı oluştururken, bir düğümde hangi özelliğin bölüneceğine karar verme şeklimiz, hangi özellik seçiminin entropiyi en çok azalttığına bağlı olacaktır. Entropiyi azaltır veya safsızlığı azaltır veya saflığı en üst düzeye çıkarır. Karar ağacı öğrenmede entropinin azaltılmasına bilgi kazancı denir.

## Putting it together
Bilgi kazanım kriterleri, tek düğümü bölmek için bir özelliğin nasıl seçileceğine karar vermenizi sağlar
Ağacın kök düğümündeki tüm eğitim örnekleriyle başlar ve tüm olası özellikler için bilgi kazancını hesaplar ve
en yüksek bilgi kazancını veren bölünecek özelliği seçer. Bu özelliği seçtikten sonra, veri kümesini seçilen özelliğe
göre iki alt kümeye ayıracak ve ağacın sol ve sağ dallarını oluşturacak ve bu özelliğin değerine göre eğitim örneklerini
sol veya sağ dala göndereceksiniz.
Bu, kök düğümde bir bölme yapmanızı sağlar. Bundan sonra, ağacın sol dalında, sağ dalında vb. bölme işlemini tekrarlamaya
devam edeceksiniz. Durdurma kriterleri karşılanana kadar bunu yapmaya devam edin. Durdurma kriterinin olabileceği durumlarda,
bir düğüm yüzde 100 tek bir tümce olduğunda, birisi entropiye sıfıra ulaştıysa veya bir düğümü daha fazla bölmek ağacın
belirlediğiniz maksimum derinliği aşmasına neden olacaksa veya ek bir bölünme eşikten daha azsa veya bir düğümdeki örnek sayısı
bir eşiğin altındaysa. Bu kriterlerden biri veya birkaçı olabilecek, seçtiğiniz durdurma kriteri karşılanana kadar bölme
işlemini tekrarlamaya devam edeceksiniz.
Doğru alt ağacı oluşturma yöntemimiz, yine beş örnekten oluşan bir alt küme üzerinde bir karar ağacı oluşturmaktı.
Bilgisayar biliminde bu, özyinelemeli bir algoritma örneğidir. Bunun anlamı, kökte bir karar ağacı oluşturmanın yolu,
sol ve sağ alt dallarda başka daha küçük karar ağaçları oluşturmaktır. Bilgisayar biliminde özyineleme, kendini çağıran
kod yazmak anlamına gelir. Bunun bir karar ağacı oluştururken ortaya çıkma şekli, daha küçük alt karar ağaçları oluşturarak
ve ardından hepsini bir araya getirerek genel karar ağacını oluşturmanızdır.

## Continuous valued features
Karar ağacını yalnızca anlaşmazlık değeri değil, sürekli değer olan özelliklerle çalışacak şekilde nasıl değiştirebileceğinize
bakalım. Bu, herhangi bir sayı olabilen özelliklerdir. Bir örnekle başlayalım, hayvanın ağırlığı olan bir özellik daha eklemek
için veri kümesinin kedi evlat edinme merkezini değiştirdim. Kediler ve köpekler arasında ortalama pound cinsinden kediler
köpeklerden biraz daha hafiftir, ancak bazı kediler bazı köpeklerden daha ağırdır. Ancak bu nedenle bir hayvanın ağırlığı,
kedi olup olmadığına karar vermek için yararlı bir özelliktir. Peki böyle bir özelliği kullanmak için bir karar ağacını nasıl
elde edersiniz? Karar ağacı öğrenme algoritması, sadece kulak şekli, yüz şekli ve bıyıklar üzerinde bölünmeyi kısıtlamak yerine,
 eskisi gibi ilerleyecektir. Kulak şekline, yüz şekline bölünmeden oluşmalısınız. bıyık veya ağırlık. Ve eğer ağırlık özelliğini
 bölmek diğer seçeneklerden daha iyi bilgi kazancı sağlıyorsa. Sonra ağırlık özelliğini böleceksiniz. Ancak ağırlık özelliğini
nasıl böleceğinize nasıl karar veriyorsunuz? Bir bakalım. İşte kökündeki verilerin bir çizimi. Yatay eksende çizilmemiş.
Hayvana ve dikey eksene giden yol, aşağıdaki kedi değil, üstteki kedidir. Yani dikey eksen etiketi gösterir, y 1 veya 0'dır.
Ağırlık özelliğine göre bölünme şeklimiz, verileri, ağırlığın bir değerden küçük veya ona eşit olup olmadığına göre bölecek
olsaydık olurdu. 8 Veya sayının bir kısmını diyelim. Bu, seçilecek öğrenme algoritmasının işi olacaktır. Ve ağırlık özelliği
üzerindeki kısıtlamalar bölündüğünde yapmamız gereken, bu eşiğin birçok farklı değerini göz önünde bulundurmak ve ardından en
iyisini seçmektir. Ve en iyisiyle, en iyi bilgi kazancıyla sonuçlananı kastediyorum. Özellikle, örnekleri ağırlığın 8'den küçük
veya eşit olup olmadığına göre bölmeyi düşünüyorsanız, bu veri kümesini iki alt kümeye böleceksiniz. Soldaki alt kümenin iki
kedisi ve sağdaki alt kümenin üç kedisi ve beş köpeği vardır. Yani her zamanki bilgi kazancı hesaplamamızı hesaplayacak
olsaydınız, entropiyi kök notada hesaplayacaksınız N C p f 0.5 eksi şimdi sol bölünmenin 2/10 katı entropinin iki kedisi daha
var. Yani 2/2 olmalı artı sağ bölmede 10 örnekten sekizi ve bir entropi F var. Bu, sağdaki üç kedideki sekiz örnekten biri.
3/8'lik girişe ve bu 0.24'e çıkıyor. Dolayısıyla, ağırlığın 8'e eşit olup olmadığına bölünürseniz bu bilgi kazancı olur,
ancak diğer değerleri de denemeliyiz. Peki ya ağırlığın 9'a eşit olup olmadığına bölünürseniz ve bu buradaki yeni çizgiye
karşılık gelirse. Ve bilgi kazancı hesaplaması H (0,5) eksi olur. Şimdi dört örneğimiz var ve tüm kedileri böldük. Yani bu
4/4'ün 4/10 katı entropi artı sağında bir kediniz olan altı örnek. Yani bu, 1/6'nın her birinin 6/10 katı, bu da 0.61'e
eşittir. Dolayısıyla buradaki bilgi kazancı çok daha iyi görünüyor, 0.61'den çok daha yüksek olan 0.24 bilgi kazancıdır.
Ya da 13 gibi başka bir değer deneyebiliriz. Ve hesaplama şuna benziyor, ki bu 0.40.

Daha genel durumda, aslında sadece üç değeri değil, X ekseni boyunca birden çok değeri deneyeceğiz. Ve bir kural, tüm örnekleri
ağırlığa veya bu özelliğin değerine göre sıralamak ve sıralanmış eğitim listesi arasında orta nokta olan tüm değerleri almak
olacaktır. Bu eşik için dikkate alınacak değerler olarak örnekler burada. Bu şekilde, 10 eğitim örneğiniz varsa, bu eşik için
dokuz farklı olası değeri test edecek ve ardından size en yüksek bilgi kazancını vereni seçmeye çalışacaksınız. Ve son olarak,
bu eşiğin belirli bir değerine bölünerek elde edilen bilgiler, diğer herhangi bir özelliğe bölünerek elde edilen bilgilerden
daha iyiyse, o notu o özelliğe bölmeye karar vereceksiniz. Ve bu örnekte 0,61'lik bir bilgi kazancının diğer tüm
özelliklerinkinden daha yüksek olduğu ortaya çıkıyor. Aslında iki eşik oldukları ortaya çıktı. Algoritmanın bu özelliği bölmek
için seçtiğini varsayarsak, veri kümesini hayvanın ağırlığının 9 sterline eşit olup olmadığına göre böleceksiniz. Ve böylece,
bunun gibi iki veri alt kümesiyle sonuçlanırsınız ve daha sonra ağacın geri kalanını oluşturmak için bu iki veri alt kümesini
kullanarak özyinelemeli, li ek karar ağaçları oluşturabilirsiniz. Bu nedenle, karar ağacının her notada sürekli değer
özellikleri üzerinde çalışmasını sağlamak için özetlemek gerekirse. Bölmeleri tüketirken, bölünecek farklı değerleri göz
önünde bulundurur, olağan bilgi kazancı hesaplamasını yapar ve mümkün olan en yüksek bilgi kazancını verirse bu sürekli
değer özelliğine bölünmeye karar verirsiniz. Böylece karar ağacını sürekli değer özellikleriyle çalışacak şekilde elde
edersiniz. Farklı eşikleri deneyin, olağan bilgi kazancı hesaplamasını yapın ve bölünebilecek tüm olası özelliklerden mümkün
olan en iyi bilgi kazancını sağlıyorsa, seçilen eşikle sürekli değer özelliğini bölün. Karar ağacı öğrenme algoritmasını
regresyon ağaçlarına genelleştiren, izleyebileceğiniz veya izleyemeyeceğiniz isteğe bağlı bir video bulunduktan sonra çekirdek
karar ağacı algoritmasında gerekli videolar için bu kadar. Şimdiye kadar, yalnızca kedi olsun ya da olmasın gibi ayrı bir
kategoriyi öngören sınıflandırmalar olan tahminler yapmak için karar ağaçlarını kullanmaktan bahsettik. Peki ya bir sonraki
videoda bir sayıyı tahmin etmek istediğiniz bir regresyon probleminiz varsa. Bununla başa çıkmak için karar ağaçlarının
genelleştirilmesinden bahsedeceğim.

## Regression Trees 
Şimdiye kadar sadece sınıflandırma algoritmaları olarak karar ağaçlarından bahsettik. Bu isteğe bağlı videoda, bir sayıyı
tahmin edebilmemiz için karar ağaçlarını regresyon algoritmaları olarak genelleyeceğiz. Bir bakalım. Bu video için kullanacağım
örnek, daha önce sahip olduğumuz bu üç özelliği, yani bu özellikleri kullanmak olacaktır. X, Hayvanın ağırlığını tahmin etmek
için, Y. Yani açık olmak gerekirse, önceki videonun aksine buradaki ağırlık artık bir girdi değil özellik. Bunun yerine,
bu hedef çıktıdır, Y, bir hayvanın kedi olup olmadığını tahmin etmeye çalışmak yerine tahmin etmek istediğimiz. Bu bir
regresyon problemidir çünkü bir sayıyı tahmin etmek istiyoruz, Y. Bir regresyon ağacının nasıl görüneceğine bakalım. Burada,
kök düğümün kulak şekline bölündüğü ve daha sonra sol ve sağ alt ağacın yüz şekline ve ayrıca sağdaki yüz şekline bölündüğü
bu regresyon problemi için zaten bir ağaç inşa ettim. Ve hem sol hem de sağ taraftaki dallarda aynı özelliğe bölünmeyi seçen
bir karar ağacında yanlış bir şey yoktur. Bölme algoritması bunu yapmayı seçerse gayet iyi. Eğer antrenman sırasında bu
bölünmelere karar vermiş olsaydınız, o zaman buradaki düğümde ağırlıkları 7.2, 7.6 ve 10.2 olan bu dört hayvan olurdu.
Bu düğüm, bu kalan iki düğüm için 9.2 ve benzeri ağırlığa sahip bir hayvana sahip olacaktır.
Yani, bu karar ağacı için doldurmamız gereken son şey, eğer bu düğüme inen bir test örneği varsa, sivri kulakları ve yuvarlak
yüz şekli olan bir hayvan için tahmin etmemiz gereken ağırlıklar nelerdir? Karar ağacı, buradaki eğitim örneklerindeki
ağırlıkların ortalamasını alarak bir tahminde bulunacaktır. Ve bu dört sayının ortalamasını alarak 8.35 elde ettiğiniz ortaya
çıkıyor.
Yüz şeklindeki tükürüğü seçecek olsaydınız, aşağıda yazılı olan karşılık gelen ağırlıklarla solda ve sağda bu hayvanlarla
sonuçlanırsınız. Ve eğer mevcut ya da yokken bıyıklara ayrılmayı seçseydin, sonunda bununla sonuçlanırdın. Öyleyse soru şu ki,
kök düğümde bölünebilecek bu üç olası özellik göz önüne alındığında, hayvanın ağırlığı için en iyi tahminleri veren hangisini
seçmek istiyorsunuz? Bir regresyon ağacı oluştururken, bir sınıflandırma problemi için sahip olduğumuz kirlilik ölçüsü olan
entropiyi azaltmaya çalışmak yerine, verilerin bu alt kümelerinin her birinde Y değerlerinin ağırlığının varyansını azaltmaya
çalışırız. Yani, değişken kavramını başka bağlamlarda gördüyseniz, bu harika. Bu, bir dakika içinde kullanacağımız varyantların
istatistiksel matematiksel kavramıdır. Ancak, daha önce bir sayı kümesinin varyansını nasıl hesaplayacağınızı görmediyseniz,
bunun için endişelenmeyin. Bu slayt için bilmeniz gereken tek şey, varyantların bir sayı kümesinin ne kadar büyük ölçüde
değiştiğini gayri resmi olarak hesaplamasıdır. Yani bu sayı kümesi için 7.2, 9.2 ve benzeri, 10.2'ye kadar, varyansın 1.47
olduğu ortaya çıktı, bu yüzden o kadar da değişmiyor. Oysa burada 8.8, 15, 11, 18 ve 20, bu sayılar 8.8'den 20'ye kadar çıkıyor.
Ve böylece varyans çok daha büyük, 21.87 varyansına çıkıyor. Ve böylece bölünmenin kalitesini değerlendirme şeklimiz, öncekiyle
aynı şeyi hesaplayacağız, sol ve sağ dallara giden örneklerin kesri olarak W sol ve W sağ. Ve bölünmeden sonraki ortalama varyans 5/10 olacak, bu W sol çarpı 1.47, bu soldaki varyans ve ardından artı sağdaki varyansın 5/10 katı, yani 21.87. Dolayısıyla, bu ağırlıklı ortalama varyansı, bir sınıflandırma problemi için hangi bölünmenin kullanılacağına karar verirken kullandığımız ağırlıklı ortalama entropisine çok benzer bir rol oynar. Ve daha sonra bölünecek diğer olası özellik seçenekleri için bu hesaplamayı tekrarlayabiliriz. Burada ortadaki ağaçta, buradaki bu sayıların varyansı 27.80 olarak ortaya çıkıyor. Buradaki varyans 1.37'dir. Ve böylece W sol ile onda yediye eşittir ve W sağ onda üçe eşittir ve bu değerlerle ağırlıklı varyansı aşağıdaki gibi hesaplayabilirsiniz. Son olarak, son örnek için, bıyık özelliğini bölecek olsaydınız, bu sol ve sağdaki varyanstır, W sol ve W sağ vardır. Ve böylece varyansın ağırlığı budur. Bir bölme seçmenin iyi bir yolu, yalnızca en düşük ağırlıklı varyansın değerini seçmek olacaktır. Bilgi kazancını hesaplarken olduğu gibi, bu denklemde sadece bir değişiklik daha yapacağım. 
Sınıflandırma problemine gelince, sadece ortalama ağırlıklı entropiyi ölçmedik, entropideki azalmayı ölçtük ve bu bilgi 
kazancıydı. Bir regresyon ağacı için varyanstaki azalmayı da benzer şekilde ölçeceğiz. Eğitim setindeki tüm örneklere bakarsanız,
on örneğin hepsine bakarsanız ve hepsinin varyansını hesaplarsanız, tüm örneklerin varyansı 20.51 olur. Ve bu, tüm bunlardaki 
kök düğümü için aynı değerdir, elbette, çünkü kök düğümündeki aynı on örnektir. Ve böylece kök düğümünün varyansını 
hesaplayacağız, ki bu 20.51 eksi buradaki ifade, 8.84'e eşit çıkıyor. Ve böylece kök düğümünde varyans 20.51 idi ve kulak 
şekline bölündükten sonra bu iki düğümdeki ortalama ağırlıklı varyans 8.84 daha düşüktü. Yani varyanstaki azalma 8.84'tür. 
Ve benzer şekilde, ortadaki bu örnek için varyansı azaltma ifadesini hesaplarsanız, 20.51 eksi daha önce sahip olduğumuz bu 
ifade, 0.64'e eşittir. Yani, bu varyansta çok küçük bir azalmadır. Ve bıyık özelliği için 6.22 olan bununla sonuçlanırsınız. 
Dolayısıyla, bu örneklerin üçü arasında 8.84 size varyanstaki en büyük azalmayı verir. Bu nedenle, daha önce bir regresyon 
ağacı için size en büyük bilgi kazancını sağlayan özelliği seçtiğimiz gibi, varyansta size en büyük azalmayı sağlayan özelliği 
seçeceksiniz, bu nedenle ayrılacak özellik olarak kulak şeklini seçiyorsunuz. Tükürmek için yıl şeklindeki özellikleri 
seçtikten sonra, şimdi sol ve sağ yan dallarda beş örnekten oluşan iki alt kümeniz var ve o zaman tekrar tekrar söylüyoruz, 
burada bu beş örneği alıp yeni bir karar ağacı yapıyorsunuz. sadece bu beş örneğe odaklanarak, yine farklı seçenekleri 
değerlendirerek ayrılacak özelliklerden ve size en büyük varyans azaltımını sağlayanı seçmek. Ve aynı şekilde sağda. 
Ve daha fazla bölünmeme kriterlerini karşılayana kadar bölünmeye devam edersiniz. Ve işte bu kadar. Bu teknikle, sadece 
sınıflandırma problemlerini değil, aynı zamanda regresyon problemlerini de yürütmek için karar tedavinizi alabilirsiniz. 
Şimdiye kadar tek bir karar ağacının nasıl eğitileceğinden bahsettik. Çok fazla karar ağacı yetiştirirseniz, buna karar 
ağaçları topluluğu diyoruz, çok daha iyi bir sonuç elde edebilirsiniz. Bir sonraki videoda bunun neden ve nasıl yapılacağına 
bir göz atalım.

## Sampling with replacement
Değiştirme ile örnekleme işlemi, orijinal eğitim setinize biraz benzeyen, ancak aynı zamanda oldukça farklı olan yeni bir 
eğitim seti oluşturmanıza olanak tanır. Bunun bir ağaç topluluğu oluşturmak için kilit yapı taşı olacağı ortaya çıktı

## Random forest algorithm
Artık, orijinal eğitim setine biraz benzeyen ancak aynı zamanda oldukça farklı olan yeni eğitim setleri oluşturmak için 
değiştirmeli bir şey kullanmanın bir yolumuz var. İlk ağaç topluluğu algoritmamızı oluşturmaya hazırız

## XGBoost
Hızlı çalışır, açık kaynak uygulamaları kolayca kullanılır, birçok ticari uygulamada olduğu gibi birçok makine öğrenimi 
yarışmasını kazanmak için de çok başarılı bir şekilde kullanılmıştır.
bir sonraki karar ağacını oluştururken, henüz iyi durumda olmadığımız örneklere daha fazla odaklanacağız. Bu nedenle, 
tüm eğitim örneklerine bakmak yerine, henüz iyi sonuç vermeyen örneklerin alt kümesine daha fazla odaklanıyoruz ve yeni karar 
ağacını alıyoruz, bir sonraki karar ağacı raporlama topluluğu, onları iyi yapmaya çalışacak. Ve bu, artırmanın arkasındaki 
fikirdir ve öğrenme algoritmasının daha iyisini daha hızlı yapmayı öğrenmesine yardımcı olduğu ortaya çıktı
XGBoost ayrıca, bölmenin ne zaman durdurulacağına ilişkin varsayılan bölme ölçütleri ve ölçütleri konusunda iyi bir seçime 
sahiptir. Ve XGBoost'taki yeniliklerden biri de, fazla takmayı önlemek için düzenlileştirir

## When to use decision trees
Karar ağaçları ve ağaç toplulukları, genellikle yapılandırılmış veriler olarak da adlandırılan tablo verileri üzerinde iyi
çalışır. Bunun anlamı, eğer veri kümeniz dev bir elektronik tabloya benziyorsa, karar ağaçları dikkate alınmaya değer olacaktır.
Örneğin, konut fiyat tahmin uygulamasında evin büyüklüğü, yatak odası sayısı, kat sayısı ve evin yaşına karşılık gelen
özelliklere sahip bir veri setimiz vardı. Kategorik veya sürekli değerli özelliklere sahip ve hem sınıflandırma hem de ayrı
bir kategori tahmin etmeye veya bir sayı tahmin etmeye çalıştığınız regresyon görevi için bir elektronik tabloda depolanan bu 
tür veriler. Tüm bu problemler, karar ağaçlarının iyi yapabileceği problemlerdir.
Buna karşılık, yapılandırılmamış veriler üzerinde karar ağaçları ve ağaç topluluklarının kullanılmasını önermem.
Bunlar, bir elektronik tablo biçiminde saklama olasılığınız daha düşük olan resim, video, ses ve metinler gibi verilerdir. 
Birazdan göreceğimiz gibi sinir ağları, yapılandırılmamış veri görevi için daha iyi çalışma eğiliminde olacaktır. 
Karar ağaçlarının ve ağaç topluluklarının büyük bir avantajı, çok hızlı eğitilebilmeleridir.
küçük karar ağaçları belki insan tarafından yorumlanabilir.
Karar ağaçları ve ağaç topluluklarının aksine, tablo veya yapılandırılmış veriler ve yapılandırılmamış veriler dahil olmak 
üzere her tür veri üzerinde iyi çalışır. Hem yapılandırılmış hem de yapılandırılmamış bileşenleri içeren karma verilerin yanı 
sıra. Tablo şeklinde yapılandırılmış verilerde, sinir ağları ve karar ağaçları genellikle görüntü, video, ses ve metin gibi 
yapılandırılmamış veriler üzerinde rekabet ederken, karar ağacı veya ağaç topluluğu değil, sinir ağı gerçekten tercih edilen 
algoritma olacaktır. Olumsuz tarafı, sinir ağları bir karar ağacından daha yavaş olabilir. Büyük bir sinir ağının eğitilmesi 
uzun zaman alabilir.
birlikte çalışan birden çok makine öğrenimi modelinden oluşan bir sistem oluşturuyorsanız, birden çok karar ağacını bir araya 
getirmek ve birden çok sinir ağını eğitmek daha kolay olabilir. Bunun nedenleri oldukça tekniktir
Ancak, birden fazla sinir ağını bir araya getirdiğinizde bile, eğim inişini kullanarak hepsini birlikte eğitebilirsiniz. Oysa 
karar ağaçları için bir seferde yalnızca bir karar ağacı eğitebilirsiniz.
